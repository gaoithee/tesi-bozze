{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a268f4e1-3a6d-464d-885b-2053e3143a2f",
   "metadata": {},
   "source": [
    "# Dataset, documents, FAISS; retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96db4863-a54b-4d3a-87e2-74c645fcc018",
   "metadata": {},
   "source": [
    "## 🔹 Load the dataset containing the tuples `(query, correct_answer, distractor_1, distractor_2)` and the one containing the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "094ab87a-0ba8-4878-991e-01515de302e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers', 'correct_answer', 'distractor_1', 'distractor_2'],\n",
       "    num_rows: 82326\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('saracandu/msmarco_modified', split=\"train\", trust_remote_code=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "856a910c-be2a-440f-8cab-c0faac78af84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='\"Since 2007, the RBA\\'s outstanding reputation has been affected by the \\'Securency\\' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA\\'s employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\"')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
    "\n",
    "loader = HuggingFaceDatasetLoader('saracandu/msmarco_filtered', 'passage_text')\n",
    "documents = loader.load()\n",
    "documents[0] # just to check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045746d4-d0d6-4cf1-b68e-63bebe72f990",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 🔹 Turn `documents` into a vector database using FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2626e646-9fb4-4235-8858-edb8a898f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# create an instance of the RecursiveCharacterTextSplitter class with specific parameters\n",
    "# (it splits text into chunks of 50 characters each with a 20-character overlap)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "\n",
    "# 'documents' holds the text you want to split, split the text into documents using the text splitter\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e91fa4f-156e-45dc-a6fa-2b89489b891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose an embedding method\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-l6-v2\",  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ccbce37-535e-4952-a076-683e009b9d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the documents 'docs' into vectors using the embedding method specified by 'embedding'\n",
    "# the result is stored in a FAISS index:\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# to avoid computing it each time (since the docs won't change), save the result in the storage\n",
    "db.save_local(folder_path=\"faiss_db\", index_name=\"MSMARCO_FaissIndex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17955f64-8650-4095-860e-5b818bd6a833",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 🔹 Upload the already existing vector database (if it exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecbc054f-ab89-47dd-941f-d002e62abcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.load_local(\n",
    "    folder_path=\"faiss_db\", # where to find it\n",
    "    embeddings=embeddings, # in which \"embedding language\" it is expressed\n",
    "    index_name=\"MSMARCO_FaissIndex\", # since the folder contains multiple vector databases, specify its name\n",
    "    allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5babae8-e52e-4046-9a29-e886bb5f4b22",
   "metadata": {},
   "source": [
    "## 🔹 Use it as a `retriever`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b061a230-7d28-4693-89ce-d4fad815925f",
   "metadata": {},
   "source": [
    "**Note:** `'k'=10` specifies the number of documents to retrieve each time `retrieved` is invoked. \n",
    "The default type of search performed is `similarity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e800071b-4968-4c0a-818d-16534f44d153",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(\n",
    "    search_kwargs={'k': 3,}\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f895debf-b05f-4bbf-aa4f-c9f32aa693d9",
   "metadata": {},
   "source": [
    "Why `'k'=10`? Because MSMARCO assigns to each `(query, answer)` pair 10 text passages, and only 1 or 2 of these are truly relevant. \n",
    "In this first step of analysis I chose not to create `len(dataset)` different vector databases, one for each `(query, answer)` pair, but instead to merge all the passages together and store them into an unique vector database. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926bb532-fa37-4cfe-89fa-731ad140276d",
   "metadata": {},
   "source": [
    "# `Llama-2-7b-chat-hf`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86132ec0-f0d0-4316-b71e-92af9861cda2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 🔹 Upload the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ce89a37-b6e1-48ff-b8fb-621e456392c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d0af05bef3d4bb7ad2fa2d72f084adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# do not run this unless necessary!\n",
    "\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc6eb0bf-fb11-4952-b14e-809b77898a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962ddad9c1ff4ecc99f54384312b178e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "#################################################################\n",
    "# Tokenizer\n",
    "#################################################################\n",
    "\n",
    "model_name=\"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "#################################################################\n",
    "# bitsandbytes parameters\n",
    "#################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "#################################################################\n",
    "# Set up quantization config\n",
    "#################################################################\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "#################################################################\n",
    "# Load pre-trained config\n",
    "#################################################################\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58bf643-6ff9-4759-96b3-adf1b9e447aa",
   "metadata": {},
   "source": [
    "## 🔹 Pipeline for **thesis** generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06649d5c-6f76-426b-9edb-5fd535376e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "response_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\", # deve matchare la scheda del modello HF o dà errore\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    repetition_penalty=1.5,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=400,\n",
    "    top_p=0.0\n",
    ")\n",
    "\n",
    "response_generation_llm = HuggingFacePipeline(pipeline=response_generation_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d43a34-fc04-4043-afcb-c8271f6a4e3f",
   "metadata": {},
   "source": [
    "## 🔹 Retrieve the proper documents for that question, format them properly and finally append them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61598fb6-67e3-4d06-94bf-892f4330ba0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what is rba',\n",
       " 'was ronald reagan a democrat',\n",
       " 'how long do you need for sydney and surrounding areas',\n",
       " 'price to install tile in shower',\n",
       " 'why conversion observed in body']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select a subset of the queries, just for test:\n",
    "first_queries = dataset['query'][:5]\n",
    "first_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e058aac-3fa5-42b7-95db-809669ac8524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for correct answers and distractors:\n",
    "correct_answers = dataset['correct_answer'][:5]\n",
    "distractors_1 = dataset['distractor_1'][:5]\n",
    "distractors_2 = dataset['distractor_2'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cab974ea-41f0-44b3-8167-592bbd323320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffles the order of the vector containing the correct answer and the two distractors\n",
    "# returns another vector, shuffled\n",
    "import random\n",
    "\n",
    "def shuffleAnswers(correct_answer, distractor_1, distractor_2):\n",
    "    merge_options = [correct_answer, distractor_1, distractor_2]\n",
    "    random.shuffle(merge_options)\n",
    "    return merge_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b38b14ba-0d52-4ca3-806c-a7dd8241e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary function to format properly the output of the retrieval step\n",
    "\n",
    "def format_page_content(documents):\n",
    "    \"\"\"\n",
    "    Formats the list of retrieved documents such that 'page_content', 'Documents', 'metadata' \n",
    "    words are removed and just the true content is kept.\n",
    "    \"\"\"\n",
    "    formatted_output = \"\"\n",
    "    for i, doc in enumerate(documents, start=1):\n",
    "        content = doc.page_content.strip(\" \")\n",
    "        formatted_output += f\"[{i}]: {content}\\n\"\n",
    "    return formatted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d7751aac-b970-455b-b2b4-4b8575aad07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template definition\n",
    "# requires question, options (a string containing the possible options) and the context as input variables!\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "    You're a helpful assistant and you are asked to answer a question correctly, given a certain number of options. \n",
    "    Answer with the correct option only and then stop.\n",
    "    Given this question: {question} \\n\n",
    "    You must answer by choosing only one option above these: {option_a}, {option_b}, {option_c}. \\n\n",
    "    Here is context to help: {context} \\n\n",
    "    The correct answer is:\n",
    " \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "49ad711e-db35-47d4-8949-ffedc071f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM chain definition\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from operator import itemgetter\n",
    "\n",
    "augmentation = {\"question\": itemgetter(\"question\"),\n",
    "                \"option_a\": itemgetter(\"option_a\"), \n",
    "                \"option_b\": itemgetter(\"option_b\"),\n",
    "                \"option_c\": itemgetter(\"option_c\"),\n",
    "                \"context\": itemgetter(\"context\"), }\n",
    "\n",
    "llm_chain = augmentation | prompt_template | response_generation_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "18ca079d-d85c-491f-87ab-2a0dfd626894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thesisGeneration(query, prompt_template, merged):\n",
    "    documents_retrieved = retriever.invoke(query)\n",
    "    formatted_context = format_page_content(documents_retrieved)\n",
    "    \n",
    "    given_answer = llm_chain.invoke({'question': query, \n",
    "                                     'option_a': merged[0], 'option_b': merged[1], 'option_c': merged[2], \n",
    "                                     'context': formatted_context})\n",
    "    return given_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "493b80e2-2715-4084-bba5-791adc4b65b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(text):\n",
    "    # trova l'indice in cui inizia il testo \"the correct answer is:\"\n",
    "    start_index = text.find(\"The correct answer is:\") + len(\"The correct answer is:\")\n",
    "    # estrai il testo dopo \"The correct answer is:\"\n",
    "    correct_answer_text = text[start_index:].strip()\n",
    "    \n",
    "    return correct_answer_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0c65767f-ef49-4f37-8a8e-7810529ef08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True answer: ['Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/pipelines/base.py:1167: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given answer: Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.\n",
      "****************\n",
      "True answer: ['Yes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/pipelines/base.py:1167: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given answer: [\"B Yes\"]\n",
      "****************\n",
      "True answer: ['20-25 minutes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/pipelines/base.py:1167: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given answer: [\"20-25 minutes\"]\n",
      "****************\n",
      "True answer: ['$11 to $22 per square foot']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/pipelines/base.py:1167: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given answer: [\"$11 to $22 per square foot\"]\n",
      "****************\n",
      "True answer: ['Due to symptoms in the body']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/pipelines/base.py:1167: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given answer: * Due to symptoms in the body\n",
      "****************\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"True answer: {correct_answers[i]}\")\n",
    "    merged_options = shuffleAnswers(correct_answers[i], distractors_1[i], distractors_2[i])\n",
    "    print(f\"Given answer: {extract_answer(thesisGeneration(first_queries[i], prompt_template, merged_options))}\")\n",
    "    print('****************')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022cd3d5-2f62-4922-8a6d-39a99b567b3b",
   "metadata": {},
   "source": [
    "# 🔹 PRIMO TENTATIVO DI CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2b29f6d5-a6bb-43e2-b45b-a02677767732",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_check_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\", # deve matchare la scheda del modello HF o dà errore\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    repetition_penalty=1.5,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=400,\n",
    "    top_p=0.0\n",
    ")\n",
    "\n",
    "response_check_llm = HuggingFacePipeline(pipeline=response_check_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8451da76-fc21-44f2-81a5-64eabe0ade4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "    You're a helpful assistant and you are asked to check whether or not a question was answered correctly, given a certain number of candidate options and the context. \n",
    "    Answer whether and why you think the answer is correct.\n",
    "    Given this question: {question} \\n these are the possible options: {options} and the answer to check is {candidate_answer}. \n",
    "    Here is context to help: {context} \\n\n",
    "    Is the answer correct? Why or why not?\n",
    " \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "70e78a69-c4b4-42c3-996f-194957c66661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct answer not found.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Use regular expression to extract the correct answer\n",
    "correct_answer_match = re.search(r\"The correct answer is:\\n\\s*\\([A-Z]\\)\", given_answer)\n",
    "\n",
    "if correct_answer_match:\n",
    "    correct_answer = correct_answer_match.group(1)\n",
    "    print(f\"The correct answer is: {correct_answer}\")\n",
    "else:\n",
    "    print(\"Correct answer not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b0adff7a-5aa1-447f-a969-ca5061f8125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from operator import itemgetter\n",
    "\n",
    "augmentation = {\"options\": itemgetter(\"options\"), \"context\": itemgetter(\"context\"), \n",
    "                \"question\": itemgetter(\"question\"), \"candidate_answer\": itemgetter(\"candidate_answer\")}\n",
    "llm_chain = augmentation | prompt_template | response_check_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "05a40859-5dca-4f2e-b4e5-8f15fddc2dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n    You're a helpful assistant and you are asked to check whether or not a question was answered correctly, given a certain number of candidate options and the context. \\n    Answer whether and why you think the answer is correct.\\n    Given this question: What is the name of the potion allowing to change appearance? \\n these are the possible options: A. Felix Felicis; B. Polyjuice Potion; C. Amortentia and the answer to check is B. \\n    Here is context to help: [1] P a g e | 184 Harry Potter and the Chamber of Secrets – J. K. Rowling man who seemed to have been turned inside out and a witch sprouting several extra pairs of arms out of her head. “Here it is,” said Hermione excitedly as she found the page headed The Polyjuice Potion. It was decorated with drawings of people halfway through transforming into other people. Harry sincerely hoped the artist had imagined the looks of intense pain on their faces. “This is the most complicated potion I’ve ever seen,” said Hermione as they scanned the recipe. “Lacewing flies, leeches, fluxweed, and knotgrass,” she murmured, running her finger down the list of ingredients. “Well, they’re easy enough, they’re in the student store-cupboard, we can help ourselves. … Oooh, look, powdered horn of a bicorn — don’t know where we’re going to get that — shredded skin of a boomslang — that’ll be tricky, too — and of course a bit of whoever we want to change into.” “Excuse me?” said Ron sharply. “What d’you mean, a\\n[2] P a g e | 240 Harry Potter and the Chamber of Secrets – J. K. Rowling Harry showed her Goyle’s hair. “Good. And I sneaked these spare robes out of the laundry,” Hermione said, holding up a small sack. “You’ll need bigger sizes once you’re Crabbe and Goyle.” The three of them stared into the cauldron. Close up, the potion looked like thick, dark mud, bubbling sluggishly. “I’m sure I’ve done everything right,” said Hermione, nervously rereading the splotched page of Moste Potente Potions. “It looks like the book says it should … once we’ve drunk it, we’ll have exactly an hour before we change back into ourselves.” “Now what?” Ron whispered. “We separate it into three glasses and add the hairs.” Hermione ladled large dollops of the potion into each of the glasses. Then, her hand trembling, she shook Millicent Bulstrode’s hair out of its bottle into the first glass. The potion hissed loudly like a boiling kettle and frothed madly. A second later, it had turned a sick sort of yellow. “Urgh\\n[3] P a g e | 178 Harry Potter and the Chamber of Secrets – J. K. Rowling “There might be a way,” said Hermione slowly, dropping her voice still further with a quick glance across the room at Percy. “Of course, it would be difficult. And dangerous, very dangerous. We’d be breaking about fifty school rules, I expect —” “If, in a month or so, you feel like explaining, you wil l let us know, won’t you?” said Ron irritably. “All right,” said Hermione coldly. “What we’d need to do is to get inside the Slytherin common room and ask Malfoy a few questions without him realizing it’s us.” “But that’s impossible,” Harry said as Ron laughed. “No, it’s not,” said Hermione. “All we’d need would be some Polyjuice Potion.” “What’s that?” said Ron and Harry together. “Snape mentioned it in class a few weeks ago —” “D’you think we’ve got nothing better to do in Potions than listen to Snape?” muttered Ron. “It transforms you into somebody else. Think about it! We could change into three of the Slytherins.\\n \\n\\n    Is the answer correct? Why or why not?\\n \""
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke({'question': \"What is the name of the potion allowing to change appearance?\", \n",
    "                  'options': \"A. Felix Felicis; B. Polyjuice Potion; C. Amortentia\", \n",
    "                  'context': formatted_context,'candidate_answer': 'B'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67911749-0c01-4745-bf15-2b7f5cd98139",
   "metadata": {},
   "outputs": [],
   "source": [
    "Il problema è che non hai ancora capito come formattare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba485875-13a8-42f0-81cc-63f0ffa17a47",
   "metadata": {},
   "source": [
    "##  Transformers pipeline (and zero-shot):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c8766c-5898-45f8-8a7c-8a52a797435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You're a helpful assistant and you are asked to answer a question correctly, given a certain number of options. Answer with the correct option only and then stop.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Given this question: Who is Taylor Swift? \\n you must answer by choosing only one option above these: A. a snowboard; B. a cat; C. a singer. The correct answer is:\"},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f9acd8-20a9-4848-b7f5-2ee6f67ceb12",
   "metadata": {},
   "source": [
    "##  Transformers AutoModelForCausalLM  (and zero-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9be92ef-df0c-43ac-aba8-b2da824f09fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You're a helpful assistant and you are asked to answer a question correctly, given a certain number of options. Answer with the correct option only and then stop.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Given this question: Who is Taylor Swift? \\n you must answer by choosing only one option above these: A. a snowboard; B. a cat; C. a singer. The correct answer is:\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2d27bf-884b-40f9-bcea-d9fe7464996a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
