{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a268f4e1-3a6d-464d-885b-2053e3143a2f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Dataset, documents, FAISS; retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96db4863-a54b-4d3a-87e2-74c645fcc018",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ðŸ”¹ Load the dataset containing the tuples `(query, correct_answer, distractor_1, distractor_2)` and the one containing the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "094ab87a-0ba8-4878-991e-01515de302e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers', 'correct_answer', 'distractor_1', 'distractor_2'],\n",
       "    num_rows: 82326\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('saracandu/msmarco_modified', split=\"train\", trust_remote_code=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "856a910c-be2a-440f-8cab-c0faac78af84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content='\"Since 2007, the RBA\\'s outstanding reputation has been affected by the \\'Securency\\' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA\\'s employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\"')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
    "\n",
    "loader = HuggingFaceDatasetLoader('saracandu/msmarco_filtered', 'passage_text')\n",
    "documents = loader.load()\n",
    "documents[0] # just to check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045746d4-d0d6-4cf1-b68e-63bebe72f990",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ðŸ”¹ Turn `documents` into a vector database using FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2626e646-9fb4-4235-8858-edb8a898f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# create an instance of the RecursiveCharacterTextSplitter class with specific parameters\n",
    "# (it splits text into chunks of 50 characters each with a 20-character overlap)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "\n",
    "# 'documents' holds the text you want to split, split the text into documents using the text splitter\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e91fa4f-156e-45dc-a6fa-2b89489b891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose an embedding method\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-l6-v2\",  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccbce37-535e-4952-a076-683e009b9d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the documents 'docs' into vectors using the embedding method specified by 'embedding'\n",
    "# the result is stored in a FAISS index:\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# to avoid computing it each time (since the docs won't change), save the result in the storage\n",
    "db.save_local(folder_path=\"faiss_db\", index_name=\"MSMARCO_FaissIndex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17955f64-8650-4095-860e-5b818bd6a833",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ðŸ”¹ Upload the already existing vector database (if it exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecbc054f-ab89-47dd-941f-d002e62abcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-l6-v2\",  \n",
    ")\n",
    "\n",
    "db = FAISS.load_local(\n",
    "    folder_path=\"faiss_db\", # where to find it\n",
    "    embeddings=embeddings, # in which \"embedding language\" it is expressed\n",
    "    index_name=\"MSMARCO_FaissIndex\", # since the folder contains multiple vector databases, specify its name\n",
    "    allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5babae8-e52e-4046-9a29-e886bb5f4b22",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ðŸ”¹ Use it as a `retriever`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b061a230-7d28-4693-89ce-d4fad815925f",
   "metadata": {},
   "source": [
    "**Note:** `'k'=10` specifies the number of documents to retrieve each time `retrieved` is invoked. \n",
    "The default type of search performed is `similarity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e800071b-4968-4c0a-818d-16534f44d153",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(\n",
    "    search_kwargs={'k': 10,}\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f895debf-b05f-4bbf-aa4f-c9f32aa693d9",
   "metadata": {},
   "source": [
    "Why `'k'=10`? Because MSMARCO assigns to each `(query, answer)` pair 10 text passages, and only 1 or 2 of these are truly relevant. \n",
    "In this first step of analysis I chose not to create `len(dataset)` different vector databases, one for each `(query, answer)` pair, but instead to merge all the passages together and store them into an unique vector database. \n",
    "\n",
    "\n",
    "**SE `'k'=10` SBAGLIA ALCUNE RISPOSTE! SE LO ABBASSI A `3` O A `4` NO :)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926bb532-fa37-4cfe-89fa-731ad140276d",
   "metadata": {},
   "source": [
    "# Model part (`Llama-2-7b-chat-hf`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86132ec0-f0d0-4316-b71e-92af9861cda2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ðŸ”¹ Upload the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce89a37-b6e1-48ff-b8fb-621e456392c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not run this unless necessary!\n",
    "\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc6eb0bf-fb11-4952-b14e-809b77898a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8b32b8e0624a03bbc83dd796286fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "#################################################################\n",
    "# Tokenizer\n",
    "#################################################################\n",
    "\n",
    "model_name=\"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "#################################################################\n",
    "# bitsandbytes parameters\n",
    "#################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "#################################################################\n",
    "# Set up quantization config\n",
    "#################################################################\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "#################################################################\n",
    "# Load pre-trained config\n",
    "#################################################################\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58bf643-6ff9-4759-96b3-adf1b9e447aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ðŸ”¹ Pipeline for **thesis** generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06649d5c-6f76-426b-9edb-5fd535376e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "response_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\", # deve matchare la scheda del modello HF o dÃ  errore\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    repetition_penalty=1.5,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=400,\n",
    "    top_p=0.0\n",
    ")\n",
    "\n",
    "response_generation_llm = HuggingFacePipeline(pipeline=response_generation_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702cc29e-5f6a-440e-a5fd-8a5bee3050f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ðŸ”¹ Select a subset of the true dataset as a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61598fb6-67e3-4d06-94bf-892f4330ba0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what is rba',\n",
       " 'was ronald reagan a democrat',\n",
       " 'how long do you need for sydney and surrounding areas',\n",
       " 'price to install tile in shower',\n",
       " 'why conversion observed in body']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select a subset of the queries, just for test:\n",
    "first_queries = dataset['query'][:5]\n",
    "first_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e058aac-3fa5-42b7-95db-809669ac8524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for correct answers and distractors:\n",
    "correct_answers = dataset['correct_answer'][:5]\n",
    "distractors_1 = dataset['distractor_1'][:5]\n",
    "distractors_2 = dataset['distractor_2'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7e39db-091e-4510-800a-0a16f2b89e76",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ðŸ”¹ Merge the true answer and the distractors into a vector, shuffling the order of the elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cab974ea-41f0-44b3-8167-592bbd323320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffles the order of the vector containing the correct answer and the two distractors\n",
    "# returns another vector, shuffled\n",
    "import random\n",
    "\n",
    "def shuffleAnswers(correct_answer, distractor_1, distractor_2):\n",
    "    merge_options = [correct_answer, distractor_1, distractor_2]\n",
    "    random.shuffle(merge_options)\n",
    "    return merge_options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d43a34-fc04-4043-afcb-c8271f6a4e3f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ðŸ”¹ Function to format them properly the retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b38b14ba-0d52-4ca3-806c-a7dd8241e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary function to format properly the output of the retrieval step\n",
    "\n",
    "def format_page_content(documents):\n",
    "    \"\"\"\n",
    "    Formats the list of retrieved documents such that 'page_content', 'Documents', 'metadata' \n",
    "    words are removed and just the true content is kept.\n",
    "    \"\"\"\n",
    "    formatted_output = \"\"\n",
    "    for i, doc in enumerate(documents, start=1):\n",
    "        content = doc.page_content.strip(\" \")\n",
    "        formatted_output += f\"[{i}]: {content}\\n\"\n",
    "    return formatted_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f722aa12-2d2a-4868-83ef-96f01617d4d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ðŸ”¹ PromptTemplate definition and a LLMChain for the **thesis** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7751aac-b970-455b-b2b4-4b8575aad07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template definition\n",
    "# requires question, options (a string containing the possible options) and the context as input variables!\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "    You're a helpful assistant and you are asked to answer a question correctly, given a certain number of options. \n",
    "    Answer with the correct option only and then stop.\n",
    "    Given this question: {question} \\n\n",
    "    You must answer by choosing only one option above these: {option_a}, {option_b}, {option_c}. \\n\n",
    "    Here is context to help: {context} \\n\n",
    "    The correct answer is:\n",
    " \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49ad711e-db35-47d4-8949-ffedc071f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM chain definition\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from operator import itemgetter\n",
    "\n",
    "augmentation = {\"question\": itemgetter(\"question\"),\n",
    "                \"option_a\": itemgetter(\"option_a\"), \n",
    "                \"option_b\": itemgetter(\"option_b\"),\n",
    "                \"option_c\": itemgetter(\"option_c\"),\n",
    "                \"context\": itemgetter(\"context\"), }\n",
    "\n",
    "thesis_chain = augmentation | prompt_template | response_generation_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcc84e0-fa8c-404d-85b4-96be5448a81d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ðŸ”¹ Function that generates the output given the prompt, the question and the set of options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18ca079d-d85c-491f-87ab-2a0dfd626894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thesisGeneration(query, prompt_template, merged):\n",
    "    documents_retrieved = retriever.invoke(query)\n",
    "    formatted_context = format_page_content(documents_retrieved)\n",
    "    \n",
    "    given_answer = thesis_chain.invoke({'question': query, \n",
    "                                     'option_a': merged[0], 'option_b': merged[1], 'option_c': merged[2], \n",
    "                                     'context': formatted_context})\n",
    "    return given_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "493b80e2-2715-4084-bba5-791adc4b65b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the true answer (i.e. remove the unnecessary)\n",
    "\n",
    "def extract_answer(text):\n",
    "    # trova l'indice in cui inizia il testo \"the correct answer is:\"\n",
    "    start_index = text.find(\"The correct answer is:\") + len(\"The correct answer is:\")\n",
    "    # estrai il testo dopo \"The correct answer is:\"\n",
    "    correct_answer_text = text[start_index:].strip()\n",
    "    \n",
    "    return correct_answer_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecf177e-3d1d-4e08-96d3-c9bf9144c986",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ðŸ”¹ Test: how well the thesis alone is able to perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c65767f-ef49-4f37-8a8e-7810529ef08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True answer: ['Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/pipelines/base.py:1167: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/pipelines/base.py:1167: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given answer: Results-Based Accountability\n",
      "****************\n",
      "True answer: ['Yes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/pipelines/base.py:1167: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/pipelines/base.py:1167: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given answer: Option B - No\n",
      "****************\n",
      "True answer: ['20-25 minutes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/pipelines/base.py:1167: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/pipelines/base.py:1167: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given answer: ['Longer']\n",
      "****************\n",
      "True answer: ['$11 to $22 per square foot']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/pipelines/base.py:1167: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/pipelines/base.py:1167: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given answer: They are fatty acids that have one double bond in the fatty acid chain with all of the remainder carbon atoms being single-bonded..\n",
      "****************\n",
      "True answer: ['Due to symptoms in the body']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/pipelines/base.py:1167: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/pipelines/base.py:1167: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given answer: Due to symtions in th ebody\n",
      "****************\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "for i in range(5):\n",
    "    print(f\"True answer: {correct_answers[i]}\")\n",
    "    merged_options = shuffleAnswers(correct_answers[i], distractors_1[i], distractors_2[i])\n",
    "    answers.append(extract_answer(thesisGeneration(first_queries[i], prompt_template, merged_options)))\n",
    "    print(f\"Given answer: {extract_answer(thesisGeneration(first_queries[i], prompt_template, merged_options))}\")\n",
    "    print('****************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8cf43b76-0053-472b-9ced-325492f127f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Results-Based Accountability',\n",
       " 'Option B - No',\n",
       " \"['Longer']\",\n",
       " 'They are fatty acids that have one double bond in the fatty acid chain with all of the remainder carbon atoms being single-bonded..',\n",
       " 'Due to symtions in th ebody']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022cd3d5-2f62-4922-8a6d-39a99b567b3b",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Pipeline for **antithesis** generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2b29f6d5-a6bb-43e2-b45b-a02677767732",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_check_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    repetition_penalty=1.5,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=400,\n",
    "    top_p=0.0\n",
    ")\n",
    "\n",
    "response_check_llm = HuggingFacePipeline(pipeline=response_check_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fc7617-7074-49ec-a1db-2d86416aeca2",
   "metadata": {},
   "source": [
    "## ðŸ”¹ PromptTemplate definition and a LLMChain for the **antithesis** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8451da76-fc21-44f2-81a5-64eabe0ade4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "    You're a helpful assistant and you are asked to check whether or not a question was answered correctly, given a certain number of candidate options and the context. \n",
    "    Answer whether and why you think the answer is correct.\n",
    "    Given this question: {question} \\n \n",
    "    These are the possible options: {option_a}, {option_b}, {option_c} \\n.\n",
    "    The answer that you have to check is {candidate_answer}. \n",
    "    Here is context to help: {context} \\n\n",
    "    Is the answer correct? Why or why not?\n",
    " \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "66388c99-daad-4de1-9188-cafc15857a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM chain definition\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from operator import itemgetter\n",
    "\n",
    "augmentation = {\"question\": itemgetter(\"question\"),\n",
    "                \"option_a\": itemgetter(\"option_a\"), \n",
    "                \"option_b\": itemgetter(\"option_b\"),\n",
    "                \"option_c\": itemgetter(\"option_c\"),\n",
    "                \"candidate_answer\": itemgetter(\"candidate_answer\"),\n",
    "                \"context\": itemgetter(\"context\"), }\n",
    "\n",
    "antithesis_chain = augmentation | prompt_template | response_check_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a643ed8f-0d35-44b4-84cd-293816c40ed4",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Function to generate the antithesis given the question, the thesis, the context and the options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ef1b239-4d92-473a-89a7-42ee0137f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def antithesisGeneration(query, prompt_template, merged, candidate_answer):\n",
    "    documents_retrieved = retriever.invoke(query)\n",
    "    formatted_context = format_page_content(documents_retrieved)\n",
    "    \n",
    "    second_answer = antithesis_chain.invoke({'question': query, \n",
    "                                            'option_a': merged[0], 'option_b': merged[1], 'option_c': merged[2], \n",
    "                                            'candidate_answer': candidate_answer,\n",
    "                                            'context': formatted_context})\n",
    "    return second_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "77d4639f-6f79-4d9e-be69-03b02a2c5b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True answer: ['Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given answer: assistant and you are asked to check whether or not a question was answered correctly, given a certain number of candidate options and the context. \n",
      "    Answer whether and why you think the answer is correct.\n",
      "    Given this question: what is rba \n",
      " \n",
      "    These are the possible options: ['Webbed feet'], ['Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.'], [\"'Other Allowance' which is basically to compensate all and any kind of allowances which is required to be paid at different regions/localities of the various project sites.\"] \n",
      ".\n",
      "    The answer that you have to check is Results-Based Accountability. \n",
      "    Here is context to help: [1]: \"Get To Know Us. RBA is a digital and technology consultancy with roots in strategy, design and technology. Our team of specialists help progressive companies deliver modern digital experiences backed by proven technology engineering. \"\n",
      "[2]: \"Results-Based Accountability\\u00ae (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. Creating Community Impact with RBA. Community impact focuses on conditions of well-being for children, families and the community as a whole that a group of leaders is working collectively to improve.\n",
      "[3]: \"Results-Based Accountability\\u00ae (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can\n",
      "[4]: \"RBA stands for ReBuildable Atomizer. They are rebuildable/reusable versions of the standard disposables items that every vaper knows \\u2013 cartomizers, atomizers, etc. The goal here is to provide a cheaper, more sustainable (and often more efficient) way to get your vapor.\"\n",
      "[5]: \"RBA uses a data-driven, decision-making process to help communities and organizations get beyond talking about problems to taking action to solve problems. It is a simple, common sense framework that everyone can understand. RBA starts with ends and works backward, towards means. The \\u201cend\\u201d or difference you are trying to make looks slightly different if you are working on a broad community level or are focusing on your specific program or organization. RBA improves the lives of\n",
      "[6]: the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;\"\n",
      "[7]: \"RBA Recognized with the 2014 Microsoft US Regional Partner of the ... by PR Newswire. Contract Awarded for supply and support the. Securitisations System used for risk management and analysis. \"\n",
      "[8]: \"vs. NetIQ Identity Manager. Risk-based authentication (RBA) is a method of applying varying levels of stringency to authentication processes based on the likelihood that access to a given system could result in its being compromised. Risk-based authentication can be categorized as either user-dependent or transaction-dependent. User-dependent RBA processes employ the same authentication for every session initiated by a given user; the exact credentials that the site demands depend on who the\n",
      "[9]: \"Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\"\n",
      "[10]: \"A rebuildable atomizer (RBA), often referred to as simply a \\u201crebuildable,\\u201d is just a special type of atomizer used in the Vape Pen and Mod Industry that connects to a personal vaporizer. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven\n",
      " \n",
      "\n",
      "    Is the answer correct? Why or why not?\n",
      "****************\n",
      "True answer: ['Yes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given answer: assistant and you are asked to check whether or not a question was answered correctly, given a certain number of candidate options and the context. \n",
      "    Answer whether and why you think the answer is correct.\n",
      "    Given this question: was ronald reagan a democrat \n",
      " \n",
      "    These are the possible options: ['A contamination which is associated with the food itself and not through other causes of contamination.'], ['50, 55, 60, 65 and 70 Â°C'], ['Yes'] \n",
      ".\n",
      "    The answer that you have to check is Option B - No. \n",
      "    Here is context to help: [1]: \"From Wikipedia, the free encyclopedia. A Reagan Democrat is a traditionally Democratic voter in the United States, especially a white working-class Northerner, who defected from their party to support Republican President Ronald Reagan in either or both the 1980 and 1984 elections. During the 1980 election a dramatic number of voters in the U.S., disillusioned with the economic 'malaise' of the 1970s and the presidency of Jimmy Carter (even more than, four years earlier, Liberal Republican\n",
      "[2]: Liberal Republican Gerald Ford), supported former California governor (and former Democrat) Ronald Reagan.\"\n",
      "[3]: \"I think Ronald Reagan changed the trajectory of America in a way that Richard Nixon did not and in a way that Bill Clinton did not. He put us on a fundamentally different path because the country was ready for it. Ronald Wilson Reagan (/\\u02c8r\\u0252n\\u0259ld \\u02c8w\\u026als\\u0259n \\u02c8re\\u026a\\u0261\\u0259n/ ; February 6, 1911 \\u2013 June 5, 2004) was an American politician, commentator, and actor, who served as the 40th President of the United States from 1981 to 1989.\"\n",
      "[4]: \"Ronald Reagan began his political life in the Democratic Party, but as he became more and more conservative, he ultimately changed to the Republican Party in the early 1960s. Yes, he switched parties in 1962. He said that he did not desert the Democrats but rather they deserted him. Yes, Ronald Reagan was a member of the Democratic Party until he s \\u2026 witched to the Republican Party in 1962, at the age of 51. 8 people found this useful.\"\n",
      "[5]: \"Ronald Wilson Reagan (/\\u02c8r\\u0252n\\u0259ld \\u02c8w\\u026als\\u0259n \\u02c8re\\u026a\\u0261\\u0259n/ ; February 6, 1911 \\u2013 June 5, 2004) was an American politician, commentator, and actor, who served as the 40th President of the United States from 1981 to 1989. I think Ronald Reagan changed the trajectory of America in a way that Richard Nixon did not and in a way that Bill Clinton did not. He put us on a fundamentally different path because the country was ready for it.\"\n",
      "[6]: \"Ronald Wilson Reagan (/\\u02c8r\\u0252n\\u0259ld \\u02c8w\\u026als\\u0259n \\u02c8re\\u026a\\u0261\\u0259n/ ; February 6, 1911 \\u2013 June 5, 2004) was an American politician, commentator, and actor, who served as the 40th President of the United States from 1981 to 1989. I think Ronald Reagan changed the trajectory of America in a way that Richard Nixon did not and in a way that Bill Clinton did not. He put us on a fundamentally different path because the country was ready for it.\"\n",
      "[7]: \"Ronald Wilson Reagan (/\\u02c8r\\u0252n\\u0259ld \\u02c8w\\u026als\\u0259n \\u02c8re\\u026a\\u0261\\u0259n/ ; February 6, 1911 \\u2013 June 5, 2004) was an American politician, commentator, and actor, who served as the 40th President of the United States from 1981 to 1989. I think Ronald Reagan changed the trajectory of America in a way that Richard Nixon did not and in a way that Bill Clinton did not. He put us on a fundamentally different path because the country was ready for it.\"\n",
      "[8]: \"He felt he had fulfilled his campaign pledge of 1980 to restore the great, confident roar of American progress and growth and optimism.. On February 6, 1911, Ronald Wilson Reagan was born to Nelle and John Reagan in Tampico, Illinois. A renewal of national self-confidence by 1984 helped Reagan and Bush win a second term with an unprecedented number of electoral votes. Their victory turned away Democratic challengers Walter F. Mondale and Geraldine Ferraro.\"\n",
      "[9]: \"Ronald Reagan, originally an American actor and politician, became the 40th President of the United States serving from 1981 to 1989. His term saw a restoration of prosperity at home, with the goal of achieving peace through strength abroad. A renewal of national self-confidence by 1984 helped Reagan and Bush win a second term with an unprecedented number of electoral votes. Their victory turned away Democratic challengers Walter F. Mondale and Geraldine Ferraro.\"\n",
      "[10]: \"Ronald Reagan, originally an American actor and politician, became the 40th President of the United States serving from 1981 to 1989. His term saw a restoration of prosperity at home, with the goal of achieving peace through strength abroad. A renewal of national self-confidence by 1984 helped Reagan and Bush win a second term with an unprecedented number of electoral votes. Their victory turned away Democratic challengers Walter F. Mondale and Geraldine Ferraro.\"\n",
      " \n",
      "\n",
      "    Is the answer correct? Why or why not?\n",
      "****************\n",
      "True answer: ['20-25 minutes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given answer: assistant and you are asked to check whether or not a question was answered correctly, given a certain number of candidate options and the context. \n",
      "    Answer whether and why you think the answer is correct.\n",
      "    Given this question: how long do you need for sydney and surrounding areas \n",
      " \n",
      "    These are the possible options: ['Yes'], ['20-25 minutes'], ['Oatmeal, beans, apples, pears, barley and prunes.'] \n",
      ".\n",
      "    The answer that you have to check is ['Longer']. \n",
      "    Here is context to help: [1]: a Few-sydney area recommendations on where to get). them\"\n",
      "[2]: Highway it is 969km from Sydney to Brisbane. That will take 48 and a half days travelling 20km per day. If you cover 30km per day it's over 32 days and 40km a day will get you there in just over 24\"\n",
      "[3]: \"The Sydney central business district, Sydney harbour and outer suburbs from the West. North Sydney 's commercial district. The extensive area covered by urban Sydney is formally divided into more than 300 suburbs for addressing and postal purposes, and administered as 38 local government areas. The Sydney Statistical Division, used for census data, is the unofficial metropolitan area and covers 12,145 km\\u00b2 (4,689 mi\\u00b2). This area includes the Central Coast and Blue Mountains as well as\n",
      "[4]: long enough so that you wake up a couple of hours before landing in Sydney.\"\n",
      "[5]: \"Best Answer: It could take as little as two days but I would recommend at least 4 or 5 days. Driving from Perth to Melbourne I would suggest stops at Norseman, Ceduna and Adelaide (and Melbourne if travelling to Sydney). When driving ensure that you have plenty of supplies (particularly drinking water and petrol). Report Abuse. Driving through the nullaboor from perth to adelaide non stop takes at least 36hours. But they don't really recommend driving at night with al the kangaroos around. So\n",
      "[6]: \"As the Crow Flies. The distance between Sydney and Melbourne is 714 kilometers (444 miles) . Sydney (Population: 4,627,345) is located in New South Wales. Melbourne (Population: 4,137,432) is located in Victoria. New South Wales and Victoria are both in Australia. Australia is located\"\n",
      "[7]: \"The distance between Sydney and Melbourne is 714 kilometers (444 miles) . Sydney (Population: 4,627,345) is located in New South Wales. Melbourne (Population: 4,137,432) is located in Victoria. New South Wales and Victoria are both in Australia. Australia is located\"\n",
      "[8]: \"Sydney, New South Wales, Australia is located in a coastal basin bordered by the Pacific Ocean to the east, the Blue Mountains to the west, the Hawkesbury River to the north and the Woronora Plateau to the south. The Sydney Statistical Division, used for census data, is the unofficial metropolitan area and covers 12,145 km\\u00b2 (4,689 mi\\u00b2). This area includes the Central Coast and Blue Mountains as well as broad swathes of national park and other non-urban land.\"\n",
      "[9]: \"Sydney is the capital city of the Australian state of New South Wales, and Australia's largest city. A week in Sydney will help you see many of the sights of Sydney and its surrounds, and understand the city and its culture. If you're up for exploring the area by bike (one of the best ways to do so as much of it is parkland), take the train to Concord West station on the Northern Line (red line on the Sydney Trains map-about 20-25 minutes from the city on a direct train).\"\n",
      "[10]: \"Sydney lies on a submergent coastline, where the ocean level has risen to flood deep river valleys (rias) carved in the sandstone. There are more than 70 harbour and ocean beaches, including the famous Bondi Beach, in the urban area. Sydney's urban area covers 1,788 km\\u00b2 (690 mi\\u00b2). The Sydney Statistical Division, used for census data, is the unofficial metropolitan area and covers 12,145 km\\u00b2 (4,689 mi\\u00b2). This area includes the Central Coast and Blue Mountains as well as\n",
      " \n",
      "\n",
      "    Is the answer correct? Why or why not?\n",
      "****************\n",
      "True answer: ['$11 to $22 per square foot']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given answer: assistant and you are asked to check whether or not a question was answered correctly, given a certain number of candidate options and the context. \n",
      "    Answer whether and why you think the answer is correct.\n",
      "    Given this question: price to install tile in shower \n",
      " \n",
      "    These are the possible options: ['$11 to $22 per square foot'], ['They are fatty acids that have one double bond in the fatty acid chain with all of the remainder carbon atoms being single-bonded.'], ['Honolulu'] \n",
      ".\n",
      "    The answer that you have to check is They are fatty acids that have one double bond in the fatty acid chain with all of the remainder carbon atoms being single-bonded... \n",
      "    Here is context to help: [1]: \"1 For tile shower installation, you\\u2019re probably looking at a minimum cost of at least $1,500, although intricate, labor-heavy designs and add-ons (such as new faucets and a tub) could easy bring the price up to $5,000 or more.\"\n",
      "[2]: \"In regards to tile installation costs, consumers can expect to pay an average of $25 per square foot, depending on the grade of material that is used. For a medium-sized shower, the price can cost about $2,000. Tile installation materials include:\"\n",
      "[3]: \"In regards to tile installation costs, consumers can expect to pay an average of $25 per square foot, depending on the grade of material that is used. For a medium-sized shower, the price can cost about $2,000. Tile installation materials include: \"\n",
      "[4]: \"In regards to tile installation costs, consumers can expect to pay an average of $25 per square foot, depending on the grade of material that is used. For a medium-sized shower, the price can cost about $2,000. Tile installation materials include: \"\n",
      "[5]: \"In regards to tile installation costs, consumers can expect to pay an average of $25 per square foot, depending on the grade of material that is used. For a medium-sized shower, the price can cost about $2,000. Tile installation materials include:\"\n",
      "[6]: \"In regards to tile installation costs, consumers can expect to pay an average of $25 per square foot, depending on the grade of material that is used. For a medium-sized shower, the price can cost about $2,000.\"\n",
      "[7]: \"1 Higher-end tile such as granite or marble is going to cost more than a standard ceramic tile. 2  On average, plan on spending anywhere from $2,500 to as much as $5,000 or more for a standard 3\\u2032 x 5\\u2032 shower. 3  Showers can widely vary in size, but most of the time the cost will be within this price range. 1 A standard floor can cost around $150 to $350, while tile can be factored into the overall square footage cost. 2  Extra features can be added within a shower as well. 3  These\n",
      "[8]: \"1 Higher-end tile such as granite or marble is going to cost more than a standard ceramic tile. 2  On average, plan on spending anywhere from $2,500 to as much as $5,000 or more for a standard 3\\u2032 x 5\\u2032 shower. 3  Showers can widely vary in size, but most of the time the cost will be within this price range. \"\n",
      "[9]: \"1 Higher-end tile such as granite or marble is going to cost more than a standard ceramic tile. 2  On average, plan on spending anywhere from $2,500 to as much as $5,000 or more for a standard 3\\u2032 x 5\\u2032 shower. 3  Showers can widely vary in size, but most of the time the cost will be within this price range.\"\n",
      "[10]: \"1 Higher-end tile such as granite or marble is going to cost more than a standard ceramic tile. 2  On average, plan on spending anywhere from $2,500 to as much as $5,000 or more for a standard 3\\u2032 x 5\\u2032 shower. 3  Showers can widely vary in size, but most of the time the cost will be within this price range.\"\n",
      " \n",
      "\n",
      "    Is the answer correct? Why or why not?\n",
      "****************\n",
      "True answer: ['Due to symptoms in the body']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given answer: assistant and you are asked to check whether or not a question was answered correctly, given a certain number of candidate options and the context. \n",
      "    Answer whether and why you think the answer is correct.\n",
      "    Given this question: why conversion observed in body \n",
      " \n",
      "    These are the possible options: ['Due to symptoms in the body'], ['A chief engineer is responsible for all operations and maintenance that has to do with any and all engineering equipment throughout the entire ship.', 'The chief engineer is responsible for the technical supervision of the development, production or operation of an engineering project for a multinational corporation, a major company or a government institution.'], ['Nigeria/Cameroon'] \n",
      ".\n",
      "    The answer that you have to check is Due to symtions in th ebody. \n",
      "    Here is context to help: [1]: \"Conversion disorder is a type of somatoform disorder where physical symptoms or signs are present that cannot be explained by a medical condition. Very importantly, unlike factitious disorders and malingering, the symptoms of somatoform disorders are not intentional or under conscious control of the patient.\"\n",
      "[2]: of red blood cells. The conversion of values relies on the accurate measurement of the packed cell volume.\"\n",
      "[3]: to the body and returns carbon dioxide and relatively deoxygenated blood to the heart for transfer to the lungs.\"\n",
      "[4]: to body tissues by transporting the blood throughout the body).\"\n",
      "[5]: and converted to energy, which helps to prevent the break down of muscle tissue.\"\n",
      "[6]: \"Conversion disorder is a mental condition in which a person has blindness, paralysis, or other nervous system (neurologic) symptoms that cannot be explained by medical evaluation.\"\n",
      "[7]: into smaller and smaller components which can be absorbed and assimilated into the body.\"\n",
      "[8]: converted into a substance suitable for absorption and assimilation into the body. 2. the function or power of digesting food\"\n",
      "[9]: converted into a substance suitable for absorption and assimilation into the body. 2. the function or power of digesting food\"\n",
      "[10]: hormone into the blood, which in turn can affect different target tissues.\"\n",
      " \n",
      "\n",
      "    Is the answer correct? Why or why not?\n",
      "****************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ant_answers = []\n",
    "for i in range(5):\n",
    "    print(f\"True answer: {correct_answers[i]}\")\n",
    "    merged_options = shuffleAnswers(correct_answers[i], distractors_1[i], distractors_2[i])\n",
    "    ant_answers.append(extract_answer(antithesisGeneration(first_queries[i], prompt_template, merged_options, answers[i])))\n",
    "    print(f\"Given answer: {extract_answer(antithesisGeneration(first_queries[i], prompt_template, merged_options, answers[i]))}\")\n",
    "    print('****************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d91dae-9707-453c-9d80-b142af804fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e4ef66-777b-441b-b7bb-4889d26223f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b14bd9b-e042-49bb-8bd1-a06a5aa5b303",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(f\"True answer: {correct_answers[i]}\")\n",
    "    merged_options = shuffleAnswers(correct_answers[i], distractors_1[i], distractors_2[i])\n",
    "    print(f\"Given answer: {extract_answer(antithesisGeneration(first_queries[i], prompt_template, merged_options))}\")\n",
    "    print('****************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e78a69-c4b4-42c3-996f-194957c66661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Use regular expression to extract the correct answer\n",
    "correct_answer_match = re.search(r\"The correct answer is:\\n\\s*\\([A-Z]\\)\", given_answer)\n",
    "\n",
    "if correct_answer_match:\n",
    "    correct_answer = correct_answer_match.group(1)\n",
    "    print(f\"The correct answer is: {correct_answer}\")\n",
    "else:\n",
    "    print(\"Correct answer not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0adff7a-5aa1-447f-a969-ca5061f8125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from operator import itemgetter\n",
    "\n",
    "augmentation = {\"options\": itemgetter(\"options\"), \"context\": itemgetter(\"context\"), \n",
    "                \"question\": itemgetter(\"question\"), \"candidate_answer\": itemgetter(\"candidate_answer\")}\n",
    "llm_chain = augmentation | prompt_template | response_check_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a40859-5dca-4f2e-b4e5-8f15fddc2dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain.invoke({'question': \"What is the name of the potion allowing to change appearance?\", \n",
    "                  'options': \"A. Felix Felicis; B. Polyjuice Potion; C. Amortentia\", \n",
    "                  'context': formatted_context,'candidate_answer': 'B'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67911749-0c01-4745-bf15-2b7f5cd98139",
   "metadata": {},
   "outputs": [],
   "source": [
    "Il problema Ã¨ che non hai ancora capito come formattare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba485875-13a8-42f0-81cc-63f0ffa17a47",
   "metadata": {},
   "source": [
    "##  Transformers pipeline (and zero-shot):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c8766c-5898-45f8-8a7c-8a52a797435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You're a helpful assistant and you are asked to answer a question correctly, given a certain number of options. Answer with the correct option only and then stop.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Given this question: Who is Taylor Swift? \\n you must answer by choosing only one option above these: A. a snowboard; B. a cat; C. a singer. The correct answer is:\"},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f9acd8-20a9-4848-b7f5-2ee6f67ceb12",
   "metadata": {},
   "source": [
    "##  Transformers AutoModelForCausalLM  (and zero-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9be92ef-df0c-43ac-aba8-b2da824f09fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You're a helpful assistant and you are asked to answer a question correctly, given a certain number of options. Answer with the correct option only and then stop.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Given this question: Who is Taylor Swift? \\n you must answer by choosing only one option above these: A. a snowboard; B. a cat; C. a singer. The correct answer is:\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2d27bf-884b-40f9-bcea-d9fe7464996a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
