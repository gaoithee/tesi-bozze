{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a268f4e1-3a6d-464d-885b-2053e3143a2f",
   "metadata": {},
   "source": [
    "# Dataset, documents, FAISS; retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96db4863-a54b-4d3a-87e2-74c645fcc018",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üîπ Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "094ab87a-0ba8-4878-991e-01515de302e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'options', 'answer', 'type', 'level', 'selected_passages'],\n",
       "    num_rows: 352\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('saracandu/filtered_hotpotQA', split=\"train\", trust_remote_code=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3232ca1-3306-450e-9654-a9a47738f61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Who was inducted into the Rock and Roll Hall of Fame, David Lee Roth or Cia Berg?',\n",
       " 'options': \"['Cia Berg', 'David Lee Roth']\",\n",
       " 'answer': 'David Lee Roth',\n",
       " 'type': 'comparison',\n",
       " 'level': 'medium',\n",
       " 'selected_passages': 'Cia Berg (born 2 December 1963), now known as Cia Soro, is a Swedish television presenter and singer. She was at one time the lead singer of the Swedish rock band Whale, who released the single \"Hobo Humpin\\' Slobo Babe\". David Lee Roth (born October 10, 1954) is an American rock vocalist, musician, songwriter, actor, author, and former radio personality. In 2007, he was inducted into the Rock and Roll Hall of Fame.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b214f052-ff9a-4049-8e6e-5a056438e9d3",
   "metadata": {},
   "source": [
    "## üîπ Select a subset of the true dataset as a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccbd160-6698-430a-b1db-feafcd5799d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffles the order of the vector containing the correct answer and the two distractors\n",
    "# returns another vector, shuffled\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Definisci una funzione di pulizia per rimuovere caratteri non validi\n",
    "def clean_text(text):\n",
    "    return re.sub(r\"[^\\w\\s.,!?\\-:;()]+\", '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ca2a69e4-be19-4f96-b4de-ab80ee00a8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_examples = 200\n",
    "\n",
    "# select a subset of the queries, just for test:\n",
    "first_queries = dataset['question'][:N_examples]\n",
    "\n",
    "# same for correct answers and distractors:\n",
    "correct_answers = dataset['answer'][:N_examples]\n",
    "possibilities = dataset['options'][:N_examples]\n",
    "# and for the sources:\n",
    "sources = dataset['selected_passages'][:N_examples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926bb532-fa37-4cfe-89fa-731ad140276d",
   "metadata": {},
   "source": [
    "# Model loading and dataset selection (for testing purposes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86132ec0-f0d0-4316-b71e-92af9861cda2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ‚ñ™Ô∏è Upload the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce89a37-b6e1-48ff-b8fb-621e456392c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not run this unless necessary!\n",
    "\n",
    "# from huggingface_hub import login\n",
    "# login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc6eb0bf-fb11-4952-b14e-809b77898a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00bb8f97aab148bfa5dd0b51f4e600ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "#################################################################\n",
    "# Tokenizer\n",
    "#################################################################\n",
    "\n",
    "model_name=\"nvidia/Llama3-ChatQA-1.5-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "#################################################################\n",
    "# bitsandbytes parameters\n",
    "#################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "#################################################################\n",
    "# Set up quantization config\n",
    "#################################################################\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "#################################################################\n",
    "# Load pre-trained config\n",
    "#################################################################\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792539a3-875f-4027-87fe-89c2ca2c3222",
   "metadata": {},
   "source": [
    "## ‚ñ™Ô∏è Create a model that allow to work with `guidance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d17128d-7b94-4e6a-9ba2-95a8dc2d4107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/guidance/chat.py:73: UserWarning: Chat template {{ bos_token }}{%- if messages[0]['role'] == 'system' -%}{% set loop_messages = messages[1:] %}{%- else -%}{% set loop_messages = messages %}{% endif %}System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions based on the context. The assistant should also indicate when the answer cannot be found in the context.\n",
      "\n",
      "{% for message in loop_messages %}{%- if message['role'] == 'user' -%}User: {{ message['content'].strip() + '\n",
      "\n",
      "' }}{%- else -%}Assistant: {{ message['content'].strip() + '\n",
      "\n",
      "' }}{%- endif %}{% if loop.last and message['role'] == 'user' %}Assistant:{% endif %}{% endfor %} was unable to be loaded directly into guidance.\n",
      "                        Defaulting to the ChatML format which may not be optimal for the selected model. \n",
      "                        For best results, create and pass in a `guidance.ChatTemplate` subclass for your model.\n",
      "  warnings.warn(f\"\"\"Chat template {chat_template} was unable to be loaded directly into guidance.\n"
     ]
    }
   ],
   "source": [
    "from guidance import models, select\n",
    "new_model = models.Transformers(model, tokenizer, temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b312ecad-c2c4-4791-bc82-bfaae5f5fa04",
   "metadata": {},
   "source": [
    "# Thesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcc84e0-fa8c-404d-85b4-96be5448a81d",
   "metadata": {},
   "source": [
    "## üîπ Function that generates the output given the prompt, the question and the set of options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "129c9abb-24b2-4903-80d6-c2ee22e3abdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def thesisGeneration(query, options, sources):\n",
    "    \n",
    "    options = ast.literal_eval(options)\n",
    "    first_option = clean_text(options[0])\n",
    "    second_option = clean_text(options[1])\n",
    "    \n",
    "    ans = new_model + f'''\\\n",
    "    Question: {query}. Choose to either {options} given the following supporting\n",
    "    Context: {sources}\n",
    "    Choice: {select([first_option, second_option], name=\"choice\")}'''\n",
    "    \n",
    "    \n",
    "    return str(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dc34eb1d-d402-4373-b8c5-e21cd51902b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(text):\n",
    "    # Trova l'indice in cui inizia il testo \"Why or why not the answer is correct:\"\n",
    "    start_index = text.find(\"Choice:\")\n",
    "\n",
    "    \n",
    "    # Se l'indice √® stato trovato, estrai la risposta corretta\n",
    "    if start_index != -1:\n",
    "        start_index += len(\"Choice:\")\n",
    "        # Estrai il testo dopo \"Why or why not the answer is correct:\"\n",
    "        correct_answer_text = text[start_index:].strip()\n",
    "        return correct_answer_text\n",
    "    else:\n",
    "        return \"The correct answer could not be found.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecf177e-3d1d-4e08-96d3-c9bf9144c986",
   "metadata": {},
   "source": [
    "## üîπ Test: how well the thesis alone is able to perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b4cf535d-2517-452e-8ef3-821dc8524cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>    Question: In which decade was the Ivel Z3 and the Prol√≥gica CP-400 developed in?. Choose to either [&#x27;1980s&#x27;, &#x27;1990s&#x27;] given the following supporting\n",
       "    Context: Ivel Z3 was an Apple IIe compatible computer developed by Ivasim in 1980s. In the middle of 1984 a Brazilian company called Prol√≥gica, which made its own versions of 8 bits US computers, brought to the Brazilian market a new equipment for its personal computer series called &quot;CP&quot; (shorten of Personal Computer in Portuguese).\n",
       "    Choice: <span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>198</span>0s</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answers = []\n",
    "for i in range(N_examples):\n",
    "    answers.append(extract_answer(thesisGeneration(first_queries[i], possibilities[i], sources[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4815bcc6-48ba-40de-b2a4-a9510da209fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Arthurs Magazine', 'Henri Leconte', 'The Wolfhounds', 'no', 'yes']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e222ef24-3231-4ee1-ae92-4b1b67551500",
   "metadata": {},
   "source": [
    "# Antithesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9e21293c-f26c-4c36-b5b0-dc3cffe4c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    repetition_penalty=1.5,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=400,\n",
    "    top_p=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "33184819-91d2-42f0-97ab-44daab45f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def query_model(\n",
    "        system_message,\n",
    "        user_message,\n",
    "        temperature = 0.0,\n",
    "        max_length=1024\n",
    "        ):\n",
    "\n",
    "    user_message = \"Question: \" + user_message + \" Correct answer:\"\n",
    "    messages = [\n",
    "        {\"role\": \"System\", \"content\": system_message},\n",
    "        {\"role\": \"User\", \"content\": user_message},\n",
    "        ]\n",
    "    prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "        )\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    sequences = pipeline(\n",
    "        prompt,\n",
    "        do_sample=False,\n",
    "        top_p=0.0,\n",
    "        temperature=temperature,\n",
    "        #num_return_sequences=1,\n",
    "        eos_token_id=terminators,\n",
    "        max_new_tokens=max_length,\n",
    "        return_full_text=False,\n",
    "        pad_token_id=pipeline.model.config.eos_token_id\n",
    "    )\n",
    "\n",
    "    answer = sequences[0]['generated_text']\n",
    "\n",
    "    return answer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c29e2fd3-113f-4780-8ccf-a8b9e82e850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently tested:\n",
    "system_message = \"\"\"\n",
    "    You are an helpful AI assistant. You are asked to determine the most correct answer for a given question, provided a set of possible options.\n",
    "    You are also provided with a candidate answer and the context. \n",
    "    Your goal is to decree which is the correct answer to the question by explaining why you think so. Use the context to ground your statements.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "user_message = \"\"\"\n",
    "    Question: {question}?\n",
    "    Candidate answer: {candidate_answer}\n",
    "    Context: {context}\n",
    "    Which of the following options: {options} is the most proper answer for the question? Why? \n",
    "    Think step by step but choose only one of the options: {options} by explicitly reporting which one.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5585df88-b285-4eae-a2a5-aeae3f075c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "    You are an helpful AI assistant. You are asked to determine the most correct answer for a given question, provided a set of possible options.\n",
    "    You are also provided with a candidate answer that you can check given the context.\n",
    "    Your goal is to decree which is the correct answer to the question by explaining why you think so. Use the context to ground your statements.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "user_message = \"\"\"\n",
    "    Question: {question}?\n",
    "    Candidate answer: {candidate_answer}\n",
    "    Context: {context}\n",
    "    Which of the following options: {options} is the most proper answer for the question? Why? \n",
    "    Think step by step but choose only one of the options: {options} by explicitly reporting which one.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e74fc4de-76d3-4fa4-bbfe-3a2e207c75ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "    You are an AI assistant asked to check whether or not a given question was asked correctly given an attempt of answer. \n",
    "    For each question there is a set of options from which the answer is taken.\n",
    "    You can you give suggestions and advices on whether the given answer is correct or not. \n",
    "    You can use the context to ground your advice.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "user_message = \"\"\"\n",
    "    Question: {question}\n",
    "    Possible options: {options}\n",
    "    Attempt of answer: {candidate_answer}\n",
    "    Context: {context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a643ed8f-0d35-44b4-84cd-293816c40ed4",
   "metadata": {},
   "source": [
    "## üî∏ Function to generate the antithesis given the question, the thesis, the context and the options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b5ffee32-0ea6-439d-9ec5-1440dd58b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def antithesisGeneration(query, merged, candidate_answer, sources):\n",
    "    merged = ast.literal_eval(merged)\n",
    "    second_answer = query_model(system_message.format(context = sources),\n",
    "    user_message.format(question=query, options = merged, candidate_answer = candidate_answer, context = sources,), max_length=1024)\n",
    "    return second_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ec71c35b-0598-429f-a99d-df9e82da06b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/pipelines/base.py:1167: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ant_answers = []\n",
    "for i in range(N_examples):\n",
    "    ant_answers.append(antithesisGeneration(first_queries[i], possibilities[i], answers[i], sources[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "93695a06-214d-4bc0-9f41-219192027cac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Arhutrs\\' Magzine\"\\nThe text clearly states that this publication came out earlier than its competitor did',\n",
       " 'The option \"Lecont\" would have been better if it had included that information about him winning three grand slam tournaments compared against Starks single victory',\n",
       " '\"The wolf hound\"',\n",
       " 'The two men were not related or even from similar backgrounds as they lived at different times; therefore it would make sense that their interests did differ somewhat too',\n",
       " \"The new pornographer isn't american\",\n",
       " 'The two locations mentioned belong to \"NEW YORK CITY\" as per information available at https://en.wikipedia.org/wiki/100_Park_Avenue',\n",
       " \"'Alexsnder ford'\\nHe has been alive longer than pablotrapera\",\n",
       " 'The reference text states that ‚ÄúFirst For Woman‚Äùis indeed as it claims,a publication aimed at females.The other option,‚ÄùNo‚Äù,implies otherwise.It‚Äôs important therefore,to consider carefully whether or not this statement accurately reflects reality.In light Of all these points,the best choice seems clear :‚ÄúYes‚Äù.',\n",
       " \"'director'\",\n",
       " '\"The Saima gesture\".']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ant_answers[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f600cfb8-6a35-4baa-8236-c02c2909868c",
   "metadata": {},
   "source": [
    "# Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0e9cfd78-a0e0-4867-9193-08e0329a4a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def_answers = [\"the correct option is \" + clean_text(correct_answer) for correct_answer in correct_answers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1edb95-c057-4d4b-8a40-c24f578e6c6b",
   "metadata": {},
   "source": [
    "## üî∫ OPTION 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "13c2cd84-9f13-411c-8717-bb4767c13c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from operator import itemgetter\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"You are a multiple-choice question answering assistant.\n",
    "You have a suggestion on which answer is the most appropriate that you are strongly suggested to follow.\n",
    "Choose the most proper option between {options} that best matches with the suggestion. You also have sources reported in case you need them. \n",
    "\n",
    "Question: {question}\n",
    "Context: The suggestion that is generated when {candidate_answer} is considered the correct answer is {critique}\n",
    "Sources: {context}\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "augmentation = {\"question\": itemgetter(\"question\"),\n",
    "                \"options\": itemgetter(\"options\"), \n",
    "                \"candidate_answer\": itemgetter(\"candidate_answer\"),\n",
    "                \"critique\": itemgetter(\"critique\"),\n",
    "                \"context\": itemgetter(\"context\"), }\n",
    "\n",
    "synthesis_chain = augmentation | prompt_template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "69a338b2-1304-48d7-bc37-ca90d1627456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesisGeneration(query, prompt_template, merged, candidate_answer, critique, sources):\n",
    "    merged = ast.literal_eval(merged)\n",
    "    augmented_prompt = synthesis_chain.invoke({'question': query, \n",
    "                                            'options': merged,\n",
    "                                            'candidate_answer': candidate_answer,\n",
    "                                            'critique': critique,\n",
    "                                            'context': sources})\n",
    "\n",
    "    normal_string = clean_text(augmented_prompt.text)\n",
    "    ans = new_model + normal_string + select([clean_text(merged[0]), clean_text(merged[1])])\n",
    "    return str(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f5ddbd32-1f6c-4b87-9be2-555e4301a57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(text):\n",
    "    # Trova l'indice in cui inizia il testo \"Why or why not the answer is correct:\"\n",
    "    start_index = text.find(\"assistant:\\n\")\n",
    "\n",
    "    \n",
    "    # Se l'indice √® stato trovato, estrai la risposta corretta\n",
    "    if start_index != -1:\n",
    "        start_index += len(\"assistant:\\n\")\n",
    "        # Estrai il testo dopo \"Why or why not the answer is correct:\"\n",
    "        correct_answer_text = text[start_index:].strip()\n",
    "        return correct_answer_text\n",
    "    else:\n",
    "        return \"The correct answer could not be found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e0748f3b-f505-4e82-b221-f881519be4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>you are a multiplechoice question answering assistant.\n",
       "you have a suggestion on which answer is the most appropriate that you are strongly suggested to follow.\n",
       "choose the most proper option between 1980s, 1990s that best matches with the suggestion. you also have sources reported in case you need them. \n",
       "\n",
       "question: in which decade was the ivel z3 and the prol√≥gica cp400 developed in?\n",
       "context: the suggestion that is generated when 1980s is considered the correct answer is the correct option is 1980s\n",
       "sources: ivel z3 was an apple iie compatible computer developed by ivasim in 1980s. in the middle of 1984 a brazilian company called prol√≥gica, which made its own versions of 8 bits us computers, brought to the brazilian market a new equipment for its personal computer series called cp (shorten of personal computer in portuguese).\n",
       "\n",
       "assistant:\n",
       "<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>198</span>0s</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "syn_answers = []\n",
    "for i in range(N_examples):\n",
    "    syn_answers.append(extract_answer(\n",
    "        synthesisGeneration(\n",
    "            first_queries[i], prompt_template, possibilities[i], answers[i], \n",
    "            def_answers[i], sources[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8bac5ba8-b7a3-4630-a291-cb3397b5f502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arthurs magazine',\n",
       " 'henri leconte',\n",
       " 'the wolfhounds',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'new york city',\n",
       " 'aleksander ford',\n",
       " 'yes',\n",
       " 'director',\n",
       " 'the saimaa gesture']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_answers[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbe3e9a-3c25-43b1-a3c7-3161bcf77342",
   "metadata": {},
   "source": [
    "## üî∫ OPTION 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4a19051a-0f5d-4a69-9541-a3e729c3ae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def synthesisGeneration2(query, options, candidate_answer, critique, sources):    \n",
    "    \n",
    "    options = ast.literal_eval(options)\n",
    "    first_option = clean_text(options[0])\n",
    "    second_option = clean_text(options[1])\n",
    "    \n",
    "    ans = new_model + f'''\\\n",
    "    Question: {query}. Choose the most proper option between {options} that best matches with the suggestion.\n",
    "    You have a suggestion on which answer is the most appropriate that you are strongly suggested to follow.\n",
    "    Context: The suggestion is {critique}\n",
    "\n",
    "    You also have sources reported in case you need them. \n",
    "    Sources: {sources}\n",
    "    \n",
    "    Choice: {select([first_option, second_option], name=\"choice\")}\n",
    "'''\n",
    "    \n",
    "    return str(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "22163578-1fd1-4006-be3e-861221d946c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer2(text):\n",
    "    # Trova l'indice in cui inizia il testo \"Why or why not the answer is correct:\"\n",
    "    start_index = text.find(\"Choice:\")\n",
    "\n",
    "    \n",
    "    # Se l'indice √® stato trovato, estrai la risposta corretta\n",
    "    if start_index != -1:\n",
    "        start_index += len(\"Choice:\")\n",
    "        # Estrai il testo dopo \"Why or why not the answer is correct:\"\n",
    "        correct_answer_text = text[start_index:].strip()\n",
    "        return correct_answer_text\n",
    "    else:\n",
    "        return \"The correct answer could not be found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1ae9ea3e-6e78-4de0-a0d0-252a7c79313e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>    Question: In which decade was the Ivel Z3 and the Prol√≥gica CP-400 developed in?. Choose the most proper option between [&#x27;1980s&#x27;, &#x27;1990s&#x27;] that best matches with the suggestion.\n",
       "    You have a suggestion on which answer is the most appropriate that you are strongly suggested to follow.\n",
       "    Context: The suggestion is the correct option is 1980s\n",
       "\n",
       "    You also have sources reported in case you need them. \n",
       "    Sources: Ivel Z3 was an Apple IIe compatible computer developed by Ivasim in 1980s. In the middle of 1984 a Brazilian company called Prol√≥gica, which made its own versions of 8 bits US computers, brought to the Brazilian market a new equipment for its personal computer series called &quot;CP&quot; (shorten of Personal Computer in Portuguese).\n",
       "    \n",
       "    Choice: <span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>198</span>0s\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "syn_answers2 = []\n",
    "for i in range(N_examples):\n",
    "    syn_answers2.append(extract_answer2(\n",
    "        synthesisGeneration2(\n",
    "            first_queries[i], possibilities[i], answers[i], \n",
    "            def_answers[i], sources[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bd30b2b7-c04e-4bef-b03e-9a0dab2f1397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Arthurs Magazine',\n",
       " 'Henri Leconte',\n",
       " 'The Wolfhounds',\n",
       " 'no',\n",
       " 'no',\n",
       " 'New York City',\n",
       " 'Aleksander Ford',\n",
       " 'yes',\n",
       " 'director',\n",
       " 'The Saimaa Gesture']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_answers2[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8170bc0d-1182-4c31-b33d-65fe64e1c385",
   "metadata": {},
   "source": [
    "## üî∫ Function to generate the synthesis given literally everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "161ace47-b366-411c-ad2f-ae5f3d57b04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {\n",
    "    'query': first_queries,\n",
    "    'correct': correct_answers,\n",
    "    'thesis': answers,\n",
    "    'antithesis': ant_answers,\n",
    "    'synthesis-1': syn_answers,\n",
    "    'synthesis-2': syn_answers2,\n",
    "    'context': sources\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e63baefa-86bc-41b3-a38a-807bc973b8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>correct</th>\n",
       "      <th>thesis</th>\n",
       "      <th>antithesis</th>\n",
       "      <th>synthesis-1</th>\n",
       "      <th>synthesis-2</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which magazine was started first Arthur's Maga...</td>\n",
       "      <td>Arthur's Magazine</td>\n",
       "      <td>Arthurs Magazine</td>\n",
       "      <td>\"Arhutrs' Magzine\"\\nThe text clearly states th...</td>\n",
       "      <td>arthurs magazine</td>\n",
       "      <td>arthurs magazine</td>\n",
       "      <td>Arthur's Magazine (1844‚Äì1846) was an American ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which tennis player won more Grand Slam titles...</td>\n",
       "      <td>Jonathan Stark</td>\n",
       "      <td>Henri Leconte</td>\n",
       "      <td>The option \"Lecont\" would have been better if ...</td>\n",
       "      <td>henri leconte</td>\n",
       "      <td>jonathan stark</td>\n",
       "      <td>Henri Leconte (born 4 July 1963) is a former F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which band was founded first, Hole, the rock b...</td>\n",
       "      <td>The Wolfhounds</td>\n",
       "      <td>The Wolfhounds</td>\n",
       "      <td>\"The wolf hound\"</td>\n",
       "      <td>the wolfhounds</td>\n",
       "      <td>courtney love</td>\n",
       "      <td>The Wolfhounds are an indie pop/noise pop band...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Were Pavel Urysohn and Leonid Levin known for ...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>The two men were not related or even from simi...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Leonid Anatolievich Levin ( ; Russian: –õ–µ–æ–Ω–∏ÃÅ–¥...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Are both The New Pornographers and Kings of Le...</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>The new pornographer isn't american</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>Kings of Leon is an American rock band that fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query            correct  \\\n",
       "0  Which magazine was started first Arthur's Maga...  Arthur's Magazine   \n",
       "1  Which tennis player won more Grand Slam titles...     Jonathan Stark   \n",
       "2  Which band was founded first, Hole, the rock b...     The Wolfhounds   \n",
       "3  Were Pavel Urysohn and Leonid Levin known for ...                 no   \n",
       "4  Are both The New Pornographers and Kings of Le...                 no   \n",
       "\n",
       "             thesis                                         antithesis  \\\n",
       "0  Arthurs Magazine  \"Arhutrs' Magzine\"\\nThe text clearly states th...   \n",
       "1     Henri Leconte  The option \"Lecont\" would have been better if ...   \n",
       "2    The Wolfhounds                                   \"The wolf hound\"   \n",
       "3                no  The two men were not related or even from simi...   \n",
       "4               yes                The new pornographer isn't american   \n",
       "\n",
       "        synthesis-1       synthesis-2  \\\n",
       "0  arthurs magazine  arthurs magazine   \n",
       "1     henri leconte    jonathan stark   \n",
       "2    the wolfhounds     courtney love   \n",
       "3                no                no   \n",
       "4               yes                no   \n",
       "\n",
       "                                             context  \n",
       "0  Arthur's Magazine (1844‚Äì1846) was an American ...  \n",
       "1  Henri Leconte (born 4 July 1963) is a former F...  \n",
       "2  The Wolfhounds are an indie pop/noise pop band...  \n",
       "3  Leonid Anatolievich Levin ( ; Russian: –õ–µ–æ–Ω–∏ÃÅ–¥...  \n",
       "4  Kings of Leon is an American rock band that fo...  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb65952-bf02-4bbb-9b00-2bd5348e6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['query'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "61349820-ad5d-4dff-a1d0-a08547f727d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('baseline-2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d280eeea-c69b-42d2-856b-1651fc77b083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>correct</th>\n",
       "      <th>thesis</th>\n",
       "      <th>antithesis</th>\n",
       "      <th>synthesis-1</th>\n",
       "      <th>synthesis-2</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which magazine was started first Arthur's Maga...</td>\n",
       "      <td>arthurs magazine</td>\n",
       "      <td>arthurs magazine</td>\n",
       "      <td>\"Arhutrs' Magzine\"\\nThe text clearly states th...</td>\n",
       "      <td>arthurs magazine</td>\n",
       "      <td>arthurs magazine</td>\n",
       "      <td>Arthur's Magazine (1844‚Äì1846) was an American ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which tennis player won more Grand Slam titles...</td>\n",
       "      <td>jonathan stark</td>\n",
       "      <td>henri leconte</td>\n",
       "      <td>The option \"Lecont\" would have been better if ...</td>\n",
       "      <td>henri leconte</td>\n",
       "      <td>jonathan stark</td>\n",
       "      <td>Henri Leconte (born 4 July 1963) is a former F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which band was founded first, Hole, the rock b...</td>\n",
       "      <td>the wolfhounds</td>\n",
       "      <td>the wolfhounds</td>\n",
       "      <td>\"The wolf hound\"</td>\n",
       "      <td>the wolfhounds</td>\n",
       "      <td>courtney love</td>\n",
       "      <td>The Wolfhounds are an indie pop/noise pop band...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Were Pavel Urysohn and Leonid Levin known for ...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>The two men were not related or even from simi...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Leonid Anatolievich Levin ( ; Russian: –õ–µ–æ–Ω–∏ÃÅ–¥...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Are both The New Pornographers and Kings of Le...</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>The new pornographer isn't american</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>Kings of Leon is an American rock band that fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query           correct  \\\n",
       "0  Which magazine was started first Arthur's Maga...  arthurs magazine   \n",
       "1  Which tennis player won more Grand Slam titles...    jonathan stark   \n",
       "2  Which band was founded first, Hole, the rock b...    the wolfhounds   \n",
       "3  Were Pavel Urysohn and Leonid Levin known for ...                no   \n",
       "4  Are both The New Pornographers and Kings of Le...                no   \n",
       "\n",
       "             thesis                                         antithesis  \\\n",
       "0  arthurs magazine  \"Arhutrs' Magzine\"\\nThe text clearly states th...   \n",
       "1     henri leconte  The option \"Lecont\" would have been better if ...   \n",
       "2    the wolfhounds                                   \"The wolf hound\"   \n",
       "3                no  The two men were not related or even from simi...   \n",
       "4               yes                The new pornographer isn't american   \n",
       "\n",
       "        synthesis-1       synthesis-2  \\\n",
       "0  arthurs magazine  arthurs magazine   \n",
       "1     henri leconte    jonathan stark   \n",
       "2    the wolfhounds     courtney love   \n",
       "3                no                no   \n",
       "4               yes                no   \n",
       "\n",
       "                                             context  \n",
       "0  Arthur's Magazine (1844‚Äì1846) was an American ...  \n",
       "1  Henri Leconte (born 4 July 1963) is a former F...  \n",
       "2  The Wolfhounds are an indie pop/noise pop band...  \n",
       "3  Leonid Anatolievich Levin ( ; Russian: –õ–µ–æ–Ω–∏ÃÅ–¥...  \n",
       "4  Kings of Leon is an American rock band that fo...  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Funzione per rimuovere le quadre e ottenere solo il contenuto\n",
    "def remove_brackets(s):\n",
    "    return s.strip(\"[] \")\n",
    "\n",
    "# Definisci una funzione di pulizia per rimuovere caratteri non validi\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s.,!?\\'\"\\-:;()]+', '', text)  # Rimuove i caratteri speciali\n",
    "    text = re.sub(r\"['\\\"-]\", '', text)  # Rimuove apostrofi, virgolette e trattini\n",
    "    text = text.lower()  # Converte in minuscolo\n",
    "    return text\n",
    "\n",
    "# Applica la funzione alla colonna 'correct answer'\n",
    "df['correct'] = df['correct'].apply(clean_text)\n",
    "df['thesis'] = df['thesis'].apply(clean_text)\n",
    "df['synthesis-1'] = df['synthesis-1'].apply(clean_text)\n",
    "df['synthesis-2'] = df['synthesis-2'].apply(clean_text)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2625980e-cca0-430a-be54-04cb76e2a1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Arthur\\'s Magazine (1844‚Äì1846) was an American literary periodical published in Philadelphia in the 19th century. Edited by T.S. Arthur, it featured work by Edgar A. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G. Spear, and others. In May 1846 it was merged into \"Godey\\'s Lady\\'s Book\". First for Women is a woman\\'s magazine published by Bauer Media Group in the USA. The magazine was started in 1989. It is based in Englewood Cliffs, New Jersey. In 2011 the circulation of the magazine was 1,310,696 copies.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['context'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "38e03a0e-1c19-41e0-9ae3-d8348bcdc3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 154\n",
      "Number of non-matches: 46\n"
     ]
    }
   ],
   "source": [
    "# Conta quante righe combaciano e quante no\n",
    "matches = 0\n",
    "non_matches = 0\n",
    "which_ones = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    correct_answer = str(row['correct']).strip()\n",
    "    thesis = str(row['thesis']).strip()\n",
    "    \n",
    "    if correct_answer == thesis:\n",
    "        matches += 1\n",
    "    else:\n",
    "        non_matches += 1\n",
    "        which_ones.append(\"thesis: {}, Correct: {}\".format(thesis, correct_answer))\n",
    "\n",
    "print(f\"Number of matches: {matches}\")\n",
    "print(f\"Number of non-matches: {non_matches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2e0614ea-7259-4f87-943c-eaa830e211b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 144\n",
      "Number of non-matches: 56\n"
     ]
    }
   ],
   "source": [
    "# Conta quante righe combaciano e quante no\n",
    "matches = 0\n",
    "non_matches = 0\n",
    "which_ones_syn = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    correct_answer = str(row['correct']).strip()\n",
    "    thesis = str(row['synthesis-1']).strip()\n",
    "    \n",
    "    if correct_answer == thesis:\n",
    "        matches += 1\n",
    "    else:\n",
    "        # print(\"Synthesis: {}, Correct: {}\".format(thesis, correct_answer))\n",
    "        non_matches += 1\n",
    "        which_ones_syn.append(\"Synthesis: {}, Correct: {}\".format(thesis, correct_answer))\n",
    "\n",
    "print(f\"Number of matches: {matches}\")\n",
    "print(f\"Number of non-matches: {non_matches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9d2f67fe-03d3-4c25-8619-3e63861caa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 153\n",
      "Number of non-matches: 47\n"
     ]
    }
   ],
   "source": [
    "# Conta quante righe combaciano e quante no\n",
    "matches = 0\n",
    "non_matches = 0\n",
    "which_ones_syn = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    correct_answer = str(row['correct']).strip()\n",
    "    thesis = str(row['synthesis-2']).strip()\n",
    "    \n",
    "    if correct_answer == thesis:\n",
    "        matches += 1\n",
    "    else:\n",
    "        # print(\"Synthesis: {}, Correct: {}\".format(thesis, correct_answer))\n",
    "        non_matches += 1\n",
    "        which_ones_syn.append(\"Synthesis: {}, Correct: {}\".format(thesis, correct_answer))\n",
    "\n",
    "print(f\"Number of matches: {matches}\")\n",
    "print(f\"Number of non-matches: {non_matches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "28ee7db1-9333-4df7-b3bb-9865a27a883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset = df.loc[~df['correct'].isin(['yes', 'no'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1da947cd-e5d1-4ca8-9e42-22238c963e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.9493670886076"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "180/(180+57)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27622337-968e-4176-afa4-f4630c3cddeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72.15189873417721"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "171/(171+66)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93baec3-a750-468a-a7ca-3197901581d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
