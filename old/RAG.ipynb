{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a268f4e1-3a6d-464d-885b-2053e3143a2f",
   "metadata": {},
   "source": [
    "# Dataset, documents, FAISS; retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96db4863-a54b-4d3a-87e2-74c645fcc018",
   "metadata": {},
   "source": [
    "## üîπ Load the dataset containing the tuples `(query, correct_answer, distractor_1, distractor_2)` and the one containing the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "094ab87a-0ba8-4878-991e-01515de302e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers', 'correct_answer', 'distractor_1', 'distractor_2'],\n",
       "    num_rows: 82326\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('saracandu/msmarco_modified', split=\"train\", trust_remote_code=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "856a910c-be2a-440f-8cab-c0faac78af84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content='\"Since 2007, the RBA\\'s outstanding reputation has been affected by the \\'Securency\\' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA\\'s employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\"')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
    "\n",
    "loader = HuggingFaceDatasetLoader('saracandu/msmarco_filtered', 'passage_text')\n",
    "documents = loader.load()\n",
    "documents[0] # just to check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045746d4-d0d6-4cf1-b68e-63bebe72f990",
   "metadata": {},
   "source": [
    "## üîπ Turn `documents` into a vector database using FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2626e646-9fb4-4235-8858-edb8a898f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# create an instance of the RecursiveCharacterTextSplitter class with specific parameters\n",
    "# (it splits text into chunks of 50 characters each with a 20-character overlap)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "\n",
    "# 'documents' holds the text you want to split, split the text into documents using the text splitter\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e91fa4f-156e-45dc-a6fa-2b89489b891c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 3.0.0.dev0, however, your version is 2.6.1. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# choose an embedding method\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\",  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccbce37-535e-4952-a076-683e009b9d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the documents 'docs' into vectors using the embedding method specified by 'embedding'\n",
    "# the result is stored in a FAISS index:\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# to avoid computing it each time (since the docs won't change), save the result in the storage\n",
    "db.save_local(folder_path=\"faiss_db\", index_name=\"MSMARCO_FaissIndex_MPNet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17955f64-8650-4095-860e-5b818bd6a833",
   "metadata": {},
   "source": [
    "## üîπ Upload the already existing vector database (if it exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbc054f-ab89-47dd-941f-d002e62abcf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/multi-qa-MiniLM-L6-dot-v1\",  \n",
    ")\n",
    "\n",
    "db = FAISS.load_local(\n",
    "    folder_path=\"faiss_db\", # where to find it\n",
    "    embeddings=embeddings, # in which \"embedding language\" it is expressed\n",
    "    index_name=\"MSMARCO_FaissIndex_MiniLM\", # since the folder contains multiple vector databases, specify its name\n",
    "    allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5babae8-e52e-4046-9a29-e886bb5f4b22",
   "metadata": {},
   "source": [
    "## üîπ Use it as a `retriever`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b061a230-7d28-4693-89ce-d4fad815925f",
   "metadata": {},
   "source": [
    "**Note:** `'k'=10` specifies the number of documents to retrieve each time `retrieved` is invoked. \n",
    "The default type of search performed is `similarity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e800071b-4968-4c0a-818d-16534f44d153",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(\n",
    "    search_kwargs={'k': 1,}\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f895debf-b05f-4bbf-aa4f-c9f32aa693d9",
   "metadata": {},
   "source": [
    "Why `'k'=10`? Because MSMARCO assigns to each `(query, answer)` pair 10 text passages, and only 1 or 2 of these are truly relevant. \n",
    "In this first step of analysis I chose not to create `len(dataset)` different vector databases, one for each `(query, answer)` pair, but instead to merge all the passages together and store them into an unique vector database. \n",
    "\n",
    "\n",
    "**SE `'k'=10` SBAGLIA ALCUNE RISPOSTE! SE LO ABBASSI A `3` O A `4` NO :)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926bb532-fa37-4cfe-89fa-731ad140276d",
   "metadata": {},
   "source": [
    "# Model part (`Llama-2-7b-chat-hf`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86132ec0-f0d0-4316-b71e-92af9861cda2",
   "metadata": {},
   "source": [
    "## ‚ñ™Ô∏è Upload the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce89a37-b6e1-48ff-b8fb-621e456392c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not run this unless necessary!\n",
    "\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6eb0bf-fb11-4952-b14e-809b77898a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "#################################################################\n",
    "# Tokenizer\n",
    "#################################################################\n",
    "\n",
    "model_name=\"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "#################################################################\n",
    "# bitsandbytes parameters\n",
    "#################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "#################################################################\n",
    "# Set up quantization config\n",
    "#################################################################\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "#################################################################\n",
    "# Load pre-trained config\n",
    "#################################################################\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d17128d-7b94-4e6a-9ba2-95a8dc2d4107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from outlines import models\n",
    "\n",
    "new_model = models.Transformers(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0545fee1-7acd-4c85-8f01-5cff7fdc7556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import outlines\n",
    "\n",
    "prompt = \"\"\"You are a sentiment-labelling assistant.\n",
    "Is the following review positive or negative?\n",
    "\n",
    "Review: This restaurant is just awesome!\n",
    "\"\"\"\n",
    "\n",
    "generator = outlines.generate.choice(new_model, [\"Positive\", \"Negative\"])\n",
    "answer = generator(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021dac4c-df3d-4e50-b09d-5314ee8bd317",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702cc29e-5f6a-440e-a5fd-8a5bee3050f6",
   "metadata": {},
   "source": [
    "## ‚ñ™Ô∏è Select a subset of the true dataset as a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61598fb6-67e3-4d06-94bf-892f4330ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a subset of the queries, just for test:\n",
    "first_queries = dataset['query'][:200]\n",
    "# first_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e058aac-3fa5-42b7-95db-809669ac8524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for correct answers and distractors:\n",
    "correct_answers = dataset['correct_answer'][:200]\n",
    "distractors_1 = dataset['distractor_1'][:200]\n",
    "distractors_2 = dataset['distractor_2'][:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0973bd-82ec-48f3-98a1-8f603bc80149",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_answers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7e39db-091e-4510-800a-0a16f2b89e76",
   "metadata": {},
   "source": [
    "## ‚ñ™Ô∏è Merge the true answer and the distractors into a vector, shuffling the order of the elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab974ea-41f0-44b3-8167-592bbd323320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffles the order of the vector containing the correct answer and the two distractors\n",
    "# returns another vector, shuffled\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Definisci una funzione di pulizia per rimuovere caratteri non validi\n",
    "def clean_text(text):\n",
    "    return re.sub(r'[^\\w\\s.,!?\\'\"\\-:;()]+', '', text)\n",
    "\n",
    "# Funzione per mescolare le risposte\n",
    "def shuffleAnswers(correct_answer, distractor_1, distractor_2):\n",
    "    # Applica la pulizia a ciascun elemento\n",
    "    correct_answer_cleaned = clean_text(correct_answer)\n",
    "    distractor_1_cleaned = clean_text(distractor_1)\n",
    "    distractor_2_cleaned = clean_text(distractor_2)\n",
    "    \n",
    "    # Unisci le opzioni pulite\n",
    "    merge_options = [correct_answer_cleaned, distractor_1_cleaned, distractor_2_cleaned]\n",
    "    \n",
    "    # Mescola le opzioni\n",
    "    random.shuffle(merge_options)\n",
    "    \n",
    "    return merge_options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d43a34-fc04-4043-afcb-c8271f6a4e3f",
   "metadata": {},
   "source": [
    "## ‚ñ™Ô∏è Function to format them properly the retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38b14ba-0d52-4ca3-806c-a7dd8241e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary function to format properly the output of the retrieval step\n",
    "\n",
    "def format_page_content(documents):\n",
    "    \"\"\"\n",
    "    Formats the list of retrieved documents such that 'page_content', 'Documents', 'metadata' \n",
    "    words are removed and just the true content is kept.\n",
    "    \"\"\"\n",
    "    formatted_output = \"\"\n",
    "    for i, doc in enumerate(documents, start=1):\n",
    "        content = doc.page_content.strip(\" \")\n",
    "        formatted_output += f\"[{i}]: {content}\\n\"\n",
    "    return formatted_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f722aa12-2d2a-4868-83ef-96f01617d4d7",
   "metadata": {},
   "source": [
    "## üîπ PromptTemplate definition and a LLMChain for the **thesis** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7751aac-b970-455b-b2b4-4b8575aad07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template definition\n",
    "# requires question, options (a string containing the possible options) and the context as input variables!\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "    You're a helpful assistant and you are asked to answer a question correctly, given a certain number of options. \n",
    "    Answer with the correct option only and then stop.\n",
    "    Given this question: {question} you have to answer given only one of the following options: {option_a}, {option_b}, {option_c}. \\n\n",
    "    Here is context to help: {context} \\n\n",
    "    The correct answer is:\n",
    " \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ad711e-db35-47d4-8949-ffedc071f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM chain definition\n",
    "from operator import itemgetter\n",
    "\n",
    "augmentation = {\"question\": itemgetter(\"question\"),\n",
    "                \"option_a\": itemgetter(\"option_a\"), \n",
    "                \"option_b\": itemgetter(\"option_b\"),\n",
    "                \"option_c\": itemgetter(\"option_c\"),\n",
    "                \"context\": itemgetter(\"context\"), }\n",
    "\n",
    "thesis_chain = augmentation | prompt_template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df44c274-1fea-484d-925c-4c0729fd9079",
   "metadata": {},
   "outputs": [],
   "source": [
    "thesis_chain.invoke({'question': 'query', 'option_a': 'a', 'option_b': 'b', 'option_c': 'c', 'context': 'something'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcc84e0-fa8c-404d-85b4-96be5448a81d",
   "metadata": {},
   "source": [
    "## üîπ Function that generates the output given the prompt, the question and the set of options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ca079d-d85c-491f-87ab-2a0dfd626894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import outlines\n",
    "\n",
    "def thesisGeneration(query, merged):\n",
    "    documents_retrieved = retriever.invoke(query)\n",
    "    formatted_context = format_page_content(documents_retrieved)\n",
    "    augmented_prompt = thesis_chain.invoke({'question': query, 'option_a': merged[0], 'option_b': merged[1], 'option_c': merged[2], 'context': formatted_context})\n",
    "    normal_string = clean_text(augmented_prompt.text)\n",
    "    options = [merged[0], merged[1], merged[2]]\n",
    "    generator = outlines.generate.choice(new_model, options)\n",
    "    answer = generator(normal_string)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecf177e-3d1d-4e08-96d3-c9bf9144c986",
   "metadata": {},
   "source": [
    "## üîπ Test: how well the thesis alone is able to perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c65767f-ef49-4f37-8a8e-7810529ef08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "for i in range(200):\n",
    "    merged_options = shuffleAnswers(correct_answers[i], distractors_1[i], distractors_2[i])\n",
    "    answers.append(thesisGeneration(first_queries[i], merged_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf43b76-0053-472b-9ced-325492f127f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fc7617-7074-49ec-a1db-2d86416aeca2",
   "metadata": {},
   "source": [
    "## üî∏ PromptTemplate definition and a LLMChain for the **antithesis** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310428aa-b58f-4b51-8c56-839d19951d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "antithesis_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    repetition_penalty=1.5,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=400,\n",
    "    top_p=0.0\n",
    ")\n",
    "\n",
    "antithesis_llm = HuggingFacePipeline(pipeline=antithesis_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8451da76-fc21-44f2-81a5-64eabe0ade4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "    You're a helpful assistant and you are asked to check whether or not a question was answered correctly, given a certain number of candidate options and the context. \n",
    "    Given this question: {question} you have to check is the following candidate answer: {candidate_answer}.\n",
    "    You have to answer by saying 'Yes' if it is correct and 'No' otherwise, then explain why you think so. \n",
    "    Think carefully of the all the possible options: {option_a}, {option_b}, {option_c} and say if one of them seems to you more appropriate than the candidate.\n",
    "    Here is context to help: {context} \\n\n",
    "    Why or why not the answer is correct:\n",
    " \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66388c99-daad-4de1-9188-cafc15857a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM chain definition\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from operator import itemgetter\n",
    "\n",
    "augmentation = {\"question\": itemgetter(\"question\"),\n",
    "                \"option_a\": itemgetter(\"option_a\"), \n",
    "                \"option_b\": itemgetter(\"option_b\"),\n",
    "                \"option_c\": itemgetter(\"option_c\"),\n",
    "                \"candidate_answer\": itemgetter(\"candidate_answer\"),\n",
    "                \"context\": itemgetter(\"context\"), }\n",
    "\n",
    "antithesis_chain = augmentation | prompt_template | antithesis_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a643ed8f-0d35-44b4-84cd-293816c40ed4",
   "metadata": {},
   "source": [
    "## üî∏ Function to generate the antithesis given the question, the thesis, the context and the options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef1b239-4d92-473a-89a7-42ee0137f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def antithesisGeneration(query, prompt_template, merged, candidate_answer):\n",
    "    documents_retrieved = retriever.invoke(query)\n",
    "    formatted_context = format_page_content(documents_retrieved)\n",
    "    \n",
    "    second_answer = antithesis_chain.invoke({'question': query, \n",
    "                                            'option_a': merged[0], 'option_b': merged[1], 'option_c': merged[2], \n",
    "                                            'candidate_answer': candidate_answer,\n",
    "                                            'context': formatted_context})\n",
    "    return second_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821f2559-ab56-4769-811e-569be6a609b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer_ant(text):\n",
    "    # Trova l'indice in cui inizia il testo \"Why or why not the answer is correct:\"\n",
    "    start_index = text.find(\"Why or why not the answer is correct:\")\n",
    "    \n",
    "    # Se l'indice √® stato trovato, estrai la risposta corretta\n",
    "    if start_index != -1:\n",
    "        start_index += len(\"Why or why not the answer is correct:\")\n",
    "        # Estrai il testo dopo \"Why or why not the answer is correct:\"\n",
    "        correct_answer_text = text[start_index:].strip()\n",
    "        return correct_answer_text\n",
    "    else:\n",
    "        return \"The correct answer could not be found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d4639f-6f79-4d9e-be69-03b02a2c5b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "ant_answers = []\n",
    "for i in range(200):\n",
    "    # print(f\"True answer: {correct_answers[i]}\")\n",
    "    merged_options = shuffleAnswers(correct_answers[i], distractors_1[i], distractors_2[i])\n",
    "    ant_answers.append(extract_answer_ant(antithesisGeneration(first_queries[i], prompt_template, merged_options, answers[i])))\n",
    "    # print(f\"Given answer: {extract_answer_ant(antithesisGeneration(first_queries[i], prompt_template, merged_options, answers[i]))}\")\n",
    "    # print('****************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679251ac-6945-4ff2-9b89-0c349662bc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "ant_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1edb95-c057-4d4b-8a40-c24f578e6c6b",
   "metadata": {},
   "source": [
    "## üî∫ PromptTemplate definition and a LLMChain for the **synthesis** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6091cee6-41d3-4164-804e-f6c492e88745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "    You're a helpful assistant and you are asked to answer a certain question, given a certain number of candidate options and the context.\n",
    "    You are also provided with an initial response and its critique, that could enforce or not the first opinion.\n",
    "    Make a reasonable synthesis of these two opinions, but answer by outputting exactly one of the answer options only.\n",
    "    Given this question: {question} you have to answer given only one of the following options: {option_a}, {option_b}, {option_c}. \\n\n",
    "    The answer that you have to check is {candidate_answer} and this is its critique: {critique}. \n",
    "    If the critique is not happy with the previous answer, correct it with another of the available options.\n",
    "    Here is context to help: {context} \\n\n",
    "    The answer is:\n",
    " \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d4d68c-5e0e-4818-89cb-93e1731ec3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM chain definition\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from operator import itemgetter\n",
    "\n",
    "augmentation = {\"question\": itemgetter(\"question\"),\n",
    "                \"option_a\": itemgetter(\"option_a\"), \n",
    "                \"option_b\": itemgetter(\"option_b\"),\n",
    "                \"option_c\": itemgetter(\"option_c\"),\n",
    "                \"candidate_answer\": itemgetter(\"candidate_answer\"),\n",
    "                \"critique\": itemgetter(\"critique\"),\n",
    "                \"context\": itemgetter(\"context\"), }\n",
    "\n",
    "synthesis_chain = augmentation | prompt_template "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8170bc0d-1182-4c31-b33d-65fe64e1c385",
   "metadata": {},
   "source": [
    "## üî∫ Function to generate the synthesis given literally everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bef727-d240-4511-a556-0d2c19db4fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesisGeneration(query, prompt_template, merged, candidate_answer, critique):\n",
    "    documents_retrieved = retriever.invoke(query)\n",
    "    formatted_context = format_page_content(documents_retrieved)\n",
    "    \n",
    "    augmented_prompt = synthesis_chain.invoke({'question': query, \n",
    "                                            'option_a': merged[0], 'option_b': merged[1], 'option_c': merged[2], \n",
    "                                            'candidate_answer': candidate_answer,\n",
    "                                            'critique': critique,\n",
    "                                            'context': formatted_context})\n",
    "\n",
    "    normal_string = clean_text(augmented_prompt.text)\n",
    "    options = [merged[0], merged[1], merged[2]]\n",
    "    generator = outlines.generate.choice(new_model, options)\n",
    "    final_answer = generator(normal_string)\n",
    "    \n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4286d7-99cf-4f0c-a320-d9b3c6f84863",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "syn_answers = []\n",
    "for i in range(200):\n",
    "    merged_options = shuffleAnswers(correct_answers[i], distractors_1[i], distractors_2[i])\n",
    "    syn_answers.append(synthesisGeneration(first_queries[i], prompt_template, merged_options, answers[i], ant_answers[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f3176f-7f6b-4c55-a4da-c984771bca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_new = {'query': first_queries, 'correct answer': correct_answers, 'thesis': answers, 'antithesis': ant_answers, 'synthesis': syn_answers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae959b96-7e8f-445e-8ddb-94e0a49116e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(dataset_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612fcd8b-db5f-4d4a-b816-b043a28d2435",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee61af5-3cfd-4aa9-971e-81a8d7d44831",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('first_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3741b36-c57c-4d5a-b435-d9fefa71ab6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
