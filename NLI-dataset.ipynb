{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "76cb9a39-6012-4ea4-b1fd-85efdfbb1480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b5dbec56564053b5d52f1c32ac94f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "693d2533-817a-4e8a-aa4a-ea3a43a3729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_message(question, options):\n",
    "    options_str = '\", \"'.join(options)\n",
    "    content = f\"\"\"\n",
    "    You are asked to create two opposite statements from a question that is given to you.\n",
    "    The idea is that they have to be used as multiple-choices answers for the given question.\n",
    "    Insert also \"than\" when asked for a comparison.\n",
    "\n",
    "    Here's an example:\n",
    "    Question: \"Which magazine was started first, Arthur's Magazine or First for Women?\"\n",
    "    Options: [\"Arthur's Magazine\", \"First for Women\"]\n",
    "    Assistant: [\"Arthur's Magazine was started before First for Women\", \"First for Women was started before Arthur's Magazine\"]\n",
    "\n",
    "    Now do the same for this question: \"{question}\", where options: [\"{options_str}\"].\n",
    "    Assistant:\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "    \n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6119f7c8-4db5-43b6-a9ed-e9da6401ad49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful AI assistant.'},\n",
       " {'role': 'user',\n",
       "  'content': '\\n    You are asked to create two opposite statements from a question that is given to you.\\n    The idea is that they have to be used as multiple-choices answers for the given question.\\n    Insert also \"than\" when asked for a comparison.\\n\\n    Here\\'s an example:\\n    Question: \"Which magazine was started first, Arthur\\'s Magazine or First for Women?\"\\n    Options: [\"Arthur\\'s Magazine\", \"First for Women\"]\\n    Assistant: [\"Arthur\\'s Magazine was started before First for Women\", \"First for Women was started before Arthur\\'s Magazine\"]\\n\\n    Now do the same for this question: \"Which tennis player won more Grand Slam titles, Henri Leconte or Jonathan Stark?\", where options: [\"Henri Leconte\", \"Jonathan Stark\"].\\n    Assistant:\\n    '}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Which tennis player won more Grand Slam titles, Henri Leconte or Jonathan Stark?\"\n",
    "options = [\"Henri Leconte\", \"Jonathan Stark\"]\n",
    "messages = create_message(question, options)\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cc0dffa7-4616-415e-b391-a9853615de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"do_sample\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dcf6cce5-042f-442e-bf7b-9950a00fd57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"Henri Leconte won more Grand Slam titles than Jonathan Stark\", \"Jonathan Stark won more Grand Slam titles than Henri Leconte\"]\n"
     ]
    }
   ],
   "source": [
    "output = pipe(messages, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9691f0d-97cf-4e5d-b112-e42c2d4913ea",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "98cfb06c-f675-401e-9237-9cc16a98be1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|█████████████████████████████| 304k/304k [00:00<00:00, 597kB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb221c53c32f4f90899a1b841036bd94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('saracandu/filtered_hotpotQA', split='train')\n",
    "dataset\n",
    "\n",
    "questions = dataset['question']\n",
    "options = dataset['options']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "cb51954a-be13-4333-90b5-d83c15c076c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "prompts = []\n",
    "for i in range(352):\n",
    "  options[i] = ast.literal_eval(options[i])\n",
    "  prompts.append(create_message(questions[i], options[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ebc45234-3a83-4b85-8943-7caca6a3793d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/pipelines/base.py:1167: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "outs = []\n",
    "for i in range(352):\n",
    "    output = pipe(prompts[i], **generation_args)\n",
    "    outs.append(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "624c88be-ea2f-4392-8e3c-2e0eaf058656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' [\"Arthur\\'s Magazine was started before First for Women\", \"First for Women was started before Arthur\\'s Magazine\"]'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2a846a-54ce-4866-80db-5e2ebf9a5f0a",
   "metadata": {},
   "source": [
    "# cure them and put them in a dataset to use them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a0f9918e-09ac-484b-b83f-b8bab625422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_assistant_label2(input_string):\n",
    "    # Verifica se la stringa inizia con 'Assistant:'\n",
    "    if input_string.startswith(' '):\n",
    "        # Rimuovi 'Assistant:' e gli spazi bianchi che seguono\n",
    "        return input_string[len(' '):].strip()\n",
    "    return input_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "01cec510-1f21-4426-9eb1-5955ffbc7d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(352):\n",
    "    outs[i] = remove_assistant_label2(outs[i])\n",
    "    # outs[i] = rimuovi_virgolette_interne(outs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1281a41d-88f7-4fb4-bb22-3a009d8d1388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"Philip Terzian is an American journalist and has been Literary Editor of The Weekly Standard\", \"Derek Sherinian is an American journalist and has been Literary Editor of The Weekly Standard\"]'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs[129]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b4f49c40-8d16-40d8-a096-3212dc3e8330",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(352):\n",
    "    outs[i] = ast.literal_eval(outs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f4b59761-b42b-41ba-a0b5-f1190cc6d464",
   "metadata": {},
   "outputs": [],
   "source": [
    "nli1 = []\n",
    "nli2 = []\n",
    "\n",
    "for i in range(352):\n",
    "    nli1.append(outs[i][0])\n",
    "    nli2.append(outs[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6a5297ee-43b8-449e-a994-32c796da5616",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = dataset['answer']\n",
    "questionType = dataset['type']\n",
    "level = dataset['level']\n",
    "passages = dataset['selected_passages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "73b138c4-9dcb-4d31-b4f4-b6bf95ade3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Crea un dizionario con i vettori\n",
    "dati = {\n",
    "    'question': questions,\n",
    "    'answer': answer,\n",
    "    'options': options,\n",
    "    'first nli': nli1,\n",
    "    'second nli': nli2,\n",
    "    'type': questionType,\n",
    "    'level': level,\n",
    "    'passages': passages\n",
    "}\n",
    "\n",
    "# Crea il DataFrame utilizzando il dizionario\n",
    "df = pd.DataFrame(dati)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b4b1e289-5872-4b31-89f9-3b9e08847b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>options</th>\n",
       "      <th>first nli</th>\n",
       "      <th>second nli</th>\n",
       "      <th>type</th>\n",
       "      <th>level</th>\n",
       "      <th>passages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>Are Melissa and Darmera vegatables</td>\n",
       "      <td>no</td>\n",
       "      <td>[yes, no]</td>\n",
       "      <td>Melissa and Darmera are not vegetables</td>\n",
       "      <td>Melissa and Darmera are vegetables</td>\n",
       "      <td>comparison</td>\n",
       "      <td>easy</td>\n",
       "      <td>Melissa is a genus of perennial herbs in the L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>Which takes place farther east, The Mosuo Sist...</td>\n",
       "      <td>The Mosuo Sisters</td>\n",
       "      <td>[The Silent Historian, The Mosuo Sisters]</td>\n",
       "      <td>The Silent Historian takes place farther east ...</td>\n",
       "      <td>The Mosuo Sisters take place farther east than...</td>\n",
       "      <td>comparison</td>\n",
       "      <td>medium</td>\n",
       "      <td>The Silent Historian (original title: \"Het zwi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>Were Yellowcard and For Against both American ...</td>\n",
       "      <td>yes</td>\n",
       "      <td>[yes, no]</td>\n",
       "      <td>Yes, Yellowcard and For Against were both Amer...</td>\n",
       "      <td>No, Yellowcard and For Against were not both A...</td>\n",
       "      <td>comparison</td>\n",
       "      <td>medium</td>\n",
       "      <td>Yellowcard was an American pop punk band that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>Were the buildings at 270 Park Avenue and 100 ...</td>\n",
       "      <td>no</td>\n",
       "      <td>[yes, no]</td>\n",
       "      <td>Yes, the buildings at 270 Park Avenue and 100 ...</td>\n",
       "      <td>No, the buildings at 270 Park Avenue and 100 E...</td>\n",
       "      <td>comparison</td>\n",
       "      <td>medium</td>\n",
       "      <td>100 East 53rd Street (formerly known as 610 Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>Which American film, Dragonslayer or Swiss Fam...</td>\n",
       "      <td>Swiss Family Robinson</td>\n",
       "      <td>[Dragonslayer, Swiss Family Robinson]</td>\n",
       "      <td>Dragonslayer was shot outside London</td>\n",
       "      <td>Swiss Family Robinson was shot outside London</td>\n",
       "      <td>comparison</td>\n",
       "      <td>medium</td>\n",
       "      <td>Dragonslayer is a 1981 American fantasy film d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question                 answer  \\\n",
       "347                 Are Melissa and Darmera vegatables                     no   \n",
       "348  Which takes place farther east, The Mosuo Sist...      The Mosuo Sisters   \n",
       "349  Were Yellowcard and For Against both American ...                    yes   \n",
       "350  Were the buildings at 270 Park Avenue and 100 ...                     no   \n",
       "351  Which American film, Dragonslayer or Swiss Fam...  Swiss Family Robinson   \n",
       "\n",
       "                                       options  \\\n",
       "347                                  [yes, no]   \n",
       "348  [The Silent Historian, The Mosuo Sisters]   \n",
       "349                                  [yes, no]   \n",
       "350                                  [yes, no]   \n",
       "351      [Dragonslayer, Swiss Family Robinson]   \n",
       "\n",
       "                                             first nli  \\\n",
       "347             Melissa and Darmera are not vegetables   \n",
       "348  The Silent Historian takes place farther east ...   \n",
       "349  Yes, Yellowcard and For Against were both Amer...   \n",
       "350  Yes, the buildings at 270 Park Avenue and 100 ...   \n",
       "351               Dragonslayer was shot outside London   \n",
       "\n",
       "                                            second nli        type   level  \\\n",
       "347                 Melissa and Darmera are vegetables  comparison    easy   \n",
       "348  The Mosuo Sisters take place farther east than...  comparison  medium   \n",
       "349  No, Yellowcard and For Against were not both A...  comparison  medium   \n",
       "350  No, the buildings at 270 Park Avenue and 100 E...  comparison  medium   \n",
       "351      Swiss Family Robinson was shot outside London  comparison  medium   \n",
       "\n",
       "                                              passages  \n",
       "347  Melissa is a genus of perennial herbs in the L...  \n",
       "348  The Silent Historian (original title: \"Het zwi...  \n",
       "349  Yellowcard was an American pop punk band that ...  \n",
       "350  100 East 53rd Street (formerly known as 610 Le...  \n",
       "351  Dragonslayer is a 1981 American fantasy film d...  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "07d441c8-ed5d-41ca-9aa4-e07417463318",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('nli_augmented.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e83c19b-b2d2-470d-8e28-6af8b55ec616",
   "metadata": {},
   "source": [
    "# Add NLI scores - part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af43c214-61af-4a60-b8d8-d8cd2af81e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>options</th>\n",
       "      <th>first nli</th>\n",
       "      <th>second nli</th>\n",
       "      <th>type</th>\n",
       "      <th>level</th>\n",
       "      <th>passages</th>\n",
       "      <th>scores 1</th>\n",
       "      <th>scores 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Which magazine was started first, Arthur's Mag...</td>\n",
       "      <td>Arthur's Magazine</td>\n",
       "      <td>[\"Arthur's Magazine\", 'First for Women']</td>\n",
       "      <td>Arthur's Magazine was started before First for...</td>\n",
       "      <td>First for Women was started before Arthur's Ma...</td>\n",
       "      <td>comparison</td>\n",
       "      <td>medium</td>\n",
       "      <td>Arthur's Magazine (1844–1846) was an American ...</td>\n",
       "      <td>0.421857</td>\n",
       "      <td>0.212900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Which tennis player won more Grand Slam titles...</td>\n",
       "      <td>Jonathan Stark</td>\n",
       "      <td>['Henri Leconte', 'Jonathan Stark']</td>\n",
       "      <td>Henri Leconte won more Grand Slam titles than ...</td>\n",
       "      <td>Jonathan Stark won more Grand Slam titles than...</td>\n",
       "      <td>comparison</td>\n",
       "      <td>medium</td>\n",
       "      <td>Henri Leconte (born 4 July 1963) is a former F...</td>\n",
       "      <td>0.081073</td>\n",
       "      <td>0.169455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Which band was founded first, Hole (the rock b...</td>\n",
       "      <td>The Wolfhounds</td>\n",
       "      <td>['The Wolfhounds', 'Courtney Love']</td>\n",
       "      <td>Hole (the rock band that Courtney Love was a f...</td>\n",
       "      <td>The Wolfhounds were founded before Hole (the r...</td>\n",
       "      <td>comparison</td>\n",
       "      <td>medium</td>\n",
       "      <td>The Wolfhounds are an indie pop/noise pop band...</td>\n",
       "      <td>0.959961</td>\n",
       "      <td>0.727310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Were Pavel Urysohn and Leonid Levin known for ...</td>\n",
       "      <td>no</td>\n",
       "      <td>['yes', 'no']</td>\n",
       "      <td>Yes, Pavel Urysohn and Leonid Levin were known...</td>\n",
       "      <td>No, Pavel Urysohn and Leonid Levin were not kn...</td>\n",
       "      <td>comparison</td>\n",
       "      <td>medium</td>\n",
       "      <td>Leonid Anatolievich Levin ( ; Russian: Леони́д...</td>\n",
       "      <td>0.732361</td>\n",
       "      <td>0.006006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Are both The New Pornographers and Kings of Le...</td>\n",
       "      <td>yes</td>\n",
       "      <td>['yes', 'no']</td>\n",
       "      <td>Yes, both The New Pornographers and Kings of L...</td>\n",
       "      <td>No, neither The New Pornographers nor Kings of...</td>\n",
       "      <td>comparison</td>\n",
       "      <td>hard</td>\n",
       "      <td>Kings of Leon is an American rock band that fo...</td>\n",
       "      <td>0.494671</td>\n",
       "      <td>0.001089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0  \\\n",
       "0             0             0           0   \n",
       "1             1             1           1   \n",
       "2             2             2           2   \n",
       "3             3             3           3   \n",
       "4             4             4           4   \n",
       "\n",
       "                                            question             answer  \\\n",
       "0  Which magazine was started first, Arthur's Mag...  Arthur's Magazine   \n",
       "1  Which tennis player won more Grand Slam titles...     Jonathan Stark   \n",
       "2  Which band was founded first, Hole (the rock b...     The Wolfhounds   \n",
       "3  Were Pavel Urysohn and Leonid Levin known for ...                 no   \n",
       "4  Are both The New Pornographers and Kings of Le...                yes   \n",
       "\n",
       "                                    options  \\\n",
       "0  [\"Arthur's Magazine\", 'First for Women']   \n",
       "1       ['Henri Leconte', 'Jonathan Stark']   \n",
       "2       ['The Wolfhounds', 'Courtney Love']   \n",
       "3                             ['yes', 'no']   \n",
       "4                             ['yes', 'no']   \n",
       "\n",
       "                                           first nli  \\\n",
       "0  Arthur's Magazine was started before First for...   \n",
       "1  Henri Leconte won more Grand Slam titles than ...   \n",
       "2  Hole (the rock band that Courtney Love was a f...   \n",
       "3  Yes, Pavel Urysohn and Leonid Levin were known...   \n",
       "4  Yes, both The New Pornographers and Kings of L...   \n",
       "\n",
       "                                          second nli        type   level  \\\n",
       "0  First for Women was started before Arthur's Ma...  comparison  medium   \n",
       "1  Jonathan Stark won more Grand Slam titles than...  comparison  medium   \n",
       "2  The Wolfhounds were founded before Hole (the r...  comparison  medium   \n",
       "3  No, Pavel Urysohn and Leonid Levin were not kn...  comparison  medium   \n",
       "4  No, neither The New Pornographers nor Kings of...  comparison    hard   \n",
       "\n",
       "                                            passages  scores 1  scores 2  \n",
       "0  Arthur's Magazine (1844–1846) was an American ...  0.421857  0.212900  \n",
       "1  Henri Leconte (born 4 July 1963) is a former F...  0.081073  0.169455  \n",
       "2  The Wolfhounds are an indie pop/noise pop band...  0.959961  0.727310  \n",
       "3  Leonid Anatolievich Levin ( ; Russian: Леони́д...  0.732361  0.006006  \n",
       "4  Kings of Leon is an American rock band that fo...  0.494671  0.001089  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('nli_augmented-2.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2da53000-d32a-433a-babb-1fbce67c3b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at FacebookAI/roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ast\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained('FacebookAI/roberta-large-mnli')\n",
    "tokenizer = AutoTokenizer.from_pretrained('FacebookAI/roberta-large-mnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0115031e-a148-41d3-8eb7-3675d80560ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:2723: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scores_1 = []\n",
    "\n",
    "for i in range(len(df)): \n",
    "    \n",
    "    options = ast.literal_eval(df['options'][i])\n",
    "    hypothesis1 = df['first nli'][i]\n",
    "    # hypothesis2 = df['second nli'][i]\n",
    "    premise = df['passages'][i]\n",
    "\n",
    "    \n",
    "    # run through model pre-trained on MNLI\n",
    "    x1 = tokenizer.encode(premise, hypothesis1, return_tensors='pt',\n",
    "                         truncation_strategy='only_first')\n",
    "    logits1 = nli_model(x1)[0]\n",
    "    entail_contradiction_logits1 = logits1[:,[0,2]]\n",
    "    probs1 = entail_contradiction_logits1.softmax(dim=1)\n",
    "    prob_label_is_true1 = probs1[:,1]\n",
    "    scores_1.append(prob_label_is_true1.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d40cf271-e755-4ad9-bde9-d9cb466ffd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['scores 1 - roberta'] = scores_1\n",
    "df.to_csv('nli_augmented-3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065b8003-3b8f-41bb-94da-6bc58a9ed444",
   "metadata": {},
   "source": [
    "# Add NLI scores - part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "819ea9a2-844c-4e7f-be87-be040682edcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.4</th>\n",
       "      <th>Unnamed: 0.3</th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>options</th>\n",
       "      <th>first nli</th>\n",
       "      <th>second nli</th>\n",
       "      <th>type</th>\n",
       "      <th>level</th>\n",
       "      <th>passages</th>\n",
       "      <th>scores 1</th>\n",
       "      <th>scores 2</th>\n",
       "      <th>scores 1 - roberta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Which magazine was started first, Arthur's Mag...</td>\n",
       "      <td>Arthur's Magazine</td>\n",
       "      <td>[\"Arthur's Magazine\", 'First for Women']</td>\n",
       "      <td>Arthur's Magazine was started before First for...</td>\n",
       "      <td>First for Women was started before Arthur's Ma...</td>\n",
       "      <td>comparison</td>\n",
       "      <td>medium</td>\n",
       "      <td>Arthur's Magazine (1844–1846) was an American ...</td>\n",
       "      <td>0.421857</td>\n",
       "      <td>0.050776</td>\n",
       "      <td>0.844637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Which tennis player won more Grand Slam titles...</td>\n",
       "      <td>Jonathan Stark</td>\n",
       "      <td>['Henri Leconte', 'Jonathan Stark']</td>\n",
       "      <td>Henri Leconte won more Grand Slam titles than ...</td>\n",
       "      <td>Jonathan Stark won more Grand Slam titles than...</td>\n",
       "      <td>comparison</td>\n",
       "      <td>medium</td>\n",
       "      <td>Henri Leconte (born 4 July 1963) is a former F...</td>\n",
       "      <td>0.081073</td>\n",
       "      <td>0.250810</td>\n",
       "      <td>0.038453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Which band was founded first, Hole (the rock b...</td>\n",
       "      <td>The Wolfhounds</td>\n",
       "      <td>['The Wolfhounds', 'Courtney Love']</td>\n",
       "      <td>Hole (the rock band that Courtney Love was a f...</td>\n",
       "      <td>The Wolfhounds were founded before Hole (the r...</td>\n",
       "      <td>comparison</td>\n",
       "      <td>medium</td>\n",
       "      <td>The Wolfhounds are an indie pop/noise pop band...</td>\n",
       "      <td>0.959961</td>\n",
       "      <td>0.129603</td>\n",
       "      <td>0.110932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Were Pavel Urysohn and Leonid Levin known for ...</td>\n",
       "      <td>no</td>\n",
       "      <td>['yes', 'no']</td>\n",
       "      <td>Yes, Pavel Urysohn and Leonid Levin were known...</td>\n",
       "      <td>No, Pavel Urysohn and Leonid Levin were not kn...</td>\n",
       "      <td>comparison</td>\n",
       "      <td>medium</td>\n",
       "      <td>Leonid Anatolievich Levin ( ; Russian: Леони́д...</td>\n",
       "      <td>0.732361</td>\n",
       "      <td>0.015183</td>\n",
       "      <td>0.567181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Are both The New Pornographers and Kings of Le...</td>\n",
       "      <td>yes</td>\n",
       "      <td>['yes', 'no']</td>\n",
       "      <td>Yes, both The New Pornographers and Kings of L...</td>\n",
       "      <td>No, neither The New Pornographers nor Kings of...</td>\n",
       "      <td>comparison</td>\n",
       "      <td>hard</td>\n",
       "      <td>Kings of Leon is an American rock band that fo...</td>\n",
       "      <td>0.494671</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.685024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.4  Unnamed: 0.3  Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0  \\\n",
       "0             0             0             0             0           0   \n",
       "1             1             1             1             1           1   \n",
       "2             2             2             2             2           2   \n",
       "3             3             3             3             3           3   \n",
       "4             4             4             4             4           4   \n",
       "\n",
       "                                            question             answer  \\\n",
       "0  Which magazine was started first, Arthur's Mag...  Arthur's Magazine   \n",
       "1  Which tennis player won more Grand Slam titles...     Jonathan Stark   \n",
       "2  Which band was founded first, Hole (the rock b...     The Wolfhounds   \n",
       "3  Were Pavel Urysohn and Leonid Levin known for ...                 no   \n",
       "4  Are both The New Pornographers and Kings of Le...                yes   \n",
       "\n",
       "                                    options  \\\n",
       "0  [\"Arthur's Magazine\", 'First for Women']   \n",
       "1       ['Henri Leconte', 'Jonathan Stark']   \n",
       "2       ['The Wolfhounds', 'Courtney Love']   \n",
       "3                             ['yes', 'no']   \n",
       "4                             ['yes', 'no']   \n",
       "\n",
       "                                           first nli  \\\n",
       "0  Arthur's Magazine was started before First for...   \n",
       "1  Henri Leconte won more Grand Slam titles than ...   \n",
       "2  Hole (the rock band that Courtney Love was a f...   \n",
       "3  Yes, Pavel Urysohn and Leonid Levin were known...   \n",
       "4  Yes, both The New Pornographers and Kings of L...   \n",
       "\n",
       "                                          second nli        type   level  \\\n",
       "0  First for Women was started before Arthur's Ma...  comparison  medium   \n",
       "1  Jonathan Stark won more Grand Slam titles than...  comparison  medium   \n",
       "2  The Wolfhounds were founded before Hole (the r...  comparison  medium   \n",
       "3  No, Pavel Urysohn and Leonid Levin were not kn...  comparison  medium   \n",
       "4  No, neither The New Pornographers nor Kings of...  comparison    hard   \n",
       "\n",
       "                                            passages  scores 1  scores 2  \\\n",
       "0  Arthur's Magazine (1844–1846) was an American ...  0.421857  0.050776   \n",
       "1  Henri Leconte (born 4 July 1963) is a former F...  0.081073  0.250810   \n",
       "2  The Wolfhounds are an indie pop/noise pop band...  0.959961  0.129603   \n",
       "3  Leonid Anatolievich Levin ( ; Russian: Леони́д...  0.732361  0.015183   \n",
       "4  Kings of Leon is an American rock band that fo...  0.494671  0.000362   \n",
       "\n",
       "   scores 1 - roberta  \n",
       "0            0.844637  \n",
       "1            0.038453  \n",
       "2            0.110932  \n",
       "3            0.567181  \n",
       "4            0.685024  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('nli_augmented-3.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca0a394c-3cec-4f3d-bae5-6f8050445443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ast\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87654800-5698-42f0-a3c8-e7dc792c91b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:2723: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scores_2 = []\n",
    "\n",
    "for i in range(len(df)): \n",
    "    \n",
    "    options = ast.literal_eval(df['options'][i])\n",
    "    hypothesis2 = df['second nli'][i]\n",
    "    premise = df['passages'][i]\n",
    "\n",
    "    \n",
    "    # run through model pre-trained on MNLI\n",
    "    x2 = tokenizer.encode(premise, hypothesis2, return_tensors='pt',\n",
    "                         truncation_strategy='only_first')\n",
    "    logits2 = nli_model(x2)[0]\n",
    "    entail_contradiction_logits2 = logits2[:,[0,2]]\n",
    "    probs2 = entail_contradiction_logits2.softmax(dim=1)\n",
    "    prob_label_is_true2 = probs2[:,1]\n",
    "    scores_2.append(prob_label_is_true2.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c124cf8e-76d6-4e5d-8a8b-a319c84867ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['scores 2'] = scores_2\n",
    "df.to_csv('nli_augmented-def.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f67c490a-8a94-456f-a268-fc60862a2832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.4</th>\n",
       "      <th>Unnamed: 0.3</th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>options</th>\n",
       "      <th>first nli</th>\n",
       "      <th>second nli</th>\n",
       "      <th>type</th>\n",
       "      <th>level</th>\n",
       "      <th>passages</th>\n",
       "      <th>scores 1</th>\n",
       "      <th>scores 2</th>\n",
       "      <th>scores 1 - roberta</th>\n",
       "      <th>scores 2 - roberta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>347</td>\n",
       "      <td>347</td>\n",
       "      <td>347</td>\n",
       "      <td>347</td>\n",
       "      <td>347</td>\n",
       "      <td>Are Melissa and Darmera vegatables</td>\n",
       "      <td>no</td>\n",
       "      <td>['yes', 'no']</td>\n",
       "      <td>Melissa and Darmera are not vegetables</td>\n",
       "      <td>Melissa and Darmera are vegetables</td>\n",
       "      <td>comparison</td>\n",
       "      <td>easy</td>\n",
       "      <td>Melissa is a genus of perennial herbs in the L...</td>\n",
       "      <td>0.009427</td>\n",
       "      <td>0.009904</td>\n",
       "      <td>0.099834</td>\n",
       "      <td>0.004520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>348</td>\n",
       "      <td>348</td>\n",
       "      <td>348</td>\n",
       "      <td>348</td>\n",
       "      <td>348</td>\n",
       "      <td>Which takes place farther east, The Mosuo Sist...</td>\n",
       "      <td>The Mosuo Sisters</td>\n",
       "      <td>['The Silent Historian', 'The Mosuo Sisters']</td>\n",
       "      <td>The Silent Historian takes place farther east ...</td>\n",
       "      <td>The Mosuo Sisters take place farther east than...</td>\n",
       "      <td>comparison</td>\n",
       "      <td>medium</td>\n",
       "      <td>The Silent Historian (original title: \"Het zwi...</td>\n",
       "      <td>0.042687</td>\n",
       "      <td>0.369730</td>\n",
       "      <td>0.011292</td>\n",
       "      <td>0.034224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>349</td>\n",
       "      <td>349</td>\n",
       "      <td>349</td>\n",
       "      <td>349</td>\n",
       "      <td>349</td>\n",
       "      <td>Were Yellowcard and For Against both American ...</td>\n",
       "      <td>yes</td>\n",
       "      <td>['yes', 'no']</td>\n",
       "      <td>Yes, Yellowcard and For Against were both Amer...</td>\n",
       "      <td>No, Yellowcard and For Against were not both A...</td>\n",
       "      <td>comparison</td>\n",
       "      <td>medium</td>\n",
       "      <td>Yellowcard was an American pop punk band that ...</td>\n",
       "      <td>0.289446</td>\n",
       "      <td>0.003603</td>\n",
       "      <td>0.858297</td>\n",
       "      <td>0.003057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>Were the buildings at 270 Park Avenue and 100 ...</td>\n",
       "      <td>no</td>\n",
       "      <td>['yes', 'no']</td>\n",
       "      <td>Yes, the buildings at 270 Park Avenue and 100 ...</td>\n",
       "      <td>No, the buildings at 270 Park Avenue and 100 E...</td>\n",
       "      <td>comparison</td>\n",
       "      <td>medium</td>\n",
       "      <td>100 East 53rd Street (formerly known as 610 Le...</td>\n",
       "      <td>0.784575</td>\n",
       "      <td>0.002681</td>\n",
       "      <td>0.973729</td>\n",
       "      <td>0.007865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>Which American film, Dragonslayer or Swiss Fam...</td>\n",
       "      <td>Swiss Family Robinson</td>\n",
       "      <td>['Dragonslayer', 'Swiss Family Robinson']</td>\n",
       "      <td>Dragonslayer was shot outside London</td>\n",
       "      <td>Swiss Family Robinson was shot outside London</td>\n",
       "      <td>comparison</td>\n",
       "      <td>medium</td>\n",
       "      <td>Dragonslayer is a 1981 American fantasy film d...</td>\n",
       "      <td>0.833212</td>\n",
       "      <td>0.985439</td>\n",
       "      <td>0.993581</td>\n",
       "      <td>0.990956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0.4  Unnamed: 0.3  Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0  \\\n",
       "347           347           347           347           347         347   \n",
       "348           348           348           348           348         348   \n",
       "349           349           349           349           349         349   \n",
       "350           350           350           350           350         350   \n",
       "351           351           351           351           351         351   \n",
       "\n",
       "                                              question                 answer  \\\n",
       "347                 Are Melissa and Darmera vegatables                     no   \n",
       "348  Which takes place farther east, The Mosuo Sist...      The Mosuo Sisters   \n",
       "349  Were Yellowcard and For Against both American ...                    yes   \n",
       "350  Were the buildings at 270 Park Avenue and 100 ...                     no   \n",
       "351  Which American film, Dragonslayer or Swiss Fam...  Swiss Family Robinson   \n",
       "\n",
       "                                           options  \\\n",
       "347                                  ['yes', 'no']   \n",
       "348  ['The Silent Historian', 'The Mosuo Sisters']   \n",
       "349                                  ['yes', 'no']   \n",
       "350                                  ['yes', 'no']   \n",
       "351      ['Dragonslayer', 'Swiss Family Robinson']   \n",
       "\n",
       "                                             first nli  \\\n",
       "347             Melissa and Darmera are not vegetables   \n",
       "348  The Silent Historian takes place farther east ...   \n",
       "349  Yes, Yellowcard and For Against were both Amer...   \n",
       "350  Yes, the buildings at 270 Park Avenue and 100 ...   \n",
       "351               Dragonslayer was shot outside London   \n",
       "\n",
       "                                            second nli        type   level  \\\n",
       "347                 Melissa and Darmera are vegetables  comparison    easy   \n",
       "348  The Mosuo Sisters take place farther east than...  comparison  medium   \n",
       "349  No, Yellowcard and For Against were not both A...  comparison  medium   \n",
       "350  No, the buildings at 270 Park Avenue and 100 E...  comparison  medium   \n",
       "351      Swiss Family Robinson was shot outside London  comparison  medium   \n",
       "\n",
       "                                              passages  scores 1  scores 2  \\\n",
       "347  Melissa is a genus of perennial herbs in the L...  0.009427  0.009904   \n",
       "348  The Silent Historian (original title: \"Het zwi...  0.042687  0.369730   \n",
       "349  Yellowcard was an American pop punk band that ...  0.289446  0.003603   \n",
       "350  100 East 53rd Street (formerly known as 610 Le...  0.784575  0.002681   \n",
       "351  Dragonslayer is a 1981 American fantasy film d...  0.833212  0.985439   \n",
       "\n",
       "     scores 1 - roberta  scores 2 - roberta  \n",
       "347            0.099834            0.004520  \n",
       "348            0.011292            0.034224  \n",
       "349            0.858297            0.003057  \n",
       "350            0.973729            0.007865  \n",
       "351            0.993581            0.990956  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97749835-299d-4e07-86d8-740deb913696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
