{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a268f4e1-3a6d-464d-885b-2053e3143a2f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Dataset, documents, FAISS; retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96db4863-a54b-4d3a-87e2-74c645fcc018",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üîπ Load the dataset containing the tuples `(query, correct_answer, distractor_1, distractor_2)` and the one containing the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "094ab87a-0ba8-4878-991e-01515de302e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'options', 'answer', 'type', 'level', 'selected_passages'],\n",
       "    num_rows: 352\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('saracandu/filtered_hotpotQA', split=\"train\", trust_remote_code=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be68b882-9fee-4ecb-b966-f64fd47f31da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisci la funzione di filtro\n",
    "def filter_function(example):\n",
    "    return example['answer'] not in ['yes', 'no']\n",
    "\n",
    "# Applica il filtro al dataset\n",
    "filtered_dataset = dataset.filter(filter_function)\n",
    "filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3232ca1-3306-450e-9654-a9a47738f61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Who was inducted into the Rock and Roll Hall of Fame, David Lee Roth or Cia Berg?',\n",
       " 'options': \"['Cia Berg', 'David Lee Roth']\",\n",
       " 'answer': 'David Lee Roth',\n",
       " 'type': 'comparison',\n",
       " 'level': 'medium',\n",
       " 'selected_passages': 'Cia Berg (born 2 December 1963), now known as Cia Soro, is a Swedish television presenter and singer. She was at one time the lead singer of the Swedish rock band Whale, who released the single \"Hobo Humpin\\' Slobo Babe\". David Lee Roth (born October 10, 1954) is an American rock vocalist, musician, songwriter, actor, author, and former radio personality. In 2007, he was inducted into the Rock and Roll Hall of Fame.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926bb532-fa37-4cfe-89fa-731ad140276d",
   "metadata": {},
   "source": [
    "# Model loading and dataset selection (for testing purposes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86132ec0-f0d0-4316-b71e-92af9861cda2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ‚ñ™Ô∏è Upload the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ce89a37-b6e1-48ff-b8fb-621e456392c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb97151d1eb84cec947beb6c3c0a2c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# do not run this unless necessary!\n",
    "\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc6eb0bf-fb11-4952-b14e-809b77898a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e3ef15872b4578a01f78901f4102bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc74fe63f6a4c01a6fb93db02446770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e4d27456eb48e8b7b154ff0296173e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b9e3b162214dd59db0951839ea09ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/653 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11432493e4f429c8e909cafed87956f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/28.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c38e62b0dcf4cf6b17f0a9083a85df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce18d688e8fd42eb9186e72d82e41989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c946c91f4364e86a1afc4eaa3721332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/6.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a84b450294946f4b5fd996ba413e441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ec1f29588d4e678d293423cf6cd1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/136 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "#################################################################\n",
    "# Tokenizer\n",
    "#################################################################\n",
    "\n",
    "model_name=\"nvidia/Llama3-ChatQA-1.5-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "#################################################################\n",
    "# bitsandbytes parameters\n",
    "#################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "#################################################################\n",
    "# Set up quantization config\n",
    "#################################################################\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "#################################################################\n",
    "# Load pre-trained config\n",
    "#################################################################\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d17128d-7b94-4e6a-9ba2-95a8dc2d4107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>Do you want a joke or a poem? A <span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>poem</span></pre>"
      ],
      "text/plain": [
       "<guidance.models.transformers._transformers.Transformers at 0x7fd1546722b0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from guidance import models, select\n",
    "\n",
    "new_model = models.Transformers(model, tokenizer)\n",
    "\n",
    "new_model + f'Do you want a joke or a poem? A ' + select(['joke', 'poem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3df2428-4a0c-4d13-9455-05cc97aaf45e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ‚ñ™Ô∏è Test with `guidance`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0545fee1-7acd-4c85-8f01-5cff7fdc7556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>You are a multiple-choice question answering assistant.\n",
       "Choose which of the following options: a star, a planet, a galaxy is the object below.\n",
       "\n",
       "Object: the sun\n",
       "a<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> star</span></pre>"
      ],
      "text/plain": [
       "<guidance.models.transformers._transformers.Transformers at 0x7fb9cdfdd100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message = \"\"\"You are a multiple-choice question answering assistant.\n",
    "Choose which of the following options: a star, a planet, a galaxy is the object below.\n",
    "\n",
    "Object: the sun\n",
    "\"\"\"\n",
    "\n",
    "new_model + system_message + select([\"a planet\", \"a galaxy\", \"a star\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580d12a6-e140-4675-8721-6577a8e5939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from operator import itemgetter\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"You are a multiple-choice question answering assistant.\n",
    "You have a suggestion on which answer is the most appropriate, that is treated as context. Use the suggestion to choose the most proper option.\n",
    "You also have an attempt of answer that you are suggested to neglect. \n",
    "\n",
    "Question: {question}\n",
    "Attempt: {candidate_answer}\n",
    "Context: {critique}\n",
    "\n",
    "The most proper option between {options} is:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "augmentation = {\"question\": itemgetter(\"question\"),\n",
    "                \"options\": itemgetter(\"options\"), \n",
    "                \"candidate_answer\": itemgetter(\"candidate_answer\"),\n",
    "                \"critique\": itemgetter(\"critique\"),\n",
    "                \"context\": itemgetter(\"context\"), }\n",
    "\n",
    "synthesis_chain = augmentation | prompt_template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fe594c-9c67-43cf-81ba-a4a26636b77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffles the order of the vector containing the correct answer and the two distractors\n",
    "# returns another vector, shuffled\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Definisci una funzione di pulizia per rimuovere caratteri non validi\n",
    "def clean_text(text):\n",
    "    return re.sub(r\"[^\\w\\s.,!?\\-:;()]+\", '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54e88d4-401d-46f4-92a4-a8c55fc6984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesisGeneration(query, prompt_template, merged, candidate_answer, critique, sources):\n",
    "    # merged = ast.literal_eval(merged)\n",
    "    augmented_prompt = synthesis_chain.invoke({'question': query, \n",
    "                                            'options': merged,\n",
    "                                            'candidate_answer': candidate_answer,\n",
    "                                            'critique': critique,\n",
    "                                            'context': sources})\n",
    "\n",
    "    normal_string = clean_text(augmented_prompt.text)\n",
    "    new_model + normal_string + select(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae4f975-7901-4eb9-9874-74d14f5b36f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesisGeneration('what is the sun', prompt_template, ['star', 'planet'], 'planet', \n",
    "                    'the correct answer is: a star since bot an asteroid and a planet are inadequate and not supported by the context', \n",
    "                    'The Sun is the star at the center of the Solar System. It is a massive, nearly perfect sphere of hot plasma, heated to incandescence by nuclear fusion reactions in its core, radiating the energy from its surface mainly as visible light and infrared radiation with 10% at ultraviolet energies.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c952a714-1f7c-48d0-8538-10bc2717091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"\"\" Arthur\\'s Magazine (1844‚Äì1846) was an American literary periodical published in Philadelphia in the 19th century. \n",
    "Edited by T.S. Arthur, it featured work by Edgar A. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G. Spear, and others. \n",
    "In May 1846 it was merged into \"Godey\\'s Lady\\'s Book. First for Women is a woman\\'s magazine published by Bauer Media Group in the USA.  \n",
    "The magazine was started in 1989. It is based in Englewood Cliffs, New Jersey. In 2011 the circulation of the magazine was 1,310,696 copies.\n",
    "    \"\"\"\n",
    "\n",
    "synthesisGeneration('Which magazine was started first Arthur\\'s Magazine or First for Women?', \n",
    "                    prompt_template, ['First for Women', 'Arthur\\'s Magazine'], 'First for Women', \n",
    "                    'the correct answer is Arthur\\'s Magazine and the context agrees', \n",
    "                    source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a80b5f8-3ff3-4531-a8e8-bdaf5f167c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"\"\" \"The Oberoi family is an Indian family that is famous for its involvement in hotels, namely through The Oberoi Group. \n",
    "The Oberoi Group is a hotel company with its head office in Delhi. \n",
    "Founded in 1934, the company owns and/or operates 30+ luxury hotels and two river cruise ships in six countries, primarily under its Oberoi Hotels & Resorts and Trident Hotels brands.\".\n",
    "    \"\"\"\n",
    "\n",
    "synthesisGeneration('The Oberoi family is part of a hotel company that has a head office in what city?', \n",
    "                    prompt_template, ['Delhi', 'Sammardenchia'], 'Sammardenchia', \n",
    "                    'Delhi is correct', \n",
    "                    source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e819edc2-c138-472e-bd94-31559555e63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"\"\" Allison Beth Allie Goertz (born March 2, 1991) is an American musician. \n",
    "Goertz is known for her satirical songs based on various pop culture topics. Her videos are posted on YouTube under the name of Cossbysweater. \n",
    "Subjects of her songs have included the film The Room, the character Milhouse from the television show The Simpsons, and the game Dungeons & Dragons. \n",
    "Her style has been compared to that of Bo Burnham. In December 2015, Goertz released a concept album based on the Adult Swim series Rick and Morty, Sad Dance Songs, \n",
    "with the album\\'s cover emulating the animation and logo of the series.  The album was made possible through Kickstarter. \n",
    "She is co-host of Everything's Coming Up Podcast, a Simpsons-focused podcast along with Julia Prescott. \n",
    "Milhouse Mussolini van Houten is a fictional character featured in the animated television series The Simpsons, voiced by Pamela Hayden, and created by Matt Groening \n",
    "who named the character after President Richard Nixon\\'s middle name. Later in the series, it is revealed that Milhouse\\'s middle name is Mussolini. \"\n",
    "    \"\"\"\n",
    "\n",
    "synthesisGeneration('Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?', \n",
    "                    prompt_template, ['Nixon', 'Obama'], 'Obama', \n",
    "                    'the correct answer is Nixon', \n",
    "                    source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b184cf-41a6-4f5a-9fa5-d9f547fe8243",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"\"\" \n",
    "Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 ‚Äì June 16, 1979) was an American film director best known for the movie Rebel Without a Cause. \n",
    "Elia Kazan (born Elias Kazantzoglou September 7, 1909 ‚Äì September 28, 2003) was a Greek-American director, producer, writer and actor, described by The New York Times as one of the most honored and influential directors in Broadway and Hollywood history.\n",
    "    \"\"\"\n",
    "\n",
    "synthesisGeneration('What profession does Nicholas Ray and Elia Kazan have in common?', \n",
    "                    prompt_template, ['director', 'writer'], 'director', \n",
    "                    'the correct answer is writer', \n",
    "                    source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702cc29e-5f6a-440e-a5fd-8a5bee3050f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ‚ñ™Ô∏è Select a subset of the true dataset as a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dccbd160-6698-430a-b1db-feafcd5799d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffles the order of the vector containing the correct answer and the two distractors\n",
    "# returns another vector, shuffled\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Definisci una funzione di pulizia per rimuovere caratteri non validi\n",
    "def clean_text(text):\n",
    "    return re.sub(r\"[^\\w\\s.,!?\\-:;()]+\", '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca2a69e4-be19-4f96-b4de-ab80ee00a8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_examples = 300\n",
    "\n",
    "# select a subset of the queries, just for test:\n",
    "first_queries = dataset['question'][:N_examples]\n",
    "\n",
    "# same for correct answers and distractors:\n",
    "correct_answers = dataset['answer'][:N_examples]\n",
    "possibilities = dataset['options'][:N_examples]\n",
    "# and for the sources:\n",
    "sources = dataset['selected_passages'][:N_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e058aac-3fa5-42b7-95db-809669ac8524",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_examples = 237\n",
    "\n",
    "# select a subset of the queries, just for test:\n",
    "first_queries = filtered_dataset['question'][:N_examples]\n",
    "\n",
    "# same for correct answers and distractors:\n",
    "correct_answers = filtered_dataset['answer'][:N_examples]\n",
    "possibilities = filtered_dataset['options'][:N_examples]\n",
    "# and for the sources:\n",
    "sources = filtered_dataset['selected_passages'][:N_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bae5e61a-3d33-4af6-bc9a-7152f16e38f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Arthur\\'s Magazine (1844‚Äì1846) was an American literary periodical published in Philadelphia in the 19th century. Edited by T.S. Arthur, it featured work by Edgar A. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G. Spear, and others. In May 1846 it was merged into \"Godey\\'s Lady\\'s Book\". First for Women is a woman\\'s magazine published by Bauer Media Group in the USA. The magazine was started in 1989. It is based in Englewood Cliffs, New Jersey. In 2011 the circulation of the magazine was 1,310,696 copies.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b312ecad-c2c4-4791-bc82-bfaae5f5fa04",
   "metadata": {},
   "source": [
    "# Thesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f722aa12-2d2a-4868-83ef-96f01617d4d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üîπ PromptTemplate definition and a LLMChain for the **thesis** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34445f5e-870a-4cbc-a1f6-1b004027426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template definition\n",
    "# requires question, options (a string containing the possible options) and the context as input variables!\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "    System: This is a chat between a user and an AI assistant. The assistant gives helpful, detailed, and polite answers to the user‚Äôs questions based on the context. \n",
    "    {context}\n",
    "    User: {question}\n",
    "    Possible options: {options}.\n",
    "    Assistant:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcc84e0-fa8c-404d-85b4-96be5448a81d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üîπ Function that generates the output given the prompt, the question and the set of options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07b5e395-05a8-4ba7-a960-07e6b496aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM chain definition\n",
    "from operator import itemgetter\n",
    "\n",
    "augmentation = {\"question\": itemgetter(\"question\"),\n",
    "                \"options\": itemgetter(\"options\"),\n",
    "                \"context\": itemgetter(\"context\"), }\n",
    "\n",
    "thesis_chain = augmentation | prompt_template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18ca079d-d85c-491f-87ab-2a0dfd626894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def thesisGeneration(query, merged, sources):\n",
    "    merged = ast.literal_eval(merged)\n",
    "    augmented_prompt = thesis_chain.invoke({'question': query, 'options': merged, 'context': sources})\n",
    "    normal_string = clean_text(augmented_prompt.text)\n",
    "    ans = new_model + normal_string + select([clean_text(merged[0]), clean_text(merged[1])])\n",
    "    return str(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc34eb1d-d402-4373-b8c5-e21cd51902b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(text):\n",
    "    # Trova l'indice in cui inizia il testo \"Why or why not the answer is correct:\"\n",
    "    start_index = text.find(\"Assistant:\")\n",
    "\n",
    "    \n",
    "    # Se l'indice √® stato trovato, estrai la risposta corretta\n",
    "    if start_index != -1:\n",
    "        start_index += len(\"Assistant:\")\n",
    "        # Estrai il testo dopo \"Why or why not the answer is correct:\"\n",
    "        correct_answer_text = text[start_index:].strip()\n",
    "        return correct_answer_text\n",
    "    else:\n",
    "        return \"The correct answer could not be found.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecf177e-3d1d-4e08-96d3-c9bf9144c986",
   "metadata": {},
   "source": [
    "## üîπ Test: how well the thesis alone is able to perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4cf535d-2517-452e-8ef3-821dc8524cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>\n",
       "    System: This is a chat between a user and an AI assistant. The assistant gives helpful, detailed, and polite answers to the users questions based on the context. \n",
       "    Don Quichotte (Don Quixote) is an opera in five acts by Jules Massenet to a French libretto by Henri Ca√Øn. It was first performed on 19 February 1910 at the Op√©ra de Monte-Carlo. The Bassarids (in German: Die Bassariden ) is an opera in one act and an intermezzo, with music by Hans Werner Henze to an English libretto by W. H. Auden and Chester Kallman, after Euripidess The Bacchae.\n",
       "    User: Which opera has more acts, The Bassarids or Don Quichotte?\n",
       "    Possible options: Don Quichotte, The Bassarids.\n",
       "    Assistant:\n",
       "<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>The</span> Bassarids</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answers = []\n",
    "for i in range(N_examples):\n",
    "    answers.append(extract_answer(thesisGeneration(first_queries[i], possibilities[i], sources[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4815bcc6-48ba-40de-b2a4-a9510da209fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['First for Women', 'Jonathan Stark', 'The Wolfhounds', 'yes', 'yes']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a43db93-0cca-4824-b479-6fb29529a06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_answers[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e222ef24-3231-4ee1-ae92-4b1b67551500",
   "metadata": {},
   "source": [
    "# Antithesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e21293c-f26c-4c36-b5b0-dc3cffe4c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    repetition_penalty=1.5,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=400,\n",
    "    top_p=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33184819-91d2-42f0-97ab-44daab45f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def query_model(\n",
    "        system_message,\n",
    "        user_message,\n",
    "        temperature = 0.0,\n",
    "        max_length=1024\n",
    "        ):\n",
    "\n",
    "    user_message = \"Question: \" + user_message + \" Correct answer:\"\n",
    "    messages = [\n",
    "        {\"role\": \"System\", \"content\": system_message},\n",
    "        {\"role\": \"User\", \"content\": user_message},\n",
    "        ]\n",
    "    prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "        )\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    sequences = pipeline(\n",
    "        prompt,\n",
    "        do_sample=False,\n",
    "        top_p=0.0,\n",
    "        temperature=temperature,\n",
    "        #num_return_sequences=1,\n",
    "        eos_token_id=terminators,\n",
    "        max_new_tokens=max_length,\n",
    "        return_full_text=False,\n",
    "        pad_token_id=pipeline.model.config.eos_token_id\n",
    "    )\n",
    "\n",
    "    answer = sequences[0]['generated_text']\n",
    "\n",
    "    return answer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c29e2fd3-113f-4780-8ccf-a8b9e82e850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "    This is a chat between a user and an AI assistant.\n",
    "    The assistant is asked to determine the most correct answer for a given question, provided a set of possible options.\n",
    "    It also has a first tentative answer that is required to check with respect to the question and the relevant context:\n",
    "    {context}\n",
    "    The goal of the assistant is to decree which is the most correct answer to the question between the available options.\n",
    "    It can consider the attempt of answer and check whether or not it is correct given the question and the context.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "user_message = \"\"\"\n",
    "    Question: {question}?\n",
    "    It also has an attempt of answer: {candidate_answer}\n",
    "    Which of the candidate answers {options} is the most proper answer for the question? Why? \n",
    "    Think step by step but choose only one of the options: {options} by explicitly reporting which one.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a643ed8f-0d35-44b4-84cd-293816c40ed4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üî∏ Function to generate the antithesis given the question, the thesis, the context and the options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b5ffee32-0ea6-439d-9ec5-1440dd58b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def antithesisGeneration(query, prompt_template, merged, candidate_answer, sources):\n",
    "    merged = ast.literal_eval(merged)\n",
    "    second_answer = query_model(system_message.format(context = sources),\n",
    "    user_message.format(question=query, options = merged, candidate_answer = candidate_answer, context = sources,), max_length=1024)\n",
    "    return second_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ec71c35b-0598-429f-a99d-df9e82da06b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ant_answers = []\n",
    "for i in range(10):\n",
    "    ant_answers.append(antithesisGeneration(first_queries[i], prompt_template, possibilities[i], answers[i], sources[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "93695a06-214d-4bc0-9f41-219192027cac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The reference text states,\"Edited by TS arthur,it feautred works b y edgar p oe\"',\n",
       " 'The appropriate response would have been \"Lecont√©\" because this individual had achieved greater success than any other competitor throughout their careers as professionals within respective fields; whereas stark did manage winning some major tournaments yet never managed reaching heights comparable too lecon',\n",
       " \"'Hole'\",\n",
       " 'The two men were well-known scientists from different fields; therefore their names would have been familiar even if they had never met each other before becoming friends later down life path as adults after high school graduation ceremony where all graduates receive diploma certificate proving successful completion degree program requirements towards earning bachelor‚Äôs/master/doctorate degrees depending upon individual choice made earlier during application process while still minors under legal guardianship until adulthood age limit reached at eighteen years old',\n",
       " 'The reference text states \"Kings Of Leon are Americans\" so we know they\\'re all US citizens; however there was some confusion about where exactly this group originated because its members were born across different parts',\n",
       " 'The reference text states \"It [the two buildings] were both built within Manhattan.\" Therefore we know they must have been constructed inside NYC as well since there aren\\'t any other cities called manhattan',\n",
       " 'The birth date difference indicates this fact as well',\n",
       " 'The reference text states clearly about two different publications being targeted at female audiences as well-known examples from popular culture; namely SASSY MAGAZINE & BETTY respectively.',\n",
       " \"'director'\",\n",
       " '\"The Saima gesture\".']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ant_answers[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b22ec6b-32a7-4c40-b6d7-518a7b99fde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_queries[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b286c56c-01be-49a2-b265-fedc34a0d8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_answers[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f600cfb8-6a35-4baa-8236-c02c2909868c",
   "metadata": {},
   "source": [
    "# Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1edb95-c057-4d4b-8a40-c24f578e6c6b",
   "metadata": {},
   "source": [
    "## üî∫ PromptTemplate definition and a LLMChain for the **synthesis** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50e0e37b-1b37-40a5-a9de-cf5a6a5dff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"You are a multiple-choice question answering assistant.\n",
    "You have a suggestion on which answer is the most appropriate that you are strongly suggested to follow.\n",
    "Choose the most proper option between {options} that best matches with the suggestion. You also have sources reported in case you need them. \n",
    "\n",
    "Question: {question}\n",
    "Context: The proposed answer is {candidate_answer} and another suggested that {critique}\n",
    "Sources: {context}\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "augmentation = {\"question\": itemgetter(\"question\"),\n",
    "                \"options\": itemgetter(\"options\"), \n",
    "                \"candidate_answer\": itemgetter(\"candidate_answer\"),\n",
    "                \"critique\": itemgetter(\"critique\"),\n",
    "                \"context\": itemgetter(\"context\"), }\n",
    "\n",
    "synthesis_chain = augmentation | prompt_template "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8170bc0d-1182-4c31-b33d-65fe64e1c385",
   "metadata": {},
   "source": [
    "## üî∫ Function to generate the synthesis given literally everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38bef727-d240-4511-a556-0d2c19db4fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesisGeneration(query, prompt_template, merged, candidate_answer, critique, sources):\n",
    "    merged = ast.literal_eval(merged)\n",
    "    augmented_prompt = synthesis_chain.invoke({'question': query, \n",
    "                                            'options': merged,\n",
    "                                            'candidate_answer': candidate_answer,\n",
    "                                            'critique': critique,\n",
    "                                            'context': sources})\n",
    "\n",
    "    normal_string = clean_text(augmented_prompt.text)\n",
    "    ans = new_model + normal_string + select([clean_text(merged[0]), clean_text(merged[1])])\n",
    "    return str(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538c897d-183c-40eb-a9e2-c05e53addd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def_answers = [\"the correct option is \" + clean_text(correct_answer)\n",
    "               + \"since the other options is not mentioned in the context\" for correct_answer in correct_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5856d161-623d-4258-bd8c-fbe224f1b286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>You are a multiple-choice question answering assistant.\n",
       "You have a suggestion on which answer is the most appropriate that you are strongly suggested to follow.\n",
       "Choose the most proper option between Don Quichotte, The Bassarids that best matches with the suggestion. You also have sources reported in case you need them. \n",
       "\n",
       "Question: Which opera has more acts, The Bassarids or Don Quichotte?\n",
       "Context: The proposed answer is The Bassarids and another suggested that The Basars\n",
       "Sources: Don Quichotte (Don Quixote) is an opera in five acts by Jules Massenet to a French libretto by Henri Ca√Øn. It was first performed on 19 February 1910 at the Op√©ra de Monte-Carlo. The Bassarids (in German: Die Bassariden ) is an opera in one act and an intermezzo, with music by Hans Werner Henze to an English libretto by W. H. Auden and Chester Kallman, after Euripidess The Bacchae.\n",
       "\n",
       "Assistant:\n",
       "<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>The</span> Bassarids</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "syn_answers = []\n",
    "for i in range(N_examples):\n",
    "    syn_answers.append(extract_answer(\n",
    "        synthesisGeneration(\n",
    "            first_queries[i], prompt_template, possibilities[i], answers[i], \n",
    "            ant_answers[i], sources[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04c195ce-2955-4454-af82-322872620217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Arthurs Magazine',\n",
       " 'Jonathan Stark',\n",
       " 'The Wolfhounds',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'New York City',\n",
       " 'Aleksander Ford',\n",
       " 'yes',\n",
       " 'director',\n",
       " 'The Saimaa Gesture']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_answers[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e925861d-ddb8-4b74-90a8-697d526b05b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_answers[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e06ead4-64f6-4742-bb48-d7b201599aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cec0231d-9fec-429b-8d6d-b265dbae7af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {\n",
    "    'query': first_queries,\n",
    "    'correct': correct_answers,\n",
    "    'thesis': answers,\n",
    "    'antithesis': ant_answers,\n",
    "    'synthesis': syn_answers,\n",
    "    'context': sources\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e63baefa-86bc-41b3-a38a-807bc973b8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>correct</th>\n",
       "      <th>thesis</th>\n",
       "      <th>antithesis</th>\n",
       "      <th>synthesis</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which magazine was started first Arthur's Maga...</td>\n",
       "      <td>Arthur's Magazine</td>\n",
       "      <td>First for Women</td>\n",
       "      <td>The reference text states,\"Edited by TS arthur...</td>\n",
       "      <td>Arthurs Magazine</td>\n",
       "      <td>Arthur's Magazine (1844‚Äì1846) was an American ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which tennis player won more Grand Slam titles...</td>\n",
       "      <td>Jonathan Stark</td>\n",
       "      <td>Jonathan Stark</td>\n",
       "      <td>The appropriate response would have been \"Leco...</td>\n",
       "      <td>Jonathan Stark</td>\n",
       "      <td>Henri Leconte (born 4 July 1963) is a former F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which band was founded first, Hole, the rock b...</td>\n",
       "      <td>The Wolfhounds</td>\n",
       "      <td>The Wolfhounds</td>\n",
       "      <td>'Hole'</td>\n",
       "      <td>The Wolfhounds</td>\n",
       "      <td>The Wolfhounds are an indie pop/noise pop band...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Were Pavel Urysohn and Leonid Levin known for ...</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>The two men were well-known scientists from di...</td>\n",
       "      <td>yes</td>\n",
       "      <td>Leonid Anatolievich Levin ( ; Russian: –õ–µ–æ–Ω–∏ÃÅ–¥...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Are both The New Pornographers and Kings of Le...</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>The reference text states \"Kings Of Leon are A...</td>\n",
       "      <td>yes</td>\n",
       "      <td>Kings of Leon is an American rock band that fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query            correct  \\\n",
       "0  Which magazine was started first Arthur's Maga...  Arthur's Magazine   \n",
       "1  Which tennis player won more Grand Slam titles...     Jonathan Stark   \n",
       "2  Which band was founded first, Hole, the rock b...     The Wolfhounds   \n",
       "3  Were Pavel Urysohn and Leonid Levin known for ...                 no   \n",
       "4  Are both The New Pornographers and Kings of Le...                 no   \n",
       "\n",
       "            thesis                                         antithesis  \\\n",
       "0  First for Women  The reference text states,\"Edited by TS arthur...   \n",
       "1   Jonathan Stark  The appropriate response would have been \"Leco...   \n",
       "2   The Wolfhounds                                             'Hole'   \n",
       "3              yes  The two men were well-known scientists from di...   \n",
       "4              yes  The reference text states \"Kings Of Leon are A...   \n",
       "\n",
       "          synthesis                                            context  \n",
       "0  Arthurs Magazine  Arthur's Magazine (1844‚Äì1846) was an American ...  \n",
       "1    Jonathan Stark  Henri Leconte (born 4 July 1963) is a former F...  \n",
       "2    The Wolfhounds  The Wolfhounds are an indie pop/noise pop band...  \n",
       "3               yes  Leonid Anatolievich Levin ( ; Russian: –õ–µ–æ–Ω–∏ÃÅ–¥...  \n",
       "4               yes  Kings of Leon is an American rock band that fo...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb65952-bf02-4bbb-9b00-2bd5348e6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['query'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "61349820-ad5d-4dff-a1d0-a08547f727d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('baseline.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d280eeea-c69b-42d2-856b-1651fc77b083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>correct</th>\n",
       "      <th>thesis</th>\n",
       "      <th>antithesis</th>\n",
       "      <th>synthesis</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which magazine was started first Arthur's Maga...</td>\n",
       "      <td>arthurs magazine</td>\n",
       "      <td>first for women</td>\n",
       "      <td>The reference text states,\"Edited by TS arthur...</td>\n",
       "      <td>arthurs magazine</td>\n",
       "      <td>Arthur's Magazine (1844‚Äì1846) was an American ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which tennis player won more Grand Slam titles...</td>\n",
       "      <td>jonathan stark</td>\n",
       "      <td>jonathan stark</td>\n",
       "      <td>The appropriate response would have been \"Leco...</td>\n",
       "      <td>jonathan stark</td>\n",
       "      <td>Henri Leconte (born 4 July 1963) is a former F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which band was founded first, Hole, the rock b...</td>\n",
       "      <td>the wolfhounds</td>\n",
       "      <td>the wolfhounds</td>\n",
       "      <td>'Hole'</td>\n",
       "      <td>the wolfhounds</td>\n",
       "      <td>The Wolfhounds are an indie pop/noise pop band...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Were Pavel Urysohn and Leonid Levin known for ...</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>The two men were well-known scientists from di...</td>\n",
       "      <td>yes</td>\n",
       "      <td>Leonid Anatolievich Levin ( ; Russian: –õ–µ–æ–Ω–∏ÃÅ–¥...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Are both The New Pornographers and Kings of Le...</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>The reference text states \"Kings Of Leon are A...</td>\n",
       "      <td>yes</td>\n",
       "      <td>Kings of Leon is an American rock band that fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query           correct  \\\n",
       "0  Which magazine was started first Arthur's Maga...  arthurs magazine   \n",
       "1  Which tennis player won more Grand Slam titles...    jonathan stark   \n",
       "2  Which band was founded first, Hole, the rock b...    the wolfhounds   \n",
       "3  Were Pavel Urysohn and Leonid Levin known for ...                no   \n",
       "4  Are both The New Pornographers and Kings of Le...                no   \n",
       "\n",
       "            thesis                                         antithesis  \\\n",
       "0  first for women  The reference text states,\"Edited by TS arthur...   \n",
       "1   jonathan stark  The appropriate response would have been \"Leco...   \n",
       "2   the wolfhounds                                             'Hole'   \n",
       "3              yes  The two men were well-known scientists from di...   \n",
       "4              yes  The reference text states \"Kings Of Leon are A...   \n",
       "\n",
       "          synthesis                                            context  \n",
       "0  arthurs magazine  Arthur's Magazine (1844‚Äì1846) was an American ...  \n",
       "1    jonathan stark  Henri Leconte (born 4 July 1963) is a former F...  \n",
       "2    the wolfhounds  The Wolfhounds are an indie pop/noise pop band...  \n",
       "3               yes  Leonid Anatolievich Levin ( ; Russian: –õ–µ–æ–Ω–∏ÃÅ–¥...  \n",
       "4               yes  Kings of Leon is an American rock band that fo...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Funzione per rimuovere le quadre e ottenere solo il contenuto\n",
    "def remove_brackets(s):\n",
    "    return s.strip(\"[] \")\n",
    "\n",
    "# Definisci una funzione di pulizia per rimuovere caratteri non validi\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s.,!?\\'\"\\-:;()]+', '', text)  # Rimuove i caratteri speciali\n",
    "    text = re.sub(r\"['\\\"-]\", '', text)  # Rimuove apostrofi, virgolette e trattini\n",
    "    text = text.lower()  # Converte in minuscolo\n",
    "    return text\n",
    "\n",
    "# Applica la funzione alla colonna 'correct answer'\n",
    "df['correct'] = df['correct'].apply(clean_text)\n",
    "df['thesis'] = df['thesis'].apply(clean_text)\n",
    "df['synthesis'] = df['synthesis'].apply(clean_text)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2625980e-cca0-430a-be54-04cb76e2a1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Arthur\\'s Magazine (1844‚Äì1846) was an American literary periodical published in Philadelphia in the 19th century. Edited by T.S. Arthur, it featured work by Edgar A. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G. Spear, and others. In May 1846 it was merged into \"Godey\\'s Lady\\'s Book\". First for Women is a woman\\'s magazine published by Bauer Media Group in the USA. The magazine was started in 1989. It is based in Englewood Cliffs, New Jersey. In 2011 the circulation of the magazine was 1,310,696 copies.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['context'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "38e03a0e-1c19-41e0-9ae3-d8348bcdc3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 218\n",
      "Number of non-matches: 82\n"
     ]
    }
   ],
   "source": [
    "# Conta quante righe combaciano e quante no\n",
    "matches = 0\n",
    "non_matches = 0\n",
    "which_ones = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    correct_answer = str(row['correct']).strip()\n",
    "    thesis = str(row['thesis']).strip()\n",
    "    \n",
    "    if correct_answer == thesis:\n",
    "        matches += 1\n",
    "    else:\n",
    "        non_matches += 1\n",
    "        which_ones.append(\"thesis: {}, Correct: {}\".format(thesis, correct_answer))\n",
    "\n",
    "print(f\"Number of matches: {matches}\")\n",
    "print(f\"Number of non-matches: {non_matches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc63dd72-bc8f-4b43-bba7-bdbd8821fc64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thesis: first for women, Correct: arthurs magazine',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: ross lynch, Correct: glenn hughes',\n",
       " 'thesis: kim clijsters, Correct: mary pierce',\n",
       " 'thesis: edwyn collins, Correct: jimmie ross',\n",
       " 'thesis: the hunchback of notre dame, Correct: saludos amigos',\n",
       " 'thesis: temagamilorrain mine, Correct: meadowbank gold mine',\n",
       " 'thesis: neil labute, Correct: h. bruce humberstone',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: jack kevorkian, Correct: jacob kevorkian',\n",
       " 'thesis: luis mu√±oz mar√≠n international airport, Correct: rickenbacker international airport',\n",
       " 'thesis: crime novelist, Correct: playwright',\n",
       " 'thesis: xavier malisse, Correct: virginia wade',\n",
       " 'thesis: george marshall, Correct: allan dwan',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: cardwellia, Correct: ochagavia',\n",
       " 'thesis: joe manganiello, Correct: kerkor kerkorian',\n",
       " 'thesis: chinese crested dog, Correct: chiengris',\n",
       " 'thesis: vincent walker, Correct: david usher',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: jesus camp, Correct: beyond the gates of splendor',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: charley and the angel, Correct: planes',\n",
       " 'thesis: the big melt, Correct: pride divide',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: novelist, Correct: director',\n",
       " 'thesis: buddleja, Correct: burchardia',\n",
       " 'thesis: pamianthe, Correct: micromeria',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: stellenbosch university, Correct: wayne state university',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: species, Correct: genus',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: musician, Correct: singer',\n",
       " 'thesis: trace cyrus, Correct: tanya donelly',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: ryo kawakita, Correct: peter garrett',\n",
       " 'thesis: flowers, Correct: plants',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: battle of okinawa, Correct: operation north wind',\n",
       " 'thesis: better than ezra, Correct: melvins',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: allan stone, Correct: yevgeny kafelnikov',\n",
       " 'thesis: marian gold, Correct: natalie merchant',\n",
       " 'thesis: raymond queneau, Correct: peter straub',\n",
       " 'thesis: gay sex in the 70s, Correct: listen to britain',\n",
       " 'thesis: sidney olcott, Correct: denis villeneuve',\n",
       " 'thesis: dick crealy, Correct: mary pierce',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: a grade school and a high school, Correct: a grade school and high school',\n",
       " 'thesis: gerina dunwich, Correct: erich maria remarque',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: kevin dubrow, Correct: lance king',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: duranta, Correct: chelidonium',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: playwright, Correct: novelist',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: yuxi, Correct: lufeng, guangdong',\n",
       " 'thesis: lehigh university, Correct: university of iowa',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: eucommia, Correct: maytenus',\n",
       " 'thesis: fuxin, Correct: l√ºliang',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: regards linux, Correct: published on monthly basis',\n",
       " 'thesis: no, Correct: yes',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: yes, Correct: no',\n",
       " 'thesis: el nuevo cojo, Correct: glamour',\n",
       " 'thesis: the bassarids, Correct: don quichotte']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "which_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2e0614ea-7259-4f87-943c-eaa830e211b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 220\n",
      "Number of non-matches: 80\n"
     ]
    }
   ],
   "source": [
    "# Conta quante righe combaciano e quante no\n",
    "matches = 0\n",
    "non_matches = 0\n",
    "which_ones_syn = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    correct_answer = str(row['correct']).strip()\n",
    "    thesis = str(row['synthesis']).strip()\n",
    "    \n",
    "    if correct_answer == thesis:\n",
    "        matches += 1\n",
    "    else:\n",
    "        # print(\"Synthesis: {}, Correct: {}\".format(thesis, correct_answer))\n",
    "        non_matches += 1\n",
    "        which_ones_syn.append(\"Synthesis: {}, Correct: {}\".format(thesis, correct_answer))\n",
    "\n",
    "print(f\"Number of matches: {matches}\")\n",
    "print(f\"Number of non-matches: {non_matches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b99b55c2-abdc-4dd7-b54f-4470b9457f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Synthesis: yes, Correct: no',\n",
       " 'Synthesis: yes, Correct: no',\n",
       " 'Synthesis: yes, Correct: no',\n",
       " 'Synthesis: ross lynch, Correct: glenn hughes',\n",
       " 'Synthesis: producer, Correct: filmmaker',\n",
       " 'Synthesis: no, Correct: yes',\n",
       " 'Synthesis: kim clijsters, Correct: mary pierce',\n",
       " 'Synthesis: edwyn collins, Correct: jimmie ross',\n",
       " 'Synthesis: robert e. howard, Correct: richard ford',\n",
       " 'Synthesis: the hunchback of notre dame, Correct: saludos amigos',\n",
       " 'Synthesis: temagamilorrain mine, Correct: meadowbank gold mine',\n",
       " 'Synthesis: jack kevorkian, Correct: jacob kevorkian',\n",
       " 'Synthesis: luis mu√±oz mar√≠n international airport, Correct: rickenbacker international airport',\n",
       " 'Synthesis: crime novelist, Correct: playwright',\n",
       " 'Synthesis: xavier malisse, Correct: virginia wade',\n",
       " 'Synthesis: george marshall, Correct: allan dwan',\n",
       " 'Synthesis: yes, Correct: no',\n",
       " 'Synthesis: cardwellia, Correct: ochagavia',\n",
       " 'Synthesis: no, Correct: yes',\n",
       " 'Synthesis: joe manganiello, Correct: kerkor kerkorian',\n",
       " 'Synthesis: bookmarks, Correct: modern drummer',\n",
       " 'Synthesis: yes, Correct: no',\n",
       " 'Synthesis: yes, Correct: no',\n",
       " 'Synthesis: charley and the angel, Correct: planes',\n",
       " 'Synthesis: the big melt, Correct: pride divide',\n",
       " 'Synthesis: novelist, Correct: director',\n",
       " 'Synthesis: armen nalbandian, Correct: arto tun√ßboyacƒ±yan',\n",
       " 'Synthesis: pamianthe, Correct: micromeria',\n",
       " 'Synthesis: yes, Correct: no',\n",
       " 'Synthesis: yes, Correct: no',\n",
       " 'Synthesis: no, Correct: yes',\n",
       " 'Synthesis: alexandra shiva, Correct: brothers quay',\n",
       " 'Synthesis: stellenbosch university, Correct: wayne state university',\n",
       " 'Synthesis: peter chelsom, Correct: billy bob thornton',\n",
       " 'Synthesis: yes, Correct: no',\n",
       " 'Synthesis: species, Correct: genus',\n",
       " 'Synthesis: musician, Correct: singer',\n",
       " 'Synthesis: musical film, Correct: documentary film',\n",
       " 'Synthesis: yes, Correct: no',\n",
       " 'Synthesis: ryo kawakita, Correct: peter garrett',\n",
       " 'Synthesis: stationery, Correct: board game',\n",
       " 'Synthesis: yes, Correct: no',\n",
       " 'Synthesis: the last shadow puppets, Correct: the classic crime',\n",
       " 'Synthesis: flowers, Correct: plants',\n",
       " 'Synthesis: yes, Correct: no',\n",
       " 'Synthesis: romance, Correct: documentary',\n",
       " 'Synthesis: yes, Correct: no',\n",
       " 'Synthesis: battle of okinawa, Correct: operation north wind',\n",
       " 'Synthesis: better than ezra, Correct: melvins',\n",
       " 'Synthesis: marian gold, Correct: natalie merchant',\n",
       " 'Synthesis: gay sex in the 70s, Correct: listen to britain',\n",
       " 'Synthesis: indie, Correct: rock',\n",
       " 'Synthesis: no, Correct: yes',\n",
       " 'Synthesis: roman karmen, Correct: francis stokes',\n",
       " 'Synthesis: dick crealy, Correct: mary pierce',\n",
       " 'Synthesis: yes, Correct: no',\n",
       " 'Synthesis: yes, Correct: no',\n",
       " 'Synthesis: a grade school, Correct: a grade school and high school',\n",
       " 'Synthesis: gerina dunwich, Correct: erich maria remarque',\n",
       " 'Synthesis: a. e. housman, Correct: dean koontz',\n",
       " 'Synthesis: yes, Correct: no',\n",
       " 'Synthesis: kevin dubrow, Correct: lance king',\n",
       " 'Synthesis: yes, Correct: no',\n",
       " 'Synthesis: portugal, Correct: spain',\n",
       " 'Synthesis: yes, Correct: no',\n",
       " 'Synthesis: playwright, Correct: novelist',\n",
       " 'Synthesis: yuxi, Correct: lufeng, guangdong',\n",
       " 'Synthesis: the misadventures of merlin jones, Correct: in search of the castaways',\n",
       " 'Synthesis: yes, Correct: no',\n",
       " 'Synthesis: lehigh university, Correct: university of iowa',\n",
       " 'Synthesis: eucommia, Correct: maytenus',\n",
       " 'Synthesis: dreamworks, Correct: disney',\n",
       " 'Synthesis: fuxin, Correct: l√ºliang',\n",
       " 'Synthesis: regards linux, Correct: published on monthly basis',\n",
       " 'Synthesis: no, Correct: yes',\n",
       " 'Synthesis: yes, Correct: no',\n",
       " 'Synthesis: no, Correct: yes',\n",
       " 'Synthesis: yes, Correct: no',\n",
       " 'Synthesis: el nuevo cojo, Correct: glamour',\n",
       " 'Synthesis: the bassarids, Correct: don quichotte']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "which_ones_syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1da947cd-e5d1-4ca8-9e42-22238c963e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73.33333333333333"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "220/(220+80)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "27622337-968e-4176-afa4-f4630c3cddeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "222/(222+78)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93baec3-a750-468a-a7ca-3197901581d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
