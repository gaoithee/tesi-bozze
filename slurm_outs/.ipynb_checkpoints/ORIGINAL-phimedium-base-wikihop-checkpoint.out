---------------------------------------------
SLURM job ID:        793835
SLURM job node list: gpu002
DATE:                Sun Sep  1 12:26:20 PM CEST 2024
---------------------------------------------
/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback
  backends.update(_get_backends("networkx.backends"))
`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:02<00:14,  2.98s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:05<00:11,  2.89s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:08<00:08,  2.86s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:11<00:05,  2.79s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:37<00:11, 11.09s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:04<00:00, 16.57s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:04<00:00, 10.72s/it]
[2K[2K<IPython.core.display.HTML object>
[2K[2K<IPython.core.display.HTML object>
You are not running the flash-attention implementation, expect numerical differences.
[2K[2K<IPython.core.display.HTML object>
[2K[2K<IPython.core.display.HTML object>
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
