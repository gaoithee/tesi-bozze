{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a268f4e1-3a6d-464d-885b-2053e3143a2f",
   "metadata": {},
   "source": [
    "# Dataset, documents, FAISS; retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96db4863-a54b-4d3a-87e2-74c645fcc018",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Load the dataset containing the tuples `(query, correct_answer, distractor_1, distractor_2)` and the one containing the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "094ab87a-0ba8-4878-991e-01515de302e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers', 'correct_answer', 'distractor_1', 'distractor_2'],\n",
       "    num_rows: 82326\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import ast\n",
    "\n",
    "dataset = load_dataset('saracandu/msmarco_modified', split=\"train\", trust_remote_code=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5678034-5b6e-4052-a330-28f255f97efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_passages = []\n",
    "\n",
    "for row in dataset:\n",
    "    passages_data = ast.literal_eval(row['passages'])\n",
    "    try:\n",
    "        selected_index = passages_data['is_selected'].index(1)\n",
    "        selected_passage = {\n",
    "            'is_selected': 1,\n",
    "            'passage_text': passages_data['passage_text'][selected_index],\n",
    "        }\n",
    "        selected_passages.append(selected_passage)\n",
    "    except ValueError:\n",
    "        # Aggiungi un passaggio vuoto se non c'Ã¨ nessun passaggio selezionato\n",
    "        selected_passages.append({'is_selected': 0, 'passage_text': ''})\n",
    "\n",
    "# Assicurati che la lunghezza dei passaggi selezionati corrisponda alla lunghezza del dataset originale\n",
    "assert len(selected_passages) == len(dataset), \"Errore: lunghezza dei passaggi selezionati non corrisponde alla lunghezza del dataset originale\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffcdd7a5-9083-470c-893d-c0fbf75ba3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Creazione di un nuovo dizionario con i dati desiderati\n",
    "new_dataset = {\n",
    "    'answers': dataset['answers'],\n",
    "    'query': dataset['query'],\n",
    "    'query_id': dataset['query_id'],\n",
    "    'query_type': dataset['query_type'],\n",
    "    'wellFormedAnswers': dataset['wellFormedAnswers'],\n",
    "    'correct_answer': dataset['correct_answer'],\n",
    "    'distractor_1': dataset['distractor_1'],\n",
    "    'distractor_2': dataset['distractor_2'],\n",
    "    'selected_passages': selected_passages\n",
    "}\n",
    "\n",
    "# Creazione del nuovo dataset\n",
    "new_dataset = Dataset.from_dict(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20df4e88-959e-4c27-8756-15923bb36567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f83c64ac18e4c979ffa559af4e3126c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/82326 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['answers', 'query', 'query_id', 'query_type', 'wellFormedAnswers', 'correct_answer', 'distractor_1', 'distractor_2', 'selected_passages'],\n",
       "    num_rows: 80143\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_correct_answer(example):\n",
    "    return example['correct_answer'] != '[]'\n",
    "\n",
    "# Applica la funzione di filtro\n",
    "new_dataset = new_dataset.filter(filter_correct_answer)\n",
    "\n",
    "# Visualizza le prime righe del dataset filtrato per verificare il risultato\n",
    "new_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926bb532-fa37-4cfe-89fa-731ad140276d",
   "metadata": {},
   "source": [
    "# Model loading and dataset selection (for testing purposes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86132ec0-f0d0-4316-b71e-92af9861cda2",
   "metadata": {},
   "source": [
    "## â–ªï¸ Upload the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce89a37-b6e1-48ff-b8fb-621e456392c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not run this unless necessary!\n",
    "\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc6eb0bf-fb11-4952-b14e-809b77898a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db54dcabeea1463aaa6310474dcfde59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "#################################################################\n",
    "# Tokenizer\n",
    "#################################################################\n",
    "\n",
    "model_name=\"nvidia/Llama3-ChatQA-1.5-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "#################################################################\n",
    "# bitsandbytes parameters\n",
    "#################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "#################################################################\n",
    "# Set up quantization config\n",
    "#################################################################\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "#################################################################\n",
    "# Load pre-trained config\n",
    "#################################################################\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d17128d-7b94-4e6a-9ba2-95a8dc2d4107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from outlines import models, generate, samplers\n",
    "\n",
    "new_model = models.Transformers(model, tokenizer)\n",
    "sampler = samplers.greedy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0545fee1-7acd-4c85-8f01-5cff7fdc7556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling FSM index for all state transitions: 100%|â–ˆâ–ˆ| 17/17 [00:00<00:00, 19.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a star'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import outlines\n",
    "\n",
    "system_message = \"\"\"You are a multiple-choice question answering assistant.\n",
    "Choose which of the following options: a star, a planet, a galaxy is the object below.\n",
    "\n",
    "Object: the sun\n",
    "\"\"\"\n",
    "\n",
    "generator = outlines.generate.choice(new_model, [\"a planet\", \"a galaxy\", \"a star\"], sampler = sampler)\n",
    "answer = generator(system_message)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "580d12a6-e140-4675-8721-6577a8e5939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import outlines\n",
    "from langchain import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from operator import itemgetter\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"You are a multiple-choice question answering assistant.\n",
    "Choose which of the following options: {option_a}, {option_b} is most likely the answer to a question. \n",
    "You have also a suggestion on which answer is the most appropriate, that you have to use as an additional context.\n",
    "\n",
    "Question: {question}\n",
    "Context: {critique}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "augmentation = {\"question\": itemgetter(\"question\"),\n",
    "                \"option_a\": itemgetter(\"option_a\"), \n",
    "                \"option_b\": itemgetter(\"option_b\"),\n",
    "                \"candidate_answer\": itemgetter(\"candidate_answer\"),\n",
    "                \"critique\": itemgetter(\"critique\"),\n",
    "                \"context\": itemgetter(\"context\"), }\n",
    "\n",
    "synthesis_chain = augmentation | prompt_template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e54e88d4-401d-46f4-92a4-a8c55fc6984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesisGeneration(query, prompt_template, merged, candidate_answer, critique, sources):\n",
    "    augmented_prompt = synthesis_chain.invoke({'question': query, \n",
    "                                            'option_a': merged[0], 'option_b': merged[1],\n",
    "                                            'candidate_answer': candidate_answer,\n",
    "                                            'critique': critique,\n",
    "                                            'context': sources})\n",
    "\n",
    "    normal_string = clean_text(augmented_prompt.text)\n",
    "    options = [merged[0], merged[1]]\n",
    "    generator = outlines.generate.choice(new_model, options, sampler = sampler)\n",
    "    final_answer = generator(normal_string, max_tokens = 30)\n",
    "    \n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ae4f975-7901-4eb9-9874-74d14f5b36f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a star'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthesisGeneration('what is the sun', prompt_template, ['a star', 'a planet'], 'a planet', \n",
    "                    'the correct answer is: a star since bot an asteroid and a planet are inadequate and not supported by the context', \n",
    "                    'The Sun is the star at the center of the Solar System. It is a massive, nearly perfect sphere of hot plasma, heated to incandescence by nuclear fusion reactions in its core, radiating the energy from its surface mainly as visible light and infrared radiation with 10% at ultraviolet energies.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c952a714-1f7c-48d0-8538-10bc2717091e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Arthur's Magazine\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = \"\"\" Arthur\\'s Magazine (1844â€“1846) was an American literary periodical published in Philadelphia in the 19th century. \n",
    "Edited by T.S. Arthur, it featured work by Edgar A. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G. Spear, and others. \n",
    "In May 1846 it was merged into \"Godey\\'s Lady\\'s Book. First for Women is a woman\\'s magazine published by Bauer Media Group in the USA.  \n",
    "The magazine was started in 1989. It is based in Englewood Cliffs, New Jersey. In 2011 the circulation of the magazine was 1,310,696 copies.\n",
    "    \"\"\"\n",
    "\n",
    "synthesisGeneration('Which magazine was started first Arthur\\'s Magazine or First for Women?', \n",
    "                    prompt_template, ['First for Women', 'Arthur\\'s Magazine'], 'First for Women', \n",
    "                    'the correct answer is Arthur\\'s Magazine and the context agrees', \n",
    "                    source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a80b5f8-3ff3-4531-a8e8-bdaf5f167c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Delhi'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = \"\"\" \"The Oberoi family is an Indian family that is famous for its involvement in hotels, namely through The Oberoi Group. \n",
    "The Oberoi Group is a hotel company with its head office in Delhi. \n",
    "Founded in 1934, the company owns and/or operates 30+ luxury hotels and two river cruise ships in six countries, primarily under its Oberoi Hotels & Resorts and Trident Hotels brands.\".\n",
    "    \"\"\"\n",
    "\n",
    "synthesisGeneration('The Oberoi family is part of a hotel company that has a head office in what city?', \n",
    "                    prompt_template, ['Delhi', 'Sammardenchia'], 'Sammardenchia', \n",
    "                    'Delhi is correct', \n",
    "                    source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e819edc2-c138-472e-bd94-31559555e63b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'President Richard Nixon'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = \"\"\" Allison Beth Allie Goertz (born March 2, 1991) is an American musician. \n",
    "Goertz is known for her satirical songs based on various pop culture topics. Her videos are posted on YouTube under the name of Cossbysweater. \n",
    "Subjects of her songs have included the film The Room, the character Milhouse from the television show The Simpsons, and the game Dungeons & Dragons. \n",
    "Her style has been compared to that of Bo Burnham. In December 2015, Goertz released a concept album based on the Adult Swim series Rick and Morty, Sad Dance Songs, \n",
    "with the album\\'s cover emulating the animation and logo of the series.  The album was made possible through Kickstarter. \n",
    "She is co-host of Everything's Coming Up Podcast, a Simpsons-focused podcast along with Julia Prescott. \n",
    "Milhouse Mussolini van Houten is a fictional character featured in the animated television series The Simpsons, voiced by Pamela Hayden, and created by Matt Groening \n",
    "who named the character after President Richard Nixon\\'s middle name. Later in the series, it is revealed that Milhouse\\'s middle name is Mussolini. \"\n",
    "    \"\"\"\n",
    "\n",
    "synthesisGeneration('Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?', \n",
    "                    prompt_template, ['President Richard Nixon', 'Gabibbo'], 'President Richard Nixon', \n",
    "                    'the correct answer is President Richard Nixon', \n",
    "                    source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702cc29e-5f6a-440e-a5fd-8a5bee3050f6",
   "metadata": {},
   "source": [
    "## â–ªï¸ Select a subset of the true dataset as a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bd1087d9-3e1d-444a-a250-b57fca3862d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_examples = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2e058aac-3fa5-42b7-95db-809669ac8524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a subset of the queries, just for test:\n",
    "first_queries = new_dataset['query'][:N_examples]\n",
    "\n",
    "# same for correct answers and distractors:\n",
    "correct_answers = new_dataset['correct_answer'][:N_examples]\n",
    "distractors_1 = new_dataset['distractor_1'][:N_examples]\n",
    "distractors_2 = new_dataset['distractor_2'][:N_examples]\n",
    "# and for the sources:\n",
    "sources = new_dataset['selected_passages'][:N_examples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7e39db-091e-4510-800a-0a16f2b89e76",
   "metadata": {},
   "source": [
    "## â–ªï¸ Merge the true answer and the distractors into a vector, shuffling the order of the elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cab974ea-41f0-44b3-8167-592bbd323320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffles the order of the vector containing the correct answer and the two distractors\n",
    "# returns another vector, shuffled\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Definisci una funzione di pulizia per rimuovere caratteri non validi\n",
    "def clean_text(text):\n",
    "    return re.sub(r\"[^\\w\\s.,!?\\-:;()]+\", '', text)\n",
    "\n",
    "# Funzione per mescolare le risposte\n",
    "def shuffleAnswers(correct_answer, distractor_1, distractor_2):\n",
    "    # Applica la pulizia a ciascun elemento\n",
    "    correct_answer_cleaned = clean_text(correct_answer)\n",
    "    distractor_1_cleaned = clean_text(distractor_1)\n",
    "    distractor_2_cleaned = clean_text(distractor_2)\n",
    "    \n",
    "    # Unisci le opzioni pulite\n",
    "    merge_options = [correct_answer_cleaned, distractor_1_cleaned, distractor_2_cleaned]\n",
    "    \n",
    "    # Mescola le opzioni\n",
    "    random.shuffle(merge_options)\n",
    "    \n",
    "    return merge_options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b312ecad-c2c4-4791-bc82-bfaae5f5fa04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5175f4e8-bacd-4a63-8024-9b487f80669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_options = []\n",
    "for i in range(N_examples):\n",
    "    merged_options.append(shuffleAnswers(correct_answers[i], distractors_1[i], distractors_2[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e2072a32-bdd4-4cdc-ac4d-deb117a569f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"'Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.'\",\n",
       "  '\"\\'Other Allowance\\' which is basically to compensate all and any kind of allowances which is required to be paid at different regionslocalities of the various project sites.\"',\n",
       "  \"'Webbed feet'\"],\n",
       " [\"'Yes'\",\n",
       "  \"'50, 55, 60, 65 and 70 C'\",\n",
       "  \"'A contamination which is associated with the food itself and not through other causes of contamination.'\"],\n",
       " [\"'Yes'\",\n",
       "  \"'Oatmeal, beans, apples, pears, barley and prunes.'\",\n",
       "  \"'20-25 minutes'\"],\n",
       " [\"'11 to 22 per square foot'\",\n",
       "  \"'They are fatty acids that have one double bond in the fatty acid chain with all of the remainder carbon atoms being single-bonded.'\",\n",
       "  \"'Honolulu'\"],\n",
       " [\"'A chief engineer is responsible for all operations and maintenance that has to do with any and all engineering equipment throughout the entire ship.', 'The chief engineer is responsible for the technical supervision of the development, production or operation of an engineering project for a multinational corporation, a major company or a government institution.'\",\n",
       "  \"'NigeriaCameroon'\",\n",
       "  \"'Due to symptoms in the body'\"],\n",
       " [\"'Inside the rib cage.'\", \"'45,278 and 60,659'\", \"'yes'\"],\n",
       " [\"'The most expensive patents are international patents, which can run up to 100,000 or higher.Domestically the costs can be 10,000 or above.'\",\n",
       "  \"'The slang term of nasal mucus.'\",\n",
       "  \"'Both Kit and Pup'\"],\n",
       " [\"'The front segment is called the Cephalothorax and the second part of the body is called the Abdomen.'\",\n",
       "  \"'140 to 202'\",\n",
       "  \"'Sophocles, Aeschylus and Euripides'\"],\n",
       " [\"'A tree or shrub which produces distinctive cones as part of its sexual reproduction.'\",\n",
       "  \"'Lapel pins are worn on the left, near the heart.'\",\n",
       "  \"'Is the brand name of a drug called procaine which is a local anaesthetic.'\"],\n",
       " [\"'Ecosystem'\",\n",
       "  \"'dream-like feeling, blurred vision, double vision, jerky muscle movements, dizziness, drowsiness, nausea, vomiting, loss of appetite, or sleep problems (insomnia).', 'dream-like feeling, blurred vision, double vision, jerky muscle movements, dizziness, drowsiness, nausea, vomiting, loss of appetite, or sleep problems (insomnia).'\",\n",
       "  \"'Somatic cells are produced by mitosis and gametes produced by most organisms combine to form a zygote with n pairs of chromosomes.'\"],\n",
       " [\"'Dr. Seuss'\", \"'Average 189,156'\", \"'Caribbean.'\"],\n",
       " [\"'45 minutes to an hour'\",\n",
       "  '\"After completing a 4-year bachelor\\'s degree program in biology, pre-medicine or a related field, aspiring orthopedic surgeons must complete four additional years of medical school, followed by a 4- to 5-year orthopedic surgery residency in a hospital.\"',\n",
       "  \"'Herb'\"],\n",
       " ['',\n",
       "  \"'6-16 a square foot'\",\n",
       "  \"'Candidiasis is a fungal infection caused by yeasts that belong to the genus Candida.', 'Candida is a strain of fungus that can cause an infection in your skin, among other locations.'\"],\n",
       " [\"'Granite.', 'Granite.'\",\n",
       "  \"'Automated Clearing House (ACH) is an electronic network for financial transactions in the United States.'\",\n",
       "  \"'To become a very good driver and To perform the duties of a driving examiner, you need a good combination of knowledge, skills and attitudes. But more importantly, you have to be a safe driver. To become an examiner, you will have to pass an extended version of the driving test, known as a Special Driving test.'\"],\n",
       " [\"'liver cancer,cancer of the vulva', 'Oncovirus'\",\n",
       "  \"'A geographic region is a defined area on the surface of the earth.'\",\n",
       "  \"'4.64 - 6.36'\"],\n",
       " [\"'Fish'\",\n",
       "  '\"A dolphin\\'s life span varies according to its environment and species. Although some bottlenose dolphins can reach 40 years of age, their average age is between 15 and 16 years. Forty is an old age for a dolphin -- one making it to 40 is comparable to a human living to be 100.\"',\n",
       "  \"'Dyslipidemia means an abnormal amount of lipids, or fats, in the blood.'\"],\n",
       " ['\"In a New York State hermit\\'s letter to the editor of an Adirondack Mountain newspaper.\"',\n",
       "  \"'A searching study, inquiry, or inspection'\",\n",
       "  \"'Douglas Richard Doug Flutie is a former football quarterback in Canada and America.He was born October 23, 1962 in Manchester, Maryland.'\"],\n",
       " [\"'Vice President of the United States in the scandal', 'The character of Andrew Nichols is a supporting character who made his debut in the second half of the third season. He is a major supporting character to the storyline of Fitz Grant and Mellie Grant.', 'Andrew Nichols'\",\n",
       "  \"'0.14 - 0.25 per square foot', '0.14 - 0.25 per square foot.'\",\n",
       "  \"'A macintosh made from cotton fabric treated with oil and pigment to make it waterproof.'\"],\n",
       " [\"'Moire patterns. This interference is called a moire moirÃ©. Ay in a scanned, Image moire moirÃ© patterns are caused by interference between two sets of fine, pattern grids the scanner samples and the halftone screen in the.'\",\n",
       "  \"'Xerophthalmia is a medical condition in which the eye film is reduced and the eye is incapable of producing tears.'\",\n",
       "  \"'7 days before to 7 days after the rash appears'\"],\n",
       " [\"'The Camerata are a group of four powerful and influential individuals in the city of Cloudbank. Established by Grant Kendrell and Royce Bracket, the group circumvents the official administration and democratic nature of Cloudbank, in order to establish some form of stability in the ever-changing city.'\",\n",
       "  \"'Africa'\",\n",
       "  \"'Stores food (sugar and salts in solution) and water.'\"]]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f722aa12-2d2a-4868-83ef-96f01617d4d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ðŸ”¹ PromptTemplate definition and a LLMChain for the **thesis** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "34445f5e-870a-4cbc-a1f6-1b004027426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template definition\n",
    "# requires question, options (a string containing the possible options) and the context as input variables!\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "    System: This is a chat between a user and an AI assistant. The assistant gives helpful, detailed, and polite answers to the userâ€™s questions based on the context. \n",
    "    {context}\n",
    "    User: {question}\n",
    "    Possible options: {option_a}, {option_b}, {option_c}.\n",
    "    Assistant:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcc84e0-fa8c-404d-85b4-96be5448a81d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ðŸ”¹ Function that generates the output given the prompt, the question and the set of options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "07b5e395-05a8-4ba7-a960-07e6b496aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM chain definition\n",
    "from operator import itemgetter\n",
    "\n",
    "augmentation = {\"question\": itemgetter(\"question\"),\n",
    "                \"option_a\": itemgetter(\"option_a\"), \n",
    "                \"option_b\": itemgetter(\"option_b\"),\n",
    "                \"option_c\": itemgetter(\"option_c\"),\n",
    "                \"context\": itemgetter(\"context\"), }\n",
    "\n",
    "thesis_chain = augmentation | prompt_template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "18ca079d-d85c-491f-87ab-2a0dfd626894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import outlines\n",
    "\n",
    "def thesisGeneration(query, merged, sources):\n",
    "    augmented_prompt = thesis_chain.invoke({'question': query, 'option_a': merged[0], 'option_b': merged[1], 'option_c': merged[2], 'context': sources})\n",
    "    normal_string = clean_text(augmented_prompt.text)\n",
    "    options = [merged[0], merged[1], merged[2]]\n",
    "    generator = outlines.generate.choice(new_model, options)\n",
    "    answer = generator(normal_string)\n",
    "    # if not answer:\n",
    "    #    answer = random.choice(options)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecf177e-3d1d-4e08-96d3-c9bf9144c986",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ðŸ”¹ Test: how well the thesis alone is able to perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b4cf535d-2517-452e-8ef3-821dc8524cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "for i in range(N_examples):\n",
    "    merged_options = shuffleAnswers(correct_answers[i], distractors_1[i], distractors_2[i])\n",
    "    answers.append(thesisGeneration(first_queries[i], merged_options, sources[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1f867057-7595-460f-b837-2e982e97a269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.'\",\n",
       " \"'A contamination which is associated with the food itself and not through other causes of contamination.'\",\n",
       " \"'20-25 minutes'\",\n",
       " \"'11 to 22 per square foot'\",\n",
       " \"'A chief engineer is responsible for all operations and maintenance that has to do with any and all engineering equipment throughout the entire ship.', 'The chief engineer is responsible for the technical supervision of the development, production or operation of an engineering project for a multinational corporation, a major company or a government institution.'\"]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e222ef24-3231-4ee1-ae92-4b1b67551500",
   "metadata": {},
   "source": [
    "# Antithesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d1f07e56-c78f-4244-9c07-8efd21806e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_options = []\n",
    "for i in range(N_examples):\n",
    "    merged_options.append(shuffleAnswers(correct_answers[i], distractors_1[i], distractors_2[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fc7617-7074-49ec-a1db-2d86416aeca2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ðŸ”¸ PromptTemplate definition and a LLMChain for the **antithesis** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9e21293c-f26c-4c36-b5b0-dc3cffe4c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    repetition_penalty=1.5,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=400,\n",
    "    top_p=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "33184819-91d2-42f0-97ab-44daab45f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def query_model(\n",
    "        system_message,\n",
    "        user_message,\n",
    "        temperature = 0.0,\n",
    "        max_length=1024\n",
    "        ):\n",
    "\n",
    "    user_message = \"Question: \" + user_message + \" Answer:\"\n",
    "    messages = [\n",
    "        {\"role\": \"System\", \"content\": system_message},\n",
    "        {\"role\": \"User\", \"content\": user_message},\n",
    "        ]\n",
    "    prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "        )\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    sequences = pipeline(\n",
    "        prompt,\n",
    "        do_sample=False,\n",
    "        top_p=0.0,\n",
    "        temperature=temperature,\n",
    "        #num_return_sequences=1,\n",
    "        eos_token_id=terminators,\n",
    "        max_new_tokens=max_length,\n",
    "        return_full_text=False,\n",
    "        pad_token_id=pipeline.model.config.eos_token_id\n",
    "    )\n",
    "\n",
    "    answer = sequences[0]['generated_text']\n",
    "\n",
    "    return user_message + \" \" + answer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c29e2fd3-113f-4780-8ccf-a8b9e82e850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "    This is a chat between a user and an AI assistant. The assistant gives helpful, detailed, and polite answers to the userâ€™s questions based on the context.\n",
    "    {context}\n",
    "    The assistant is asked to check whether or not a question was answered correctly, given a certain number of candidate options and the context.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "user_message = \"\"\"\n",
    "    Question: {question}? The candidate answer for the question has to be one of these: {option_a}, {option_b}, {option_c}.\n",
    "    It has been previously suggested that {candidate_answer} could be the correct answer. \n",
    "    Which of the candidate answers is the most proper answer for the question? Why? You can also say \"I don't know\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a643ed8f-0d35-44b4-84cd-293816c40ed4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ðŸ”¸ Function to generate the antithesis given the question, the thesis, the context and the options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b5ffee32-0ea6-439d-9ec5-1440dd58b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def antithesisGeneration(query, prompt_template, merged, candidate_answer, sources):\n",
    "    second_answer = query_model(system_message.format(context = sources),\n",
    "    user_message.format(question=query, option_a = merged[0], option_b = merged[1], \n",
    "                        option_c = merged[2], candidate_answer = candidate_answer, context = sources,), max_length=400)\n",
    "    return second_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bdf63893-af51-4f42-8e8b-6de9403fd05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer_ant(text):\n",
    "    # Trova l'indice in cui inizia il testo \"Why or why not the answer is correct:\"\n",
    "    start_index = text.find(\"Answer:\")\n",
    "\n",
    "    \n",
    "    # Se l'indice Ã¨ stato trovato, estrai la risposta corretta\n",
    "    if start_index != -1:\n",
    "        start_index += len(\"Answer:\")\n",
    "        # Estrai il testo dopo \"Why or why not the answer is correct:\"\n",
    "        correct_answer_text = text[start_index:].strip()\n",
    "        return correct_answer_text\n",
    "    else:\n",
    "        return \"The correct answer could not be found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ec71c35b-0598-429f-a99d-df9e82da06b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/pipelines/base.py:1167: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ant_answers = []\n",
    "for i in range(N_examples):\n",
    "    ant_answers.append(extract_answer_ant(antithesisGeneration(first_queries[i], prompt_template, merged_options[i], answers[i], sources[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "93695a06-214d-4bc0-9f41-219192027cac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The passage text clearly states Results Based accountability(RBAs), it means its just another name called RBa',\n",
       " 'The passage text states clearly about how he changed his political affiliation before becoming president',\n",
       " 'The reference text states,\"A Week In Sydny Will Help See Many Of Its Sights And Understand City Culture.\"',\n",
       " 'The average costs range from eleven dollars ($11) up until twenty-two dollar($21).',\n",
       " \"'Due To Symptoms In Body'\"]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ant_answers[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6b22ec6b-32a7-4c40-b6d7-518a7b99fde6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what is rba',\n",
       " 'was ronald reagan a democrat',\n",
       " 'how long do you need for sydney and surrounding areas',\n",
       " 'price to install tile in shower',\n",
       " 'why conversion observed in body']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_queries[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f600cfb8-6a35-4baa-8236-c02c2909868c",
   "metadata": {},
   "source": [
    "# Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1edb95-c057-4d4b-8a40-c24f578e6c6b",
   "metadata": {},
   "source": [
    "## ðŸ”º PromptTemplate definition and a LLMChain for the **synthesis** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6091cee6-41d3-4164-804e-f6c492e88745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"You are a multiple-choice question answering assistant.\n",
    "Choose which of the following options: {option_a}, {option_b}, {option_c} is most likely the answer to a question. \n",
    "You have also an attempt of answer, an opinion on which answer is the most appropriate and the context.\n",
    "\n",
    "Question: {question}\n",
    "Attempt of answer: {candidate_answer}\n",
    "Opinion: {critique}\n",
    "Context: {context}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# LLM chain definition\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from operator import itemgetter\n",
    "\n",
    "augmentation = {\"question\": itemgetter(\"question\"),\n",
    "                \"option_a\": itemgetter(\"option_a\"), \n",
    "                \"option_b\": itemgetter(\"option_b\"),\n",
    "                \"option_c\": itemgetter(\"option_c\"),\n",
    "                \"candidate_answer\": itemgetter(\"candidate_answer\"),\n",
    "                \"critique\": itemgetter(\"critique\"),\n",
    "                \"context\": itemgetter(\"context\"), }\n",
    "\n",
    "synthesis_chain = augmentation | prompt_template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "50e0e37b-1b37-40a5-a9de-cf5a6a5dff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"You are a multiple-choice question answering assistant.\n",
    "Choose which of the following options: {option_a}, {option_b}, {option_c} is most likely the answer to a question. \n",
    "You have also an advice on which answer is the most appropriate and the context.\n",
    "Choose the option that is most similar to the advice.\n",
    "\n",
    "Question: {question}\n",
    "Advice: {critique}\n",
    "Context: {context}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "augmentation = {\"question\": itemgetter(\"question\"),\n",
    "                \"option_a\": itemgetter(\"option_a\"), \n",
    "                \"option_b\": itemgetter(\"option_b\"),\n",
    "                \"option_c\": itemgetter(\"option_c\"),\n",
    "                \"candidate_answer\": itemgetter(\"candidate_answer\"),\n",
    "                \"critique\": itemgetter(\"critique\"),\n",
    "                \"context\": itemgetter(\"context\"), }\n",
    "\n",
    "synthesis_chain = augmentation | prompt_template "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8170bc0d-1182-4c31-b33d-65fe64e1c385",
   "metadata": {},
   "source": [
    "## ðŸ”º Function to generate the synthesis given literally everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b5cf33b3-082d-48c1-8cd7-0e6639490222",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = samplers.greedy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "38bef727-d240-4511-a556-0d2c19db4fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesisGeneration(query, prompt_template, merged, candidate_answer, critique, sources):\n",
    "    augmented_prompt = synthesis_chain.invoke({'question': query, \n",
    "                                            'option_a': merged[0], 'option_b': merged[1], 'option_c': merged[2], \n",
    "                                            'candidate_answer': candidate_answer,\n",
    "                                            'critique': critique,\n",
    "                                            'context': sources})\n",
    "\n",
    "    normal_string = clean_text(augmented_prompt.text)\n",
    "    options = [merged[0], merged[1], merged[2]]\n",
    "    generator = outlines.generate.choice(new_model, options, sampler = sampler)\n",
    "    final_answer = generator(normal_string)\n",
    "    \n",
    "    # if not final_answer:\n",
    "    #     final_answer = candidate_answer\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "538c897d-183c-40eb-a9e2-c05e53addd03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.is correct',\n",
       " 'Yesis correct',\n",
       " '20-25 minutesis correct',\n",
       " '11 to 22 per square footis correct',\n",
       " 'Due to symptoms in the bodyis correct',\n",
       " 'Inside the rib cage.is correct',\n",
       " 'The most expensive patents are international patents, which can run up to 100,000 or higher.Domestically the costs can be 10,000 or above.is correct',\n",
       " 'Sophocles, Aeschylus and Euripidesis correct',\n",
       " 'A tree or shrub which produces distinctive cones as part of its sexual reproduction.is correct',\n",
       " 'Somatic cells are produced by mitosis and gametes produced by most organisms combine to form a zygote with n pairs of chromosomes.is correct',\n",
       " 'Dr. Seussis correct',\n",
       " '45 minutes to an houris correct',\n",
       " '6-16 a square footis correct',\n",
       " 'Granite., Granite.is correct',\n",
       " '4.64 - 6.36is correct',\n",
       " 'Fishis correct',\n",
       " 'In a New York State hermits letter to the editor of an Adirondack Mountain newspaper.is correct',\n",
       " 'A macintosh made from cotton fabric treated with oil and pigment to make it waterproof.is correct',\n",
       " '7 days before to 7 days after the rash appearsis correct',\n",
       " 'The Camerata are a group of four powerful and influential individuals in the city of Cloudbank. Established by Grant Kendrell and Royce Bracket, the group circumvents the official administration and democratic nature of Cloudbank, in order to establish some form of stability in the ever-changing city.is correct']"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def_answers = [clean_text(correct_answer) + \"is correct\" for correct_answer in correct_answers]\n",
    "def_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "ca4286d7-99cf-4f0c-a320-d9b3c6f84863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling FSM index for all state transitions: 100%|â–ˆ| 373/373 [00:18<00:00, 20.68it/s\n",
      "Compiling FSM index for all state transitions: 100%|â–ˆâ–ˆ| 65/65 [00:03<00:00, 20.66it/s]\n",
      "Compiling FSM index for all state transitions: 100%|â–ˆ| 164/164 [00:07<00:00, 20.83it/s\n",
      "Compiling FSM index for all state transitions: 100%|â–ˆ| 408/408 [00:19<00:00, 20.95it/s\n",
      "Compiling FSM index for all state transitions: 100%|â–ˆâ–ˆ| 42/42 [00:02<00:00, 20.67it/s]\n",
      "Compiling FSM index for all state transitions: 100%|â–ˆ| 184/184 [00:08<00:00, 20.84it/s\n",
      "Compiling FSM index for all state transitions: 100%|â–ˆ| 146/146 [00:06<00:00, 20.93it/s\n",
      "Compiling FSM index for all state transitions: 100%|â–ˆ| 205/205 [00:09<00:00, 20.80it/s\n",
      "Compiling FSM index for all state transitions: 100%|â–ˆ| 467/467 [00:22<00:00, 20.99it/s\n",
      "Compiling FSM index for all state transitions: 100%|â–ˆâ–ˆ| 39/39 [00:01<00:00, 20.79it/s]\n",
      "Compiling FSM index for all state transitions: 100%|â–ˆ| 277/277 [00:13<00:00, 20.91it/s\n",
      "Compiling FSM index for all state transitions: 100%|â–ˆ| 441/441 [00:21<00:00, 20.68it/s\n",
      "Compiling FSM index for all state transitions: 100%|â–ˆ| 128/128 [00:06<00:00, 20.37it/s\n",
      "Compiling FSM index for all state transitions: 100%|â–ˆ| 358/358 [00:17<00:00, 20.73it/s\n",
      "Compiling FSM index for all state transitions: 100%|â–ˆ| 266/266 [00:12<00:00, 20.86it/s\n",
      "Compiling FSM index for all state transitions: 100%|â–ˆ| 362/362 [00:17<00:00, 20.86it/s\n"
     ]
    }
   ],
   "source": [
    "syn_answers = []\n",
    "for i in range(20):\n",
    "    merged_options = shuffleAnswers(correct_answers[i], distractors_1[i], distractors_2[i])\n",
    "    syn_answers.append(synthesisGeneration(first_queries[i], prompt_template, merged_options, correct_answers[i], def_answers[i], sources[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "e0dd28ab-5af1-45c4-a311-02694b1def51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'They are fatty acids that have one double bond in the fatty acid chain with all of the remainder carbon atoms being single-bonded.',\n",
       " 'A chief engineer is responsible for all operations and maintenance that has to do with any and all engineering equipment throughout the entire ship., The chief engineer is responsible for the technical supervision of the development, production or operation of an engineering project for a multinational corporation, a major company or a government institution.',\n",
       " 'Inside the rib cage.',\n",
       " 'The most expensive patents are international patents, which can run up to 100,000 or higher.Domestically the costs can be 10,000 or above.',\n",
       " 'The front segment is called the Cephalothorax and the second part of the body is called the Abdomen.',\n",
       " 'Is the brand name of a drug called procaine which is a local anaesthetic.',\n",
       " 'Somatic cells are produced by mitosis and gametes produced by most organisms combine to form a zygote with n pairs of chromosomes.',\n",
       " 'Dr. Seuss',\n",
       " 'After completing a 4-year bachelors degree program in biology, pre-medicine or a related field, aspiring orthopedic surgeons must complete four additional years of medical school, followed by a 4- to 5-year orthopedic surgery residency in a hospital.',\n",
       " 'Candidiasis is a fungal infection caused by yeasts that belong to the genus Candida., Candida is a strain of fungus that can cause an infection in your skin, among other locations.',\n",
       " 'Automated Clearing House ACH is an electronic network for financial transactions in the United States.',\n",
       " 'A geographic region is a defined area on the surface of the earth.',\n",
       " 'A dolphins life span varies according to its environment and species. Although some bottlenose dolphins can reach 40 years of age, their average age is between 15 and 16 years. Forty is an old age for a dolphin -- one making it to 40 is comparable to a human living to be 100.',\n",
       " 'A searching study, inquiry, or inspection',\n",
       " '0.14 - 0.25 per square foot, 0.14 - 0.25 per square foot.',\n",
       " 'Moire patterns. This interference is called a moire moirÃ©. Ay in a scanned, Image moire moirÃ© patterns are caused by interference between two sets of fine, pattern grids the scanner samples and the halftone screen in the.',\n",
       " 'The Camerata are a group of four powerful and influential individuals in the city of Cloudbank. Established by Grant Kendrell and Royce Bracket, the group circumvents the official administration and democratic nature of Cloudbank, in order to establish some form of stability in the ever-changing city.']"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "07409f6e-7032-4456-b407-e14e3b3d9ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'NigeriaCameroon'\""
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_options = [clean_text(correct_answers[4]), clean_text(distractors_1[4]), clean_text(distractors_2[4])]\n",
    "\n",
    "synthesisGeneration(clean_text(first_queries[4]), prompt_template, merged_options, \n",
    "                    clean_text(correct_answers[4]), 'answer by saying Due to symptoms', sources[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "65f3176f-7f6b-4c55-a4da-c984771bca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_new = {'query': first_queries, 'correct answer': correct_answers, 'thesis': answers, 'antithesis': ant_answers, 'synthesis': syn_answers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ae959b96-7e8f-445e-8ddb-94e0a49116e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(dataset_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "612fcd8b-db5f-4d4a-b816-b043a28d2435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>correct answer</th>\n",
       "      <th>thesis</th>\n",
       "      <th>antithesis</th>\n",
       "      <th>synthesis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is rba</td>\n",
       "      <td>['Results-Based Accountability is a discipline...</td>\n",
       "      <td>'Results-Based Accountability is a disciplined...</td>\n",
       "      <td>The text clearly states \"RBA\" stands for Resul...</td>\n",
       "      <td>'Results-Based Accountability is a disciplined...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>was ronald reagan a democrat</td>\n",
       "      <td>['Yes']</td>\n",
       "      <td>'Yes'</td>\n",
       "      <td>The text explicitly states that \"[f]romm tradi...</td>\n",
       "      <td>'Yes'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how long do you need for sydney and surroundin...</td>\n",
       "      <td>['20-25 minutes']</td>\n",
       "      <td>'Oatmeal, beans, apples, pears, barley and pru...</td>\n",
       "      <td>The text states that taking public transportat...</td>\n",
       "      <td>'Oatmeal, beans, apples, pears, barley and pru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>price to install tile in shower</td>\n",
       "      <td>['$11 to $22 per square foot']</td>\n",
       "      <td>'11 to 22 per square foot'</td>\n",
       "      <td>The reference text states \"Average costs range...</td>\n",
       "      <td>'11 to 22 per square foot'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>why conversion observed in body</td>\n",
       "      <td>['Due to symptoms in the body']</td>\n",
       "      <td>'Due to symptoms in the body'</td>\n",
       "      <td>Yes\\n\\nExplanation:\\nSomatization Disorder (SD...</td>\n",
       "      <td>'NigeriaCameroon'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>where are the lungs located in the back</td>\n",
       "      <td>['Inside the rib cage.']</td>\n",
       "      <td>'Inside the rib cage.'</td>\n",
       "      <td>The reference text states that \"This [the loca...</td>\n",
       "      <td>'yes'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cost to get a patent</td>\n",
       "      <td>['The most expensive patents are international...</td>\n",
       "      <td>'The most expensive patents are international ...</td>\n",
       "      <td>\"The most expensiv epatents arerinternational ...</td>\n",
       "      <td>'Both Kit and Pup'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>best tragedies of ancient greece</td>\n",
       "      <td>['Sophocles, Aeschylus and Euripides']</td>\n",
       "      <td>'Sophocles, Aeschylus and Euripides'</td>\n",
       "      <td>Yes</td>\n",
       "      <td>'The front segment is called the Cephalothorax...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>what is a conifer</td>\n",
       "      <td>['A tree or shrub which produces distinctive c...</td>\n",
       "      <td>'A tree or shrub which produces distinctive co...</td>\n",
       "      <td>The text explicitly states that \"a Confer\" ref...</td>\n",
       "      <td>'Is the brand name of a drug called procaine w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>in animals somatic cells are produced by and g...</td>\n",
       "      <td>['Somatic cells are produced by mitosis and ga...</td>\n",
       "      <td>'dream-like feeling, blurred vision, double vi...</td>\n",
       "      <td>The reference text states that \"somatic\" refer...</td>\n",
       "      <td>'dream-like feeling, blurred vision, double vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>remembering the name of the author who wrote t...</td>\n",
       "      <td>['Dr. Seuss']</td>\n",
       "      <td>'Dr. Seuss'</td>\n",
       "      <td>Yes</td>\n",
       "      <td>'Caribbean.'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>how long cooking chicken legs in the big easy</td>\n",
       "      <td>['45 minutes to an hour']</td>\n",
       "      <td>\"After completing a 4-year bachelor's degree p...</td>\n",
       "      <td>The text does provide information regarding di...</td>\n",
       "      <td>\"After completing a 4-year bachelor's degree p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>average cost of heating per square foot</td>\n",
       "      <td>['$6-$16 a square foot']</td>\n",
       "      <td>'6-16 a square foot'</td>\n",
       "      <td>The reference text states \"Expect to pay aroun...</td>\n",
       "      <td>'6-16 a square foot'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>is mount pinatubo made of granite or basalt</td>\n",
       "      <td>['Granite.', 'Granite.']</td>\n",
       "      <td>'Granite.', 'Granite.'</td>\n",
       "      <td>The text mentions that Mount Pinautobo contain...</td>\n",
       "      <td>'To become a very good driver and To perform t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>concrete pads cost</td>\n",
       "      <td>['$4.64 - $6.36']</td>\n",
       "      <td>'4.64 - 6.36'</td>\n",
       "      <td>The reference text states that \"For a basic **...</td>\n",
       "      <td>'4.64 - 6.36'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>what kind of organism is a black damsel</td>\n",
       "      <td>['Fish']</td>\n",
       "      <td>'Dyslipidemia means an abnormal amount of lipi...</td>\n",
       "      <td>The text does mention about dysplipidaemias bu...</td>\n",
       "      <td>\"A dolphin's life span varies according to its...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>who coined the phrase it is what it is</td>\n",
       "      <td>[\"In a New York State hermit's letter to the e...</td>\n",
       "      <td>\"In a New York State hermit's letter to the ed...</td>\n",
       "      <td>\"The\" would have been better than just using p...</td>\n",
       "      <td>'Douglas Richard Doug Flutie is a former footb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>what is oilskin fabric</td>\n",
       "      <td>['A macintosh made from cotton fabric treated ...</td>\n",
       "      <td>'A macintosh made from cotton fabric treated w...</td>\n",
       "      <td>Yes\\n\\nExplanation:\\nOil skin refers to A MacI...</td>\n",
       "      <td>'A macintosh made from cotton fabric treated w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>how long is german measles contagious</td>\n",
       "      <td>['7 days before to 7 days after the rash appea...</td>\n",
       "      <td>'Moire patterns. This interference is called a...</td>\n",
       "      <td>The reference text states that \"German Measles...</td>\n",
       "      <td>'7 days before to 7 days after the rash appears'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>what is a camerata</td>\n",
       "      <td>['The Camerata are a group of four powerful an...</td>\n",
       "      <td>'The Camerata are a group of four powerful and...</td>\n",
       "      <td>Yes\\n\\nExplanation:\\n\"The Camarate\" refers spe...</td>\n",
       "      <td>'The Camerata are a group of four powerful and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  \\\n",
       "0                                         what is rba   \n",
       "1                        was ronald reagan a democrat   \n",
       "2   how long do you need for sydney and surroundin...   \n",
       "3                     price to install tile in shower   \n",
       "4                     why conversion observed in body   \n",
       "5             where are the lungs located in the back   \n",
       "6                                cost to get a patent   \n",
       "7                    best tragedies of ancient greece   \n",
       "8                                   what is a conifer   \n",
       "9   in animals somatic cells are produced by and g...   \n",
       "10  remembering the name of the author who wrote t...   \n",
       "11      how long cooking chicken legs in the big easy   \n",
       "12            average cost of heating per square foot   \n",
       "13        is mount pinatubo made of granite or basalt   \n",
       "14                                 concrete pads cost   \n",
       "15            what kind of organism is a black damsel   \n",
       "16             who coined the phrase it is what it is   \n",
       "17                             what is oilskin fabric   \n",
       "18              how long is german measles contagious   \n",
       "19                                 what is a camerata   \n",
       "\n",
       "                                       correct answer  \\\n",
       "0   ['Results-Based Accountability is a discipline...   \n",
       "1                                             ['Yes']   \n",
       "2                                   ['20-25 minutes']   \n",
       "3                      ['$11 to $22 per square foot']   \n",
       "4                     ['Due to symptoms in the body']   \n",
       "5                            ['Inside the rib cage.']   \n",
       "6   ['The most expensive patents are international...   \n",
       "7              ['Sophocles, Aeschylus and Euripides']   \n",
       "8   ['A tree or shrub which produces distinctive c...   \n",
       "9   ['Somatic cells are produced by mitosis and ga...   \n",
       "10                                      ['Dr. Seuss']   \n",
       "11                          ['45 minutes to an hour']   \n",
       "12                           ['$6-$16 a square foot']   \n",
       "13                           ['Granite.', 'Granite.']   \n",
       "14                                  ['$4.64 - $6.36']   \n",
       "15                                           ['Fish']   \n",
       "16  [\"In a New York State hermit's letter to the e...   \n",
       "17  ['A macintosh made from cotton fabric treated ...   \n",
       "18  ['7 days before to 7 days after the rash appea...   \n",
       "19  ['The Camerata are a group of four powerful an...   \n",
       "\n",
       "                                               thesis  \\\n",
       "0   'Results-Based Accountability is a disciplined...   \n",
       "1                                               'Yes'   \n",
       "2   'Oatmeal, beans, apples, pears, barley and pru...   \n",
       "3                          '11 to 22 per square foot'   \n",
       "4                       'Due to symptoms in the body'   \n",
       "5                              'Inside the rib cage.'   \n",
       "6   'The most expensive patents are international ...   \n",
       "7                'Sophocles, Aeschylus and Euripides'   \n",
       "8   'A tree or shrub which produces distinctive co...   \n",
       "9   'dream-like feeling, blurred vision, double vi...   \n",
       "10                                        'Dr. Seuss'   \n",
       "11  \"After completing a 4-year bachelor's degree p...   \n",
       "12                               '6-16 a square foot'   \n",
       "13                             'Granite.', 'Granite.'   \n",
       "14                                      '4.64 - 6.36'   \n",
       "15  'Dyslipidemia means an abnormal amount of lipi...   \n",
       "16  \"In a New York State hermit's letter to the ed...   \n",
       "17  'A macintosh made from cotton fabric treated w...   \n",
       "18  'Moire patterns. This interference is called a...   \n",
       "19  'The Camerata are a group of four powerful and...   \n",
       "\n",
       "                                           antithesis  \\\n",
       "0   The text clearly states \"RBA\" stands for Resul...   \n",
       "1   The text explicitly states that \"[f]romm tradi...   \n",
       "2   The text states that taking public transportat...   \n",
       "3   The reference text states \"Average costs range...   \n",
       "4   Yes\\n\\nExplanation:\\nSomatization Disorder (SD...   \n",
       "5   The reference text states that \"This [the loca...   \n",
       "6   \"The most expensiv epatents arerinternational ...   \n",
       "7                                                 Yes   \n",
       "8   The text explicitly states that \"a Confer\" ref...   \n",
       "9   The reference text states that \"somatic\" refer...   \n",
       "10                                                Yes   \n",
       "11  The text does provide information regarding di...   \n",
       "12  The reference text states \"Expect to pay aroun...   \n",
       "13  The text mentions that Mount Pinautobo contain...   \n",
       "14  The reference text states that \"For a basic **...   \n",
       "15  The text does mention about dysplipidaemias bu...   \n",
       "16  \"The\" would have been better than just using p...   \n",
       "17  Yes\\n\\nExplanation:\\nOil skin refers to A MacI...   \n",
       "18  The reference text states that \"German Measles...   \n",
       "19  Yes\\n\\nExplanation:\\n\"The Camarate\" refers spe...   \n",
       "\n",
       "                                            synthesis  \n",
       "0   'Results-Based Accountability is a disciplined...  \n",
       "1                                               'Yes'  \n",
       "2   'Oatmeal, beans, apples, pears, barley and pru...  \n",
       "3                          '11 to 22 per square foot'  \n",
       "4                                   'NigeriaCameroon'  \n",
       "5                                               'yes'  \n",
       "6                                  'Both Kit and Pup'  \n",
       "7   'The front segment is called the Cephalothorax...  \n",
       "8   'Is the brand name of a drug called procaine w...  \n",
       "9   'dream-like feeling, blurred vision, double vi...  \n",
       "10                                       'Caribbean.'  \n",
       "11  \"After completing a 4-year bachelor's degree p...  \n",
       "12                               '6-16 a square foot'  \n",
       "13  'To become a very good driver and To perform t...  \n",
       "14                                      '4.64 - 6.36'  \n",
       "15  \"A dolphin's life span varies according to its...  \n",
       "16  'Douglas Richard Doug Flutie is a former footb...  \n",
       "17  'A macintosh made from cotton fabric treated w...  \n",
       "18   '7 days before to 7 days after the rash appears'  \n",
       "19  'The Camerata are a group of four powerful and...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8f34f7dc-6407-4f3b-b601-d86c5296d0dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Candidate response matches reference text exactly; therefore it must have been correct'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['antithesis'][199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c045384f-b1da-44e4-a995-3aeff4ad682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('test-4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5856d161-623d-4258-bd8c-fbe224f1b286",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
