{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b5bb0e0-4880-4cb3-80c0-f7f66173e2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    ")\n",
    "from guidance import models, select\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from operator import itemgetter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e261b683-5d2b-4367-8072-5f38fedaed43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisci una funzione di pulizia per rimuovere caratteri non validi\n",
    "def clean_text(text):\n",
    "    return re.sub(r\"[^\\w\\s.,!?\\-:;()]+\", '', text)\n",
    "\n",
    "# Definisci una funzione di pulizia per rimuovere caratteri non validi\n",
    "def clean_text_final(text):\n",
    "    text = re.sub(r'[^\\w\\s.,!?\\'\"\\-:;()]+', '', text)  # Rimuove i caratteri speciali\n",
    "    text = re.sub(r\"['\\\"-]\", '', text)  # Rimuove apostrofi, virgolette e trattini\n",
    "    text = text.lower()  # Converte in minuscolo\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "199fb981-a727-4ebb-8a2f-6c108da3a352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt augmentation for the (format of the) synthesis:\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"You are a multiple-choice question answering assistant.\n",
    "Choose the most proper option between {options} that best matches with the suggestion. \n",
    "\n",
    "Question: {question}\n",
    "Context: {critique}\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    ")\n",
    "augmentation = {\"question\": itemgetter(\"question\"),\n",
    "                \"options\": itemgetter(\"options\"), \n",
    "                \"critique\": itemgetter(\"critique\"),}\n",
    "synthesis_chain = augmentation | prompt_template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee20958d-fd82-4465-9123-eca3d28cf836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_message_thesis(question, options):\n",
    "    options_str = '\", \"'.join(options)\n",
    "    content = f\"\"\"\n",
    "\n",
    "    Now do the same for this question: \"{question}\", where options: [\"{options_str}\"]. Assistant:\n",
    "    \"\"\"\n",
    "\n",
    "    user_content = \"Answer to the following question: \" + question + \" providing one of these options as answer: \" + str(options) + \"Assistant:\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"\n",
    "        You are an helpful AI assistant. You have to provide helpful answers to the user’s questions. \n",
    "        \"\"\"},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "\n",
    "    return messages\n",
    "\n",
    "def extract_answer_thesis(text):\n",
    "    # Trova l'indice in cui inizia il testo \"Why or why not the answer is correct:\"\n",
    "    start_index = text.find(\"}]\")\n",
    "\n",
    "    \n",
    "    # Se l'indice è stato trovato, estrai la risposta corretta\n",
    "    if start_index != -1:\n",
    "        start_index += len(\"}]\")\n",
    "        # Estrai il testo dopo \"Why or why not the answer is correct:\"\n",
    "        correct_answer_text = text[start_index:].strip()\n",
    "        return correct_answer_text\n",
    "    else:\n",
    "        return \"The correct answer could not be found.\"\n",
    "\n",
    "def thesisGeneration(query, merged):\n",
    "    merged = ast.literal_eval(merged)\n",
    "    augmented_prompt = create_message_thesis(query, merged)\n",
    "    ans = new_model + str(augmented_prompt) + select(merged)\n",
    "    return str(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e35dce3-0e81-4133-9fa8-1bf569310623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_message_antithesis(question, candidate, options):\n",
    "    options_str = '\", \"'.join(options)\n",
    "    content = f\"\"\"\n",
    "\n",
    "    Now do the same for this question: \"{question}\", where options: [\"{options_str}\"]. Assistant:\n",
    "    \"\"\"\n",
    "\n",
    "    user_content = \"Question: \" + question + \"\\n Options: \" + str(options) + \"\\n Candidate answer: \" + candidate + \"\\n Assistant: \\n\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"\"\"\n",
    "        You are an helpful AI assistant. You are asked to determine the most correct answer for a given question, provided a set of possible options.\n",
    "        You also have at disposal a first tentative answer that you are required to check with respect to the question and the relevant context.\n",
    "        Your goal is to decree which is the most correct answer to the question between the available options.\n",
    "\n",
    "        Here's an example of how to do it:\n",
    "        Question: What is the sun, a star or a planet?\n",
    "        Options: ['a star', 'a planet']\n",
    "        Candidate answer: a planet\n",
    "        \"\"\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"\"\"\n",
    "        The correct answer should be 'a star' since it is surely not a planet.\n",
    "        \"\"\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Now do the same for the following question: \\n\" + user_content}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt\n",
    "\n",
    "def extract_antithesis(text):\n",
    "    pattern = re.compile(r'<start_of_turn>model(.*?)<end_of_turn>', re.DOTALL)\n",
    "    matches = pattern.findall(text)\n",
    "    \n",
    "    if matches:\n",
    "        # Prendi l'ultimo match\n",
    "        extracted_text = matches[-1]\n",
    "        # Rimuovi i simboli \"_\"\n",
    "        cleaned_text = extracted_text.replace('▁', '').strip()\n",
    "        return cleaned_text\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def antithesisGeneration(query, merged, candidate):\n",
    "    merged = ast.literal_eval(merged)\n",
    "    prompt = create_message_antithesis(query, candidate, merged)\n",
    "    inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=500)\n",
    "    return extract_antithesis(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a16e30e-cd80-4a50-9641-9b0195657c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_message_presynthesis(question, candidate, suggestion, options):\n",
    "\n",
    "    user_content = \"Question: \" + question + \"\\n Options: \" + str(options) + \"\\n Candidate answer: \" + candidate + \"\\n Suggestion: \" + suggestion + \"\\n Assistant: \\n\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"\"\"\n",
    "        You are an helpful AI assistant. You are asked to determine the most correct answer for a given question, provided a set of possible options.\n",
    "        You also have at disposal a first tentative answer and a suggestion on which is the correct answer.\n",
    "        Your goal is to decree which is the most correct answer to the question between the available options according to the context.\n",
    "\n",
    "        Here's an example of how to do it:\n",
    "        Question: What is the sun, a star or a planet?\n",
    "        Options: ['a star', 'a planet']\n",
    "        Candidate answer: a planet\n",
    "        Suggestion: a star is the correct option since the context clearly specifies that the Sun is the star at the center of the Solar System\n",
    "        \"\"\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"\"\"\n",
    "        The correct option is 'a star', since the suggestion is grounded in the context, even if the candidate answer does not agree.\n",
    "        \"\"\"\n",
    "        },\n",
    "            {\"role\": \"user\", \"content\": \"Now do the same for the following question: \"+ user_content}\n",
    "        ]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt\n",
    "\n",
    "def preSynthGeneration(query, candidate_answer, critique, merged):\n",
    "    prompt = create_message_presynthesis(query, merged, candidate_answer, critique)\n",
    "    inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=500)\n",
    "    return extract_antithesis(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7df54d1a-2f54-4161-a2f4-8376892684d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesisGeneration(query, merged, pre_answer):\n",
    "    merged = ast.literal_eval(merged)\n",
    "    augmented_prompt = synthesis_chain.invoke({'question': query, \n",
    "                                            'options': merged,\n",
    "                                            'critique': pre_answer})\n",
    "\n",
    "    normal_string = clean_text(augmented_prompt.text)\n",
    "    ans = new_model + normal_string + select(merged)\n",
    "    return str(ans)\n",
    "\n",
    "def extract_answer_synthesis(text):\n",
    "    # Trova l'indice in cui inizia il testo \"Why or why not the answer is correct:\"\n",
    "    start_index = text.find(\"Assistant:\\n\")\n",
    "\n",
    "    \n",
    "    # Se l'indice è stato trovato, estrai la risposta corretta\n",
    "    if start_index != -1:\n",
    "        start_index += len(\"Assistant:\\n\")\n",
    "        # Estrai il testo dopo \"Why or why not the answer is correct:\"\n",
    "        correct_answer_text = text[start_index:].strip()\n",
    "        return correct_answer_text\n",
    "    else:\n",
    "        return \"The correct answer could not be found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5290cfd-71c9-41da-b7c5-4eedf3dfc997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cadad502691645ae96a37574148c002a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\", use_fast = False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "new_model = models.Transformers(model, tokenizer, temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17402b85-f9ad-4f27-b39a-244dd9bb4a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('saracandu/hotpotQA_nli', split=\"train\", trust_remote_code=True)\n",
    "\n",
    "# select a subset of the queries, just for test:\n",
    "first_queries = dataset['question']\n",
    "\n",
    "# same for correct answers and distractors:\n",
    "correct_answers = dataset['answer']\n",
    "possibilities = dataset['options']\n",
    "\n",
    "#nli\n",
    "first_nli = dataset['first nli']\n",
    "second_nli = dataset['second nli']\n",
    "\n",
    "bart1 = dataset['BART1']\n",
    "bart2 = dataset['BART2']\n",
    "\n",
    "rob1 = dataset['ROBERTA1']\n",
    "rob2 = dataset['ROBERTA2']\n",
    "\n",
    "N_rows = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c75e5f53-1be9-4add-9308-4c71913a6291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>You are a multiple-choice question answering assistant.\n",
       "Choose the most proper option between yes, no that best matches with the suggestion. \n",
       "\n",
       "Question: Are both The New Pornographers and Kings of Leon American rock bands?\n",
       "Context: The correct answer is no. \n",
       "\n",
       "Explanation:\n",
       "\n",
       "The candidate answer is incorrect because it assumes the question is about the bands genre, not their nationality.The question specifically asks about their nationality, and the provided information about their origins is relevant to the question.\n",
       "\n",
       "Assistant:\n",
       "<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>yes</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# THESIS\n",
    "answers = []\n",
    "for i in range(N_rows):\n",
    "    answers.append(extract_answer_thesis(thesisGeneration(first_queries[i], possibilities[i])))\n",
    "\n",
    "\n",
    "# ANTITHESIS\n",
    "ant_answers = []\n",
    "for i in range(N_rows):\n",
    "    ant_answers.append(antithesisGeneration(first_queries[i], possibilities[i], answers[i]))\n",
    "\n",
    "# SYNTHESIS\n",
    "pre_answers = []\n",
    "for i in range(N_rows):\n",
    "    pre_answers.append(preSynthGeneration(first_queries[i], possibilities[i], answers[i], ant_answers[i]))\n",
    "\n",
    "\n",
    "# format synthesis\n",
    "syn_answers = []\n",
    "for i in range(N_rows):\n",
    "    syn_answers.append(extract_answer_synthesis(\n",
    "        synthesisGeneration(\n",
    "            first_queries[i], possibilities[i], pre_answers[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d7d2c06-f504-43e6-91af-240aa84ed62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['First for Women', 'Henri Leconte', 'The Wolfhounds', 'yes', 'no']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf23ea2-0e44-426a-bb2a-d97fbafcbe14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
