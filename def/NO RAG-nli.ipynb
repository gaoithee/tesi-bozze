{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a268f4e1-3a6d-464d-885b-2053e3143a2f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Dataset, documents, FAISS; retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96db4863-a54b-4d3a-87e2-74c645fcc018",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üîπ Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "094ab87a-0ba8-4878-991e-01515de302e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 375k/375k [00:00<00:00, 868kB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97aed7cf90814fe2befee2767fb8d3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'options', 'first nli', 'second nli', 'type', 'level', 'passages', 'BART1', 'BART2', 'ROBERTA1', 'ROBERTA2'],\n",
       "    num_rows: 352\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('saracandu/hotpotQA_nli', split=\"train\", trust_remote_code=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cfe5b6-d104-463c-a99e-62098b54d6a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üîπ Select a subset of the true dataset as a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3232ca1-3306-450e-9654-a9a47738f61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Who was inducted into the Rock and Roll Hall of Fame, David Lee Roth or Cia Berg?',\n",
       " 'answer': 'David Lee Roth',\n",
       " 'options': \"['Cia Berg', 'David Lee Roth']\",\n",
       " 'first nli': 'David Lee Roth was inducted into the Rock and Roll Hall of Fame before Cia Berg',\n",
       " 'second nli': 'Cia Berg was inducted into the Rock and Roll Hall of Fame before David Lee Roth',\n",
       " 'type': 'comparison',\n",
       " 'level': 'medium',\n",
       " 'passages': 'Cia Berg (born 2 December 1963), now known as Cia Soro, is a Swedish television presenter and singer. She was at one time the lead singer of the Swedish rock band Whale, who released the single \"Hobo Humpin\\' Slobo Babe\". David Lee Roth (born October 10, 1954) is an American rock vocalist, musician, songwriter, actor, author, and former radio personality. In 2007, he was inducted into the Rock and Roll Hall of Fame.',\n",
       " 'BART1': 0.885607123374939,\n",
       " 'BART2': 0.0146357780322432,\n",
       " 'ROBERTA1': 0.3287698328495025,\n",
       " 'ROBERTA2': 0.0123543506488204}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffles the order of the vector containing the correct answer and the two distractors\n",
    "# returns another vector, shuffled\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Definisci una funzione di pulizia per rimuovere caratteri non validi\n",
    "def clean_text(text):\n",
    "    return re.sub(r\"[^\\w\\s.,!?\\-:;()]+\", '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ddc1cc-c797-4975-b62d-ec80ffdb4f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a subset of the queries, just for test:\n",
    "first_queries = dataset['question']\n",
    "\n",
    "# same for correct answers and distractors:\n",
    "correct_answers = dataset['answer']\n",
    "possibilities = dataset['options']\n",
    "\n",
    "# and for the sources:\n",
    "sources = dataset['passages']\n",
    "\n",
    "#nli\n",
    "first_nli = dataset['first nli']\n",
    "second_nli = dataset['second nli']\n",
    "\n",
    "bart1 = dataset['BART1']\n",
    "bart2 = dataset['BART2']\n",
    "\n",
    "rob1 = dataset['ROBERTA1']\n",
    "rob2 = dataset['ROBERTA2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0fbefa-6118-430b-bf2e-36a4ef7c1022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "926bb532-fa37-4cfe-89fa-731ad140276d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Model loading and dataset selection (for testing purposes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86132ec0-f0d0-4316-b71e-92af9861cda2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ‚ñ™Ô∏è Upload the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce89a37-b6e1-48ff-b8fb-621e456392c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not run this unless necessary!\n",
    "\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc6eb0bf-fb11-4952-b14e-809b77898a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6393260ac9fa445bbdbe829bf7951822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "#################################################################\n",
    "# Tokenizer\n",
    "#################################################################\n",
    "\n",
    "model_name=\"nvidia/Llama3-ChatQA-1.5-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "#################################################################\n",
    "# bitsandbytes parameters\n",
    "#################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "#################################################################\n",
    "# Set up quantization config\n",
    "#################################################################\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "#################################################################\n",
    "# Load pre-trained config\n",
    "#################################################################\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e7d0aa-2978-4478-86de-34455b89ac89",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ‚ñ™Ô∏è Model through `guidance`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d17128d-7b94-4e6a-9ba2-95a8dc2d4107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>Do you want a joke or a poem? A <span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>poem</span></pre>"
      ],
      "text/plain": [
       "<guidance.models.transformers._transformers.Transformers at 0x7f4e1b927610>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from guidance import models, select\n",
    "\n",
    "new_model = models.Transformers(model, tokenizer, temperature=0.0)\n",
    "\n",
    "new_model + f'Do you want a joke or a poem? A ' + select(['joke', 'poem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b312ecad-c2c4-4791-bc82-bfaae5f5fa04",
   "metadata": {},
   "source": [
    "# Thesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f722aa12-2d2a-4868-83ef-96f01617d4d7",
   "metadata": {},
   "source": [
    "## üîπ PromptTemplate definition and a LLMChain for the **thesis** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34445f5e-870a-4cbc-a1f6-1b004027426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template definition\n",
    "# requires question, options (a string containing the possible options) and the context as input variables!\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "    System: This is a chat between a user and an AI assistant. The assistant gives helpful, detailed, and polite answers to the user‚Äôs questions based on the context. \n",
    "    {context}\n",
    "    User: {question}\n",
    "    Possible options: {options}.\n",
    "    Assistant:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcc84e0-fa8c-404d-85b4-96be5448a81d",
   "metadata": {},
   "source": [
    "## üîπ Function that generates the output given the prompt, the question and the set of options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07b5e395-05a8-4ba7-a960-07e6b496aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM chain definition\n",
    "from operator import itemgetter\n",
    "\n",
    "augmentation = {\"question\": itemgetter(\"question\"),\n",
    "                \"options\": itemgetter(\"options\"),\n",
    "                \"context\": itemgetter(\"context\"), }\n",
    "\n",
    "thesis_chain = augmentation | prompt_template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18ca079d-d85c-491f-87ab-2a0dfd626894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def thesisGeneration(query, merged, sources):\n",
    "    merged = ast.literal_eval(merged)\n",
    "    augmented_prompt = thesis_chain.invoke({'question': query, 'options': merged, 'context': sources})\n",
    "    normal_string = clean_text(augmented_prompt.text)\n",
    "    ans = new_model + normal_string + select([clean_text(merged[0]), clean_text(merged[1])])\n",
    "    return str(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc34eb1d-d402-4373-b8c5-e21cd51902b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(text):\n",
    "    # Trova l'indice in cui inizia il testo \"Why or why not the answer is correct:\"\n",
    "    start_index = text.find(\"Assistant:\\n\")\n",
    "\n",
    "    \n",
    "    # Se l'indice √® stato trovato, estrai la risposta corretta\n",
    "    if start_index != -1:\n",
    "        start_index += len(\"Assistant:\\n\")\n",
    "        # Estrai il testo dopo \"Why or why not the answer is correct:\"\n",
    "        correct_answer_text = text[start_index:].strip()\n",
    "        return correct_answer_text\n",
    "    else:\n",
    "        return \"The correct answer could not be found.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecf177e-3d1d-4e08-96d3-c9bf9144c986",
   "metadata": {},
   "source": [
    "## üîπ Test: how well the thesis alone is able to perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4cf535d-2517-452e-8ef3-821dc8524cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>\n",
       "    System: This is a chat between a user and an AI assistant. The assistant gives helpful, detailed, and polite answers to the users questions based on the context. \n",
       "    Dragonslayer is a 1981 American fantasy film directed by Matthew Robbins, from a screenplay he co-wrote with Hal Barwood. It stars Peter MacNicol, Ralph Richardson, John Hallam and Caitlin Clarke. The story, set in a fictional medieval kingdom, follows a young wizard who experiences danger and opposition as he attempts to defeat a dragon. Swiss Family Robinson is a 1960 American Adventure film starring John Mills, Dorothy McGuire, James MacArthur, Janet Munro, Tommy Kirk and Kevin Corcoran in a tale of a shipwrecked family building an island home, loosely based on the 1812 novel Der Schweizerische Robinson (literally, The Swiss Robinson) by Johann David Wyss. The film was directed by Ken Annakin and shot in Tobago and Pinewood Studios outside London. It was the second feature film version of the story (the first film version was released by RKO in 1940) and was a commercial success.\n",
       "    User: Which American film, Dragonslayer or Swiss Family Robinson, was shot outside London?\n",
       "    Possible options: Dragonslayer, Swiss Family Robinson.\n",
       "    Assistant:\n",
       "<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>Sw</span>iss Family Robinson</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answers = []\n",
    "for i in range(len(first_queries)):\n",
    "    answers.append(extract_answer(thesisGeneration(first_queries[i], possibilities[i], sources[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4815bcc6-48ba-40de-b2a4-a9510da209fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Arthurs Magazine', 'Jonathan Stark', 'The Wolfhounds', 'yes', 'yes']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f28ff4-de3a-4c41-b8d9-0ff10b7a13f9",
   "metadata": {},
   "source": [
    "# Antithesis - level of entailment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa387dd8-5302-4782-96e9-1c74c7aa0685",
   "metadata": {},
   "source": [
    "## Listening to the NLI verifier only: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f63e65af-7c4e-42b9-a661-026a7068ec65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bart_answers = []\n",
    "\n",
    "for i in range(len(first_queries)):\n",
    "\n",
    "    if bart1[i] > bart2[i]:\n",
    "        bart_answers.append([first_nli[i], bart1[i]])\n",
    "    else:\n",
    "        bart_answers.append([second_nli[i], bart2[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bf7520f-2729-4c55-8344-514634ebaaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rob_answers = []\n",
    "\n",
    "for i in range(len(first_queries)):\n",
    "\n",
    "    if rob1[i] > rob2[i]:\n",
    "        rob_answers.append([first_nli[i], rob1[i]])\n",
    "    else:\n",
    "        rob_answers.append([second_nli[i], rob2[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a643ed8f-0d35-44b4-84cd-293816c40ed4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üî∏ Function to generate the antithesis given the question, the thesis, the context and the options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ffee32-0ea6-439d-9ec5-1440dd58b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def antithesisGeneration(query, prompt_template, merged, candidate_answer, sources):\n",
    "    merged = ast.literal_eval(merged)\n",
    "    second_answer = query_model(system_message.format(context = sources),\n",
    "    user_message.format(question=query, options = merged, candidate_answer = candidate_answer, context = sources,), max_length=1024)\n",
    "    return second_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec71c35b-0598-429f-a99d-df9e82da06b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ant_answers = []\n",
    "for i in range(len(first_queries)):\n",
    "    ant_answers.append(antithesisGeneration(first_queries[i], prompt_template, possibilities[i], answers[i], sources[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93695a06-214d-4bc0-9f41-219192027cac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ant_answers[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22c886d-a2d7-430b-8841-baa40155dc24",
   "metadata": {},
   "source": [
    "# Pre Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "420ce7fd-13b8-4bc3-bf65-6cfad78ab7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt per baseline-2\n",
    "system_message = \"\"\"\n",
    "    You are an helpful AI assistant.\n",
    "    You are asked to determine the most correct answer for a given question.\n",
    "    You have at disposal a first tentative answer (a candidate answer) and another opinion on which should be the correct option according to context (a suggestion).\n",
    "    \n",
    "    They could agree on the correct option; in this case, directly output the option on which they agree.\n",
    "    If instead they disagree, use the context to determine the correct answer for the question, given the set of possible options.\n",
    "    \n",
    "    The goal of the assistant is to decree which is the most correct answer to the question between the available options. \n",
    "    Answer by explicitly reporting the correct answer to you.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "user_message = \"\"\"\n",
    "    Question: {question}\n",
    "    Options: {options}\n",
    "    Candidate answer: {candidate_answer}\n",
    "    Suggestion: {critique}\n",
    "    Which of the candidate answers {options} is the most proper answer for the question?\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3e0dc5e3-2a4a-4d09-81be-31670f1030fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt per baseline-3\n",
    "system_message = \"\"\"\n",
    "    You are an helpful AI assistant.\n",
    "    You are asked to determine the most correct answer for a given question.\n",
    "    You have at disposal a first tentative answer (a candidate answer) and another opinion on which should be the correct option (a suggestion).\n",
    "    \n",
    "    They could agree on the correct option; in this case, directly output the option on which they agree.\n",
    "    If instead they disagree, use the context to determine the correct answer for the question, given the set of possible options.\n",
    "    \n",
    "    Your goal is to decree which is the most correct answer to the question between the available options. \n",
    "    Answer by explicitly reporting the correct answer to the question.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "user_message = \"\"\"\n",
    "    Question: {question}\n",
    "    Options: {options}\n",
    "    Candidate answer: {candidate_answer}\n",
    "    Suggestion: {critique}\n",
    "    Which of the candidate answers {options} is the most proper answer for the question {question}?\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7891dbd-7cac-4edd-a879-4bbb4b42f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    repetition_penalty=1.5,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=400,\n",
    "    top_p=0.0\n",
    ")\n",
    "\n",
    "def query_model(\n",
    "        system_message,\n",
    "        user_message,\n",
    "        temperature = 0.0,\n",
    "        max_length=400\n",
    "        ):\n",
    "\n",
    "    user_message = \"Question: \" + user_message + \" Correct answer:\"\n",
    "    messages = [\n",
    "        {\"role\": \"System\", \"content\": system_message},\n",
    "        {\"role\": \"User\", \"content\": user_message},\n",
    "        ]\n",
    "    prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "        )\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    sequences = pipeline(\n",
    "        prompt,\n",
    "        top_p=0.0,\n",
    "        temperature=temperature,\n",
    "        #num_return_sequences=1,\n",
    "        eos_token_id=terminators,\n",
    "        max_new_tokens=max_length,\n",
    "        return_full_text=False,\n",
    "        pad_token_id=pipeline.model.config.eos_token_id\n",
    "    )\n",
    "\n",
    "    answer = sequences[0]['generated_text']\n",
    "    return answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad263e0f-acf1-4974-a7e0-3ce08a6f131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preSynthGeneration(query, merged, candidate_answer, critique, sources):\n",
    "    merged = ast.literal_eval(merged)\n",
    "    second_answer = query_model(system_message.format(context = sources),\n",
    "    user_message.format(question=query, options = merged, candidate_answer = candidate_answer, critique=critique, context = sources,), max_length=1024)\n",
    "    return second_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "799c7103-f3fb-44c3-941f-e2dae194fcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/pipelines/base.py:1167: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pre_answers = []\n",
    "for i in range(len(first_queries)):\n",
    "    pre_answers.append(preSynthGeneration(first_queries[i], possibilities[i], answers[i], bart_answers[i], sources[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "07aae555-c57d-4307-96b1-e42415ac6a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' \"Arthurs\\' Magazines\"',\n",
       " 'The suggested response indicates that there was some disagreement about who had actually achieved greater success as measured against their peers within professional sports circles - but ultimately it appears clear from both sources presented here today, namely Wikipedia itself alongside other online resources such.',\n",
       " '\"The wolf hound\"',\n",
       " \"'No'\\n\\nExplanation:\\nPavel was more into mathematics while Levinn worked mainly with computer science so it can't really say that both did similar kind if works\",\n",
       " 'The new pornographer',\n",
       " \"'New York City'\\n\\nThe two sources provide conflicting information about whether or not these addresses belong within San Franicsoo territory limits versus those belonging inside NYC boundaries\",\n",
       " 'The suggested response \"Ford\" indicates that it has been verified as true information about who came into existence earlier than whom',\n",
       " 'The suggested response \"Jane\" refers specifically',\n",
       " 'The two men were indeed famous film-makers who had much more than just their chosen career path as filmmakers/actors/directors etc..',\n",
       " '\"The Saimma gesture\"']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_answers[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f600cfb8-6a35-4baa-8236-c02c2909868c",
   "metadata": {},
   "source": [
    "# Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1edb95-c057-4d4b-8a40-c24f578e6c6b",
   "metadata": {},
   "source": [
    "## üî∫ PromptTemplate definition and a LLMChain for the **synthesis** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d91fc7b5-1837-4c1d-b8ef-a41b366c933a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"You are a multiple-choice question answering assistant.\n",
    "Choose the most proper option between {options} that best matches with the suggestion. \n",
    "\n",
    "Question: {question}\n",
    "Context: {critique}\n",
    "Sources: {context}\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "augmentation = {\"question\": itemgetter(\"question\"),\n",
    "                \"options\": itemgetter(\"options\"), \n",
    "                \"critique\": itemgetter(\"critique\"),\n",
    "                \"context\": itemgetter(\"context\"), }\n",
    "\n",
    "synthesis_chain = augmentation | prompt_template "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8170bc0d-1182-4c31-b33d-65fe64e1c385",
   "metadata": {},
   "source": [
    "## üî∫ Function to generate the synthesis given literally everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "38bef727-d240-4511-a556-0d2c19db4fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesisGeneration(query, prompt_template, merged, pre_answer, sources):\n",
    "    merged = ast.literal_eval(merged)\n",
    "    augmented_prompt = synthesis_chain.invoke({'question': query, \n",
    "                                            'options': merged,\n",
    "                                            'critique': pre_answer,\n",
    "                                            'context': sources})\n",
    "\n",
    "    normal_string = clean_text(augmented_prompt.text)\n",
    "    ans = new_model + normal_string + select([clean_text(merged[0]), clean_text(merged[1])])\n",
    "    return str(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "538c897d-183c-40eb-a9e2-c05e53addd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def_answers = [\"the correct option is \" + clean_text(correct_answer)\n",
    "               + \" since the other options is not mentioned in the context\" for correct_answer in correct_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9185be49-f36a-45e7-91a6-d0988fddd1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(text):\n",
    "    # Trova l'indice in cui inizia il testo \"Why or why not the answer is correct:\"\n",
    "    start_index = text.find(\"assistant:\\n\")\n",
    "\n",
    "    \n",
    "    # Se l'indice √® stato trovato, estrai la risposta corretta\n",
    "    if start_index != -1:\n",
    "        start_index += len(\"assistant:\\n\")\n",
    "        # Estrai il testo dopo \"Why or why not the answer is correct:\"\n",
    "        correct_answer_text = text[start_index:].strip()\n",
    "        return correct_answer_text\n",
    "    else:\n",
    "        return \"The correct answer could not be found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5856d161-623d-4258-bd8c-fbe224f1b286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>you are a multiplechoice question answering assistant.\n",
       "choose the most proper option between dragonslayer, swiss family robinson that best matches with the suggestion. \n",
       "\n",
       "question: which american film, dragonslayer or swiss family robinson, was shot outside london?\n",
       "context: swiss family robinson was shot outside london, 0.9854393005371094\n",
       "sources: dragonslayer is a 1981 american fantasy film directed by matthew robbins, from a screenplay he cowrote with hal barwood. it stars peter macnicol, ralph richardson, john hallam and caitlin clarke. the story, set in a fictional medieval kingdom, follows a young wizard who experiences danger and opposition as he attempts to defeat a dragon. swiss family robinson is a 1960 american adventure film starring john mills, dorothy mcguire, james macarthur, janet munro, tommy kirk and kevin corcoran in a tale of a shipwrecked family building an island home, loosely based on the 1812 novel der schweizerische robinson (literally, the swiss robinson) by johann david wyss. the film was directed by ken annakin and shot in tobago and pinewood studios outside london. it was the second feature film version of the story (the first film version was released by rko in 1940) and was a commercial success.\n",
       "\n",
       "assistant:\n",
       "<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>sw</span>iss family robinson</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "syn_answers = []\n",
    "for i in range(len(first_queries)):\n",
    "    syn_answers.append(extract_answer(\n",
    "        synthesisGeneration(\n",
    "            first_queries[i], prompt_template, possibilities[i], \n",
    "            bart_answers[i], sources[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "04c195ce-2955-4454-af82-322872620217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arthurs magazine',\n",
       " 'jonathan stark',\n",
       " 'the wolfhounds',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'new york city',\n",
       " 'aleksander ford',\n",
       " 'yes',\n",
       " 'director',\n",
       " 'the saimaa gesture']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_answers[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b81cea-bebb-4fc9-9787-5b2353af1113",
   "metadata": {},
   "source": [
    "# Dataset conversion and performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cec0231d-9fec-429b-8d6d-b265dbae7af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {\n",
    "    'query': first_queries,\n",
    "    'correct': correct_answers,\n",
    "    'thesis': answers,\n",
    "    'antithesis': bart_answers,\n",
    "#     'pre-synthesis': pre_answers,\n",
    "    'synthesis': syn_answers,\n",
    "    'context': sources\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e63baefa-86bc-41b3-a38a-807bc973b8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>correct</th>\n",
       "      <th>thesis</th>\n",
       "      <th>antithesis</th>\n",
       "      <th>synthesis</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which magazine was started first, Arthur's Mag...</td>\n",
       "      <td>Arthur's Magazine</td>\n",
       "      <td>Arthurs Magazine</td>\n",
       "      <td>[Arthur's Magazine was started before First fo...</td>\n",
       "      <td>arthurs magazine</td>\n",
       "      <td>Arthur's Magazine (1844‚Äì1846) was an American ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which tennis player won more Grand Slam titles...</td>\n",
       "      <td>Jonathan Stark</td>\n",
       "      <td>Jonathan Stark</td>\n",
       "      <td>[Jonathan Stark won more Grand Slam titles tha...</td>\n",
       "      <td>jonathan stark</td>\n",
       "      <td>Henri Leconte (born 4 July 1963) is a former F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which band was founded first, Hole (the rock b...</td>\n",
       "      <td>The Wolfhounds</td>\n",
       "      <td>The Wolfhounds</td>\n",
       "      <td>[Hole (the rock band that Courtney Love was a ...</td>\n",
       "      <td>the wolfhounds</td>\n",
       "      <td>The Wolfhounds are an indie pop/noise pop band...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Were Pavel Urysohn and Leonid Levin known for ...</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>[Yes, Pavel Urysohn and Leonid Levin were know...</td>\n",
       "      <td>yes</td>\n",
       "      <td>Leonid Anatolievich Levin ( ; Russian: –õ–µ–æ–Ω–∏ÃÅ–¥...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Are both The New Pornographers and Kings of Le...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>[Yes, both The New Pornographers and Kings of ...</td>\n",
       "      <td>yes</td>\n",
       "      <td>Kings of Leon is an American rock band that fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query            correct  \\\n",
       "0  Which magazine was started first, Arthur's Mag...  Arthur's Magazine   \n",
       "1  Which tennis player won more Grand Slam titles...     Jonathan Stark   \n",
       "2  Which band was founded first, Hole (the rock b...     The Wolfhounds   \n",
       "3  Were Pavel Urysohn and Leonid Levin known for ...                 no   \n",
       "4  Are both The New Pornographers and Kings of Le...                yes   \n",
       "\n",
       "             thesis                                         antithesis  \\\n",
       "0  Arthurs Magazine  [Arthur's Magazine was started before First fo...   \n",
       "1    Jonathan Stark  [Jonathan Stark won more Grand Slam titles tha...   \n",
       "2    The Wolfhounds  [Hole (the rock band that Courtney Love was a ...   \n",
       "3               yes  [Yes, Pavel Urysohn and Leonid Levin were know...   \n",
       "4               yes  [Yes, both The New Pornographers and Kings of ...   \n",
       "\n",
       "          synthesis                                            context  \n",
       "0  arthurs magazine  Arthur's Magazine (1844‚Äì1846) was an American ...  \n",
       "1    jonathan stark  Henri Leconte (born 4 July 1963) is a former F...  \n",
       "2    the wolfhounds  The Wolfhounds are an indie pop/noise pop band...  \n",
       "3               yes  Leonid Anatolievich Levin ( ; Russian: –õ–µ–æ–Ω–∏ÃÅ–¥...  \n",
       "4               yes  Kings of Leon is an American rock band that fo...  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "61349820-ad5d-4dff-a1d0-a08547f727d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('baseline-true.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d280eeea-c69b-42d2-856b-1651fc77b083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>correct</th>\n",
       "      <th>thesis</th>\n",
       "      <th>antithesis</th>\n",
       "      <th>synthesis</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which magazine was started first, Arthur's Mag...</td>\n",
       "      <td>arthurs magazine</td>\n",
       "      <td>arthurs magazine</td>\n",
       "      <td>[Arthur's Magazine was started before First fo...</td>\n",
       "      <td>arthurs magazine</td>\n",
       "      <td>Arthur's Magazine (1844‚Äì1846) was an American ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which tennis player won more Grand Slam titles...</td>\n",
       "      <td>jonathan stark</td>\n",
       "      <td>jonathan stark</td>\n",
       "      <td>[Jonathan Stark won more Grand Slam titles tha...</td>\n",
       "      <td>jonathan stark</td>\n",
       "      <td>Henri Leconte (born 4 July 1963) is a former F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which band was founded first, Hole (the rock b...</td>\n",
       "      <td>the wolfhounds</td>\n",
       "      <td>the wolfhounds</td>\n",
       "      <td>[Hole (the rock band that Courtney Love was a ...</td>\n",
       "      <td>the wolfhounds</td>\n",
       "      <td>The Wolfhounds are an indie pop/noise pop band...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Were Pavel Urysohn and Leonid Levin known for ...</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>[Yes, Pavel Urysohn and Leonid Levin were know...</td>\n",
       "      <td>yes</td>\n",
       "      <td>Leonid Anatolievich Levin ( ; Russian: –õ–µ–æ–Ω–∏ÃÅ–¥...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Are both The New Pornographers and Kings of Le...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>[Yes, both The New Pornographers and Kings of ...</td>\n",
       "      <td>yes</td>\n",
       "      <td>Kings of Leon is an American rock band that fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query           correct  \\\n",
       "0  Which magazine was started first, Arthur's Mag...  arthurs magazine   \n",
       "1  Which tennis player won more Grand Slam titles...    jonathan stark   \n",
       "2  Which band was founded first, Hole (the rock b...    the wolfhounds   \n",
       "3  Were Pavel Urysohn and Leonid Levin known for ...                no   \n",
       "4  Are both The New Pornographers and Kings of Le...               yes   \n",
       "\n",
       "             thesis                                         antithesis  \\\n",
       "0  arthurs magazine  [Arthur's Magazine was started before First fo...   \n",
       "1    jonathan stark  [Jonathan Stark won more Grand Slam titles tha...   \n",
       "2    the wolfhounds  [Hole (the rock band that Courtney Love was a ...   \n",
       "3               yes  [Yes, Pavel Urysohn and Leonid Levin were know...   \n",
       "4               yes  [Yes, both The New Pornographers and Kings of ...   \n",
       "\n",
       "          synthesis                                            context  \n",
       "0  arthurs magazine  Arthur's Magazine (1844‚Äì1846) was an American ...  \n",
       "1    jonathan stark  Henri Leconte (born 4 July 1963) is a former F...  \n",
       "2    the wolfhounds  The Wolfhounds are an indie pop/noise pop band...  \n",
       "3               yes  Leonid Anatolievich Levin ( ; Russian: –õ–µ–æ–Ω–∏ÃÅ–¥...  \n",
       "4               yes  Kings of Leon is an American rock band that fo...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Funzione per rimuovere le quadre e ottenere solo il contenuto\n",
    "def remove_brackets(s):\n",
    "    return s.strip(\"[] \")\n",
    "\n",
    "# Definisci una funzione di pulizia per rimuovere caratteri non validi\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s.,!?\\'\"\\-:;()]+', '', text)  # Rimuove i caratteri speciali\n",
    "    text = re.sub(r\"['\\\"-]\", '', text)  # Rimuove apostrofi, virgolette e trattini\n",
    "    text = text.lower()  # Converte in minuscolo\n",
    "    return text\n",
    "\n",
    "# Applica la funzione alla colonna 'correct answer'\n",
    "df['correct'] = df['correct'].apply(clean_text)\n",
    "df['thesis'] = df['thesis'].apply(clean_text)\n",
    "df['synthesis'] = df['synthesis'].apply(clean_text)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "38e03a0e-1c19-41e0-9ae3-d8348bcdc3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 260\n",
      "Number of non-matches: 92\n"
     ]
    }
   ],
   "source": [
    "# Conta quante righe combaciano e quante no\n",
    "matches = 0\n",
    "non_matches = 0\n",
    "which_ones = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    correct_answer = str(row['correct']).strip()\n",
    "    thesis = str(row['thesis']).strip()\n",
    "    \n",
    "    if correct_answer == thesis:\n",
    "        matches += 1\n",
    "    else:\n",
    "        non_matches += 1\n",
    "        which_ones.append(\"thesis: {}, Correct: {}\".format(thesis, correct_answer))\n",
    "\n",
    "print(f\"Number of matches: {matches}\")\n",
    "print(f\"Number of non-matches: {non_matches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2e0614ea-7259-4f87-943c-eaa830e211b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 241\n",
      "Number of non-matches: 111\n"
     ]
    }
   ],
   "source": [
    "# Conta quante righe combaciano e quante no\n",
    "matches = 0\n",
    "non_matches = 0\n",
    "which_ones_syn = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    correct_answer = str(row['correct']).strip()\n",
    "    thesis = str(row['synthesis']).strip()\n",
    "    \n",
    "    if correct_answer == thesis:\n",
    "        matches += 1\n",
    "    else:\n",
    "        # print(\"Synthesis: {}, Correct: {}\".format(thesis, correct_answer))\n",
    "        non_matches += 1\n",
    "        which_ones_syn.append(\"Synthesis: {}, Correct: {}\".format(thesis, correct_answer))\n",
    "\n",
    "print(f\"Number of matches: {matches}\")\n",
    "print(f\"Number of non-matches: {non_matches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d97283d4-811a-417a-947a-18ea10059e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('bart-nli-naive-1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519a4d8c-bcef-4c93-b462-9c9e87044435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
