{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3ed6246-e9aa-4fbc-a775-dc25888cdaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    ")\n",
    "from guidance import models, select\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "583be627-ea69-4520-ada3-67713ac6ef04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisci una funzione di pulizia per rimuovere caratteri non validi\n",
    "def clean_text(text):\n",
    "    return re.sub(r\"[^\\w\\s.,!?\\-:;()]+\", '', text)\n",
    "\n",
    "# Definisci una funzione di pulizia per rimuovere caratteri non validi\n",
    "def clean_text_final(text):\n",
    "    text = re.sub(r'[^\\w\\s.,!?\\'\"\\-:;()]+', '', text)  # Rimuove i caratteri speciali\n",
    "    text = re.sub(r\"['\\\"-]\", '', text)  # Rimuove apostrofi, virgolette e trattini\n",
    "    text = text.lower()  # Converte in minuscolo\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd823455-1bb7-47e3-8c28-8632f236ff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt augmentation for the (format of the) synthesis:\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"You are a multiple-choice question answering assistant.\n",
    "Choose the most proper option between {options} that best matches with the suggestion. \n",
    "\n",
    "Question: {question}\n",
    "Context: {critique}\n",
    "Sources: {context}\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    ")\n",
    "augmentation = {\"question\": itemgetter(\"question\"),\n",
    "                \"options\": itemgetter(\"options\"), \n",
    "                \"critique\": itemgetter(\"critique\"),\n",
    "                \"context\": itemgetter(\"context\"), }\n",
    "synthesis_chain = augmentation | prompt_template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c96c0705-1b51-45d0-8ba1-2a4cae245196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_message_thesis(question, options, context):\n",
    "    options_str = '\", \"'.join(options)\n",
    "    content = f\"\"\"\n",
    "\n",
    "    Now do the same for this question: \"{question}\", where options: [\"{options_str}\"]. Assistant:\n",
    "    \"\"\"\n",
    "\n",
    "    user_content = \"Answer to the following question: \" + question + \" providing one of these options as answer: \" + str(options) + \"Assistant:\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"\n",
    "        You are an helpful AI assistant. You have to provide helpful answers to the user’s questions based on the context: \n",
    "        \"\"\" + context},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "\n",
    "    return messages\n",
    "\n",
    "def extract_answer_thesis(text):\n",
    "    # Trova l'indice in cui inizia il testo \"Why or why not the answer is correct:\"\n",
    "    start_index = text.find(\"}]\")\n",
    "\n",
    "    \n",
    "    # Se l'indice è stato trovato, estrai la risposta corretta\n",
    "    if start_index != -1:\n",
    "        start_index += len(\"}]\")\n",
    "        # Estrai il testo dopo \"Why or why not the answer is correct:\"\n",
    "        correct_answer_text = text[start_index:].strip()\n",
    "        return correct_answer_text\n",
    "    else:\n",
    "        return \"The correct answer could not be found.\"\n",
    "\n",
    "def thesisGeneration(query, merged, sources):\n",
    "    merged = ast.literal_eval(merged)\n",
    "    augmented_prompt = create_message_thesis(query, merged, sources)\n",
    "    ans = new_model + str(augmented_prompt) + select(merged)\n",
    "    return str(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c0adcaf-24b7-46ba-9eed-021f0c4215c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_message_antithesis(question, candidate, options, context):\n",
    "    options_str = '\", \"'.join(options)\n",
    "    content = f\"\"\"\n",
    "\n",
    "    Now do the same for this question: \"{question}\", where options: [\"{options_str}\"]. Assistant:\n",
    "    \"\"\"\n",
    "\n",
    "    user_content = \"Question: \" + question + \"\\n Options: \" + str(options) + \"\\n Candidate answer: \" + candidate + \"\\n Context: \" + context + \"\\n Assistant: \\n\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"\n",
    "        You are an helpful AI assistant. You are asked to determine the most correct answer for a given question, provided a set of possible options.\n",
    "        You also have at disposal a first tentative answer that you are required to check with respect to the question and the relevant context.\n",
    "        Your goal is to decree which is the most correct answer to the question between the available options.\n",
    "\n",
    "        Here's an example of how to do it:\n",
    "        \"\"\"},\n",
    "        {\"role\": \"user\", \"content\": \"\"\"\n",
    "        Question: What event did Juan Rossell participate in?\n",
    "        Options: ['1996 summer olympics', 'olympic games', 'sport']\n",
    "        Candidate answer: 1996 summer olympics\n",
    "        Context: The 2004 Summer Olympic Games, held in Athens, Greece, marked the return of the games to their birthplace. With a motto of \"Welcome Home,\" the event featured 10,625 athletes from 201 countries competing in 301 medal events across 28 sports. This edition of the Games was significant as it was the first time all countries with a National Olympic Committee participated.\\n\\nIn the realm of beach volleyball, a sport introduced to the Olympics in 1996, Juan Miguel Rossell Milanes from Cuba emerged as a notable figure. He won the gold medal in the men\\'s beach team competition at the 2003 Pan American Games in Santo Domingo, Dominican Republic, partnering Francisco Alvarez. Rossell Milanes also represented Cuba at the 1996 and 2004 Summer Olympics.\\n\\nThe Pan American Games, a major sporting event in the Americas, also featured beach volleyball as a competitive discipline. The 14th Pan American Games took place in Santo Domingo, Dominican Republic, in 2003. The successful bid for the games was made in the mid-1990s, when the Dominican Republic experienced one of the highest growth rates in Latin America.\\n\\nThe International Olympic Committee (IOC) voted in 1986 to separate the Summer and Winter Games, which had been held in the same year since 1924, and place them in alternating even-numbered years, beginning in 1994. The 1996 Summer Games were the first to be staged in a different year from the Winter Games. Atlanta, Georgia, United States, hosted the 1996 Summer Games, becoming the fifth American city to host the Olympic Games and the third to hold a Summer Olympic Games.\n",
    "\n",
    "        Assistant: Let's consider each option and check whether or not is the proper one with respect to the context.'sport' is an extremely generic answer, inappropriate for the question asked. 'olympic games' is not the most proper answer, since it is true that Juan Rossell introduced beach volleyball as an olympic sport, but he participated only at Summer Olympics, both in 1996 and in 2003. Therefore, the correct option is '1996 summer olympics'.\n",
    "\n",
    "        \"\"\"\n",
    "        },\n",
    "        {\"role\": \"system\", \"content\": \"Now do the same for the following question:\"},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "\n",
    "    return messages\n",
    "\n",
    "def antithesisGeneration(query, merged, candidate, sources):\n",
    "    merged = ast.literal_eval(merged)\n",
    "    prompt = create_message_antithesis(query, candidate, merged, sources)\n",
    "    output = pipe(prompt, **generation_args)\n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "507e20fb-b0f7-47ab-92db-836c9a4d5e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_message_presynthesis(question, candidate, suggestion, options, context):\n",
    "\n",
    "    user_content = \"Question: \" + question + \"\\n Options: \" + str(options) + \"\\n Candidate answer: \" + candidate + \"\\n Suggestion: \" + suggestion + \"\\n Context: \" + context + \"\\n Assistant: \\n\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"\n",
    "        You are an helpful AI assistant. You are asked to determine the most correct answer for a given question, provided a set of possible options.\n",
    "        You also have at disposal a first tentative answer and a suggestion on which is the correct answer.\n",
    "        Your goal is to decree which is the most correct answer to the question between the available options according to the context.\n",
    "\n",
    "        Here's a few examples on how to do it:\n",
    "        \"\"\"},\n",
    "        {\"role\": \"user\", \"content\": \"\"\"\n",
    "        Question: What event did Juan Rossell participate in?\n",
    "        Options: ['1996 summer olympics', 'olympic games', 'sport']\n",
    "        Candidate answer: 1996 summer olympics\n",
    "        Suggestion: Let's consider each option and check whether or not is the proper one with respect to the context.'sport' is an extremely generic answer, inappropriate for the question asked. 'olympic games' is not the most proper answer, since it is true that Juan Rossell introduced beach volleyball as an olympic sport, but he participated only at Summer Olympics, both in 1996 and in 2003. Therefore, the correct option is '1996 summer olympics'.\n",
    "        Context: The 2004 Summer Olympic Games, held in Athens, Greece, marked the return of the games to their birthplace. With a motto of \"Welcome Home,\" the event featured 10,625 athletes from 201 countries competing in 301 medal events across 28 sports. This edition of the Games was significant as it was the first time all countries with a National Olympic Committee participated.\\n\\nIn the realm of beach volleyball, a sport introduced to the Olympics in 1996, Juan Miguel Rossell Milanes from Cuba emerged as a notable figure. He won the gold medal in the men\\'s beach team competition at the 2003 Pan American Games in Santo Domingo, Dominican Republic, partnering Francisco Alvarez. Rossell Milanes also represented Cuba at the 1996 and 2004 Summer Olympics.\\n\\nThe Pan American Games, a major sporting event in the Americas, also featured beach volleyball as a competitive discipline. The 14th Pan American Games took place in Santo Domingo, Dominican Republic, in 2003. The successful bid for the games was made in the mid-1990s, when the Dominican Republic experienced one of the highest growth rates in Latin America.\\n\\nThe International Olympic Committee (IOC) voted in 1986 to separate the Summer and Winter Games, which had been held in the same year since 1924, and place them in alternating even-numbered years, beginning in 1994. The 1996 Summer Games were the first to be staged in a different year from the Winter Games. Atlanta, Georgia, United States, hosted the 1996 Summer Games, becoming the fifth American city to host the Olympic Games and the third to hold a Summer Olympic Games.\n",
    "        \"\"\"\n",
    "        },\n",
    "        {\"role\": \"assistant\", \"content\": \"\"\"\n",
    "        Assistant: The correct option is '1996 summer olympics', both the candidate answer and the suggestion agree.\n",
    "        \"\"\"\n",
    "        },\n",
    "        \n",
    "\n",
    "        \n",
    "        {\"role\": \"system\", \"content\": \"Now do the same for the following question:\"},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "\n",
    "    return messages\n",
    "\n",
    "def preSynthGeneration(query, candidate_answer, critique, merged, sources):\n",
    "    prompt = create_message_presynthesis(query, merged, candidate_answer, critique, sources)\n",
    "    output = pipe(prompt, **generation_args)\n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abe2291b-5961-41dd-af73-1290d53da65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def synthesisGeneration(query, merged, pre_answer, sources):\n",
    "    merged = ast.literal_eval(merged)\n",
    "    augmented_prompt = synthesis_chain.invoke({'question': query, \n",
    "                                            'options': merged,\n",
    "                                            'critique': pre_answer,\n",
    "                                            'context': sources})\n",
    "\n",
    "    normal_string = clean_text(augmented_prompt.text)\n",
    "    ans = new_model + normal_string + select(merged)\n",
    "    return str(ans)\n",
    "\n",
    "def extract_answer_synthesis(text):\n",
    "    # Trova l'indice in cui inizia il testo \"Why or why not the answer is correct:\"\n",
    "    start_index = text.find(\"Assistant:\\n\")\n",
    "\n",
    "    \n",
    "    # Se l'indice è stato trovato, estrai la risposta corretta\n",
    "    if start_index != -1:\n",
    "        start_index += len(\"Assistant:\\n\")\n",
    "        # Estrai il testo dopo \"Why or why not the answer is correct:\"\n",
    "        correct_answer_text = text[start_index:].strip()\n",
    "        return correct_answer_text\n",
    "    else:\n",
    "        return \"The correct answer could not be found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c1a1953-0234-4513-82bf-d7b24f3403dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a16e955325f746d897ee7d496690de8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", use_fast=False)\n",
    "new_model = models.Transformers(model, tokenizer, temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e679fc5c-4c26-418d-b891-f495e9b1b0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"do_sample\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99d00b0b-9cab-455a-b075-4f5240044ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../../wikihop_dataset/wikihop-merged-summarized.csv')\n",
    "\n",
    "# select a subset of the queries, just for test:\n",
    "first_queries = df['query']\n",
    "\n",
    "# same for correct answers and distractors:\n",
    "correct_answers = df['answer']\n",
    "possibilities = df['options']\n",
    "\n",
    "# and for the sources:\n",
    "sources = df['sum_supports']\n",
    "\n",
    "N_rows = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19b88dd6-39e6-449f-a2da-979aede59652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['english', 'greek', 'koine greek', 'nahuatl', 'spanish']\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possibilities[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4c9fe93-1fe2-4a2a-b063-5626d6146fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Christianity is a monotheistic religion based on the life and teachings of Jesus Christ. It is the largest religion in the world, with over 2 billion followers. Christians believe in one God and follow the teachings of Jesus Christ, who they believe is the Son of God and the savior of humanity.\\n\\nLakewood Church is a nondenominational charismatic Christian megachurch located in Houston, Texas. It is the largest congregation in the United States, with an average attendance of about 52,000 people per week. The church is led by Joel Osteen, who is the son of John Hillery Osteen, the founder and first pastor of Lakewood Church.\\n\\nMexico is a federal republic in North America, bordered by the United States, Guatemala, Belize, the Caribbean Sea, and the Pacific Ocean. It is the most populous Spanish-speaking country in the world and the second most populous country in Latin America. Mexico is a federation comprising 31 states and a federal district that is also its capital and most populous city.\\n\\nJohn Hillery Osteen founded Lakewood Church in Houston, Texas, in 1959. He served as the first pastor until his death in 1999. His son, Joel Osteen, succeeded him as pastor and continues to lead the church today.\\n\\nHouston is the most populous city in Texas and the fourth-most populous city in the United States. It is located in Southeast Texas near the Gulf of Mexico and is the seat of Harris County. Houston is the principal city of the Houston metropolitan area, which is the fifth-most populated metropolitan area in the United States.\\n\\nIn summary, Christianity is a monotheistic religion based on the life and teachings of Jesus Christ, with over 2 billion followers worldwide. Lakewood Church in Houston, Texas, is the largest congregation in the United States, led by Joel Osteen, the son of John Hillery Osteen, the founder and first pastor of the church. Houston is the most populous city in Texas and the fourth-most populous city in the United States, and is the principal city of the Houston metropolitan area. <|end|>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18a1b5a9-c8b4-406b-8404-917e469f8d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What languages did John Osteen speak or write?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_queries[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93583acf-3392-4a3c-a93f-994361b43675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>[{&#x27;role&#x27;: &#x27;system&#x27;, &#x27;content&#x27;: &#x27;\\n        You are an helpful AI assistant. You have to provide helpful answers to the user’s questions based on the context: \\n         The Three Kingdoms period (220280) in China was marked by the competition for supremacy among the states of Wei, Shu, and Wu. Wu, also known as Eastern Wu, was founded by Sun Quan in 222 and declared independence from the Cao Wei state in 229. Sun Quan ruled as &quot;King of Wu&quot; until 229 and as &quot;Emperor of Wu&quot; until 252. The state\\&#x27;s capital was initially Jianye (present-day Nanjing, Jiangsu) but later moved to Wuchang (present-day Ezhou, Hubei). The state of Cao Wei was established by Cao Pi in 220, with its capital at Luoyang. Cao Wei\\&#x27;s authority weakened after the death of Cao Rui, leading to the rise of Sima Yi and his family. The state of Shu Han was founded by Liu Bei in 221, with its capital at Chengdu. The Yellow Turban Rebellion (184-205) marked a significant event in Taoism\\&#x27;s history due to its association with secret Taoist societies. Japan, located off the eastern coast of Asia, is an island nation in the Pacific Ocean. The Qin dynasty (221206 BC) was the first unified Chinese empire, founded by Qin Shi Huang after conquering the other states during the Warring States period. The Warring States period (481403 BC) was a time of conflict that ended with the Qin dynasty\\&#x27;s victory in 221 BC. Archimedes of Syracuse was a renowned mathematician and scientist. The concept of buoyancy, or Archimedes\\&#x27; principle, states that the upward force exerted on a submerged object is equal to the weight of the fluid displaced by the object. The state of Cao Wei was founded by Cao Cao, a warlord who rose to power during the late Eastern Han dynasty. Cao Cao\\&#x27;s son, Cao Pi, forced Emperor Xian to abdicate in his favor, establishing the Cao Wei state. Liu Bei declared himself emperor of Shu Han, while Sun&#x27;}, {&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &quot;Answer to the following question: What was the occupation of Cao Chong? providing one of these options as answer: [&#x27;academic&#x27;, &#x27;builder&#x27;, &#x27;chancellor&#x27;, &#x27;classics&#x27;, &#x27;confucian scholar&#x27;, &#x27;designer&#x27;, &#x27;duke&#x27;, &#x27;emperor&#x27;, &#x27;engineer&#x27;, &#x27;engineering&#x27;, &#x27;father&#x27;, &#x27;founder&#x27;, &#x27;general&#x27;, &#x27;king&#x27;, &#x27;leader&#x27;, &#x27;major&#x27;, &#x27;mathematician&#x27;, &#x27;military&#x27;, &#x27;official&#x27;, &#x27;peasant&#x27;, &#x27;physicist&#x27;, &#x27;physics&#x27;, &#x27;politician&#x27;, &#x27;prior&#x27;, &#x27;rebel&#x27;, &#x27;research&#x27;, &#x27;ruler&#x27;, &#x27;science&#x27;, &#x27;script&#x27;, &#x27;social reformer&#x27;, &#x27;socialist&#x27;, &#x27;sovereign&#x27;, &#x27;taiwan&#x27;]Assistant:&quot;}]<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>t</span>aiwan</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "# THESIS\n",
    "answers = []\n",
    "for i in range(N_rows):\n",
    "    answers.append(extract_answer_thesis(thesisGeneration(first_queries[i], possibilities[i], sources[i])))\n",
    "\n",
    "\n",
    "# ANTITHESIS\n",
    "ant_answers = []\n",
    "for i in range(N_rows):\n",
    "    ant_answers.append(antithesisGeneration(first_queries[i], possibilities[i], answers[i], sources[i]))\n",
    "\n",
    "# SYNTHESIS\n",
    "pre_answers = []\n",
    "for i in range(N_rows):\n",
    "    pre_answers.append(preSynthGeneration(first_queries[i], possibilities[i], answers[i], ant_answers[i], sources[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f43d422-3418-4354-9c30-139536a15c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" The correct option is '1996 summer olympics'. Both the candidate answer and the suggestion agree. <|end|>\",\n",
       " \" The correct options are 'english' and'spanish'. John Osteen, as an American pastor, likely speaks and writes 'english'. However, since he has connections to Mexico through his church, it is also possible that he speaks and writes'spanish'. Both 'english' and'spanish' are correct answers.  <|end|>\",\n",
       " \" The correct option is 'tetrapoda'. The candidate answer and the suggestion agree.\\n\\nTetrapoda is the most appropriate answer because it is the taxonomic group that includes reptiles. Proaigialosaurus is a genus of extinct reptiles, and therefore, it belongs to the group Tetrapoda. The other options are either specific orders or families within the reptile group, or they are not related to reptiles at all.\\n\\nHere's a breakdown of the options:\\n\\n- 'alligatoridae' and 'gavialidae' are families within the order Crocodilia, which is a subset of reptiles.\\n- 'amphibia' and 'lissamphibia' are orders that include frogs, toads, and salamanders, which are not reptiles.\\n- 'amphisbaenia' is an order that includes worm lizards, which are not reptiles.\\n- 'animal' is too broad and not specific enough.\\n- 'bipes' is a genus of sand lizards, which are reptiles.\\n- 'crocodile' is a genus within the order Crocodilia, which is a subset of reptiles.\\n- 'crocodilia' is an order that includes crocodiles, alligators, and gavials, which are reptiles.\\n- 'diapsid' is a group of reptiles that includes crocodiles, lizards, snakes, and birds.\\n- 'dibamidae' is a family of blind lizards, which are reptiles.\\n- 'dinosaur' is a group of extinct reptiles, which are not the parent taxon of Proaigialosaurus.\\n- 'gymnophiona' is an order of limbless, burrowing amphibians, which are not reptiles.\\n- 'homo' and 'human' are members of the family Hominidae, which are mammals, not reptiles.\\n- 'lepidosauria' is a group of reptiles that includes lizards, snakes, and tuatara.\\n- 'lizard' is a type of reptile, but not a\",\n",
       " ' The correct option is \\'crocodile\\', as Australosuchus is a genus of crocodile that belongs to the Mekosuchinae subfamily. \\n\\nSuggestion: [\\'animal\\', \\'area\\', \\'crocodile\\', \\'crocodilia\\', \\'homo\\',\\'mekosuchinae\\', \\'plant\\', \\'tomistoma\\']\\n\\nContext: Mekosuchinae was a subfamily of crocodiles that lived in Australia and the South Pacific. They first appeared in the Eocene epoch in Australia and survived until the Pleistocene in Australia and until the arrival of humans in the Pacific islands of Fiji, New Caledonia, and Vanuatu. There is some debate over whether Mekosuchinae should be classified as a subfamily within Crocodylidae or as a distinct family, Mekosuchidae, within the superfamily Crocodyloidea.\\n\\nAustralosuchus is an extinct genus of crocodylian that belonged to the Mekosuchinae subfamily. The only known species, Australosuchus clarkae, lived during the Late Oligocene and Early Miocene periods in southern Australia. The generic name Australosuchus means \"Southern crocodile.\" The species was described in 1991 based on fossil material found at Lake Palankarinna in South Australia. \\n\\n <|end|>',\n",
       " \" The correct option for Cao Chong's occupation is 'general'. The context provided discusses the Three Kingdoms period in China and mentions Cao Chong as a general who played a crucial role in the Battle of Red Cliffs. The other options, such as 'academic', 'builder', 'chancellor', 'classics', 'confucian scholar', 'designer', 'duke', 'emperor', 'engineer', 'engineering', 'father', 'founder', 'king', 'leader','major','mathematician','military', 'official', 'peasant', 'physicist', 'physics', 'politician', 'prior','rebel','research', 'ruler','science','script','social reformer','socialist','sovereign', 'taiwan', and 'taiwan' are not relevant to Cao Chong's occupation. <|end|>\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83845efe-d115-4836-9e67-c936368b143d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1996 summer olympics\n",
       "1                 english\n",
       "2            lepidosauria\n",
       "3              crocodilia\n",
       "4               physicist\n",
       "Name: answer, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_answers[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89c1f42-f7e4-4668-bfb2-68d4e66a6947",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
