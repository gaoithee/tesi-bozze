{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3ed6246-e9aa-4fbc-a775-dc25888cdaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    ")\n",
    "from guidance import models, select\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "583be627-ea69-4520-ada3-67713ac6ef04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisci una funzione di pulizia per rimuovere caratteri non validi\n",
    "def clean_text(text):\n",
    "    return re.sub(r\"[^\\w\\s.,!?\\-:;()]+\", '', text)\n",
    "\n",
    "# Definisci una funzione di pulizia per rimuovere caratteri non validi\n",
    "def clean_text_final(text):\n",
    "    text = re.sub(r'[^\\w\\s.,!?\\'\"\\-:;()]+', '', text)  # Rimuove i caratteri speciali\n",
    "    text = re.sub(r\"['\\\"-]\", '', text)  # Rimuove apostrofi, virgolette e trattini\n",
    "    text = text.lower()  # Converte in minuscolo\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd823455-1bb7-47e3-8c28-8632f236ff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt augmentation for the (format of the) synthesis:\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"You are a multiple-choice question answering assistant.\n",
    "Choose the most proper option between {options} that best matches with the suggestion. \n",
    "\n",
    "Question: {question}\n",
    "Context: {critique}\n",
    "Sources: {context}\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    ")\n",
    "augmentation = {\"question\": itemgetter(\"question\"),\n",
    "                \"options\": itemgetter(\"options\"), \n",
    "                \"critique\": itemgetter(\"critique\"),\n",
    "                \"context\": itemgetter(\"context\"), }\n",
    "synthesis_chain = augmentation | prompt_template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c96c0705-1b51-45d0-8ba1-2a4cae245196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_message_thesis(question, options, context):\n",
    "    options_str = '\", \"'.join(options)\n",
    "    content = f\"\"\"\n",
    "\n",
    "    Now do the same for this question: \"{question}\", where options: [\"{options_str}\"]. Assistant:\n",
    "    \"\"\"\n",
    "\n",
    "    user_content = \"Answer to the following question: \" + question + \" providing one of these options as answer: \" + str(options) + \"Assistant:\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"\n",
    "        You are an helpful AI assistant. You have to provide helpful answers to the user’s questions based on the context: \n",
    "        \"\"\" + context},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "\n",
    "    return messages\n",
    "\n",
    "def extract_answer_thesis(text):\n",
    "    # Trova l'indice in cui inizia il testo \"Why or why not the answer is correct:\"\n",
    "    start_index = text.find(\"}]\")\n",
    "\n",
    "    \n",
    "    # Se l'indice è stato trovato, estrai la risposta corretta\n",
    "    if start_index != -1:\n",
    "        start_index += len(\"}]\")\n",
    "        # Estrai il testo dopo \"Why or why not the answer is correct:\"\n",
    "        correct_answer_text = text[start_index:].strip()\n",
    "        return correct_answer_text\n",
    "    else:\n",
    "        return \"The correct answer could not be found.\"\n",
    "\n",
    "def thesisGeneration(query, merged, sources):\n",
    "    merged = ast.literal_eval(merged)\n",
    "    augmented_prompt = create_message_thesis(query, merged, sources)\n",
    "    ans = new_model + str(augmented_prompt) + select(merged)\n",
    "    return str(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b96b8290-3db2-4c56-868f-37653366e185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_message_antithesis_original(question, candidate, options, context):\n",
    "    options_str = '\", \"'.join(options)\n",
    "    content = f\"\"\"\n",
    "\n",
    "    Now do the same for this question: \"{question}\", where options: [\"{options_str}\"]. Assistant:\n",
    "    \"\"\"\n",
    "\n",
    "    user_content = \"Question: \" + question + \"\\n Options: \" + str(options) + \"\\n Candidate answer: \" + candidate + \"\\n Context: \" + context + \"\\n Assistant: \\n\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"\n",
    "        You are an helpful AI assistant. You are asked to determine the most correct answer for a given question, provided a set of possible options.\n",
    "        You also have at disposal a first tentative answer that you are required to check with respect to the question and the relevant context.\n",
    "        Your goal is to decree which is the most correct answer to the question between the available options.\n",
    "\n",
    "        Here's an example of how to do it:\n",
    "        \"\"\"},\n",
    "        {\"role\": \"user\", \"content\": \"\"\"\n",
    "        Question: What is the sun?\n",
    "        Options: ['a star', 'a planet']\n",
    "        Candidate answer: a planet\n",
    "        Context: The Sun is the star at the center of the Solar System. It is a massive, nearly perfect sphere of hot plasma, heated to incandescence by nuclear fusion reactions in its core, radiating the energy from its surface mainly as visible light and infrared radiation with 10% at ultraviolet energies.\n",
    "\n",
    "        Assistant: The correct answer should be 'a star' due to the fact that the context explicitly say so. On the opposite, the context never mentions the fact that the Sun could be a planet.\n",
    "\n",
    "        \"\"\"\n",
    "        },\n",
    "        {\"role\": \"system\", \"content\": \"Now do the same for the following question:\"},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "\n",
    "    return messages\n",
    "\n",
    "def antithesisGeneration(query, merged, candidate, sources):\n",
    "    merged = ast.literal_eval(merged)\n",
    "    prompt = create_message_antithesis_original(query, candidate, merged, sources)\n",
    "    output = pipe(prompt, **generation_args)\n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a094e702-2b0a-4f14-9c30-f40241f0d01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_message_antithesis_cot(question, candidate, options, context):\n",
    "    options_str = '\", \"'.join(options)\n",
    "    content = f\"\"\"\n",
    "\n",
    "    Now do the same for this question: \"{question}\", where options: [\"{options_str}\"]. Assistant:\n",
    "    \"\"\"\n",
    "\n",
    "    user_content = \"Question: \" + question + \"\\n Options: \" + str(options) + \"\\n Candidate answer: \" + candidate + \"\\n Context: \" + context + \"\\n Assistant: \\n\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"\n",
    "        You are an helpful AI assistant. You are asked to determine the most correct answer for a given question, provided a set of possible options.\n",
    "        You also have at disposal a first tentative answer that you are required to check with respect to the question and the relevant context.\n",
    "        Your goal is to decree which is the most correct answer to the question between the available options.\n",
    "\n",
    "        Here's an example of how to do it:\n",
    "        \"\"\"},\n",
    "        {\"role\": \"user\", \"content\": \"\"\"\n",
    "        Question: What is the sun?\n",
    "        Options: ['a star', 'a planet']\n",
    "        Candidate answer: a planet\n",
    "        Context: The Sun is the star at the center of the Solar System. It is a massive, nearly perfect sphere of hot plasma, heated to incandescence by nuclear fusion reactions in its core, radiating the energy from its surface mainly as visible light and infrared radiation with 10% at ultraviolet energies.\n",
    "        \n",
    "        Assistant: Let's consider the options and check whether or not they are correct. The context clearly identifies the Sun as 'the star at the center of the Solar System', thus 'a star' is probably the correct option. On the opposite, 'a planet' is not mentioned in the context, thus it is unlikely to be the correct option. Therefore, the correct option is 'a star'.\n",
    "        \"\"\"\n",
    "        },\n",
    "        {\"role\": \"system\", \"content\": \"Now do the same for the following question:\"},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "\n",
    "    return messages\n",
    "\n",
    "def antithesisGeneration(query, merged, candidate, sources):\n",
    "    merged = ast.literal_eval(merged)\n",
    "    prompt = create_message_antithesis_cot(query, candidate, merged, sources)\n",
    "    output = pipe(prompt, **generation_args)\n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "23485e9d-172b-4ec0-bda3-432ef0eaa411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_message_antithesis_ctx(question, candidate, options, context):\n",
    "    options_str = '\", \"'.join(options)\n",
    "    content = f\"\"\"\n",
    "\n",
    "    Now do the same for this question: \"{question}\", where options: [\"{options_str}\"]. Assistant:\n",
    "    \"\"\"\n",
    "\n",
    "    user_content = \"Question: \" + question + \"\\n Options: \" + str(options) + \"\\n Candidate answer: \" + candidate + \"\\n Context: \" + context + \"\\n Assistant: \"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"\n",
    "        You are an helpful AI assistant. You are asked to determine the most correct answer for a given question, provided a set of possible options.\n",
    "        You also have at disposal a first tentative answer that you are required to check with respect to the question and the relevant context.\n",
    "        Your goal is to decree which is the most correct answer to the question between the available options.\n",
    "\n",
    "        Here's an example of how to do it:\n",
    "        \"\"\"},\n",
    "        {\"role\": \"user\", \"content\": \"\"\"\n",
    "        Question: What languages did John Osteen speak or write?\n",
    "        Options: ['english', 'greek', 'koine greek', 'nahuatl', 'spanish']\n",
    "        Candidate answer: english\n",
    "        Context: Christianity is a monotheistic religion based on the life and teachings of Jesus Christ. It is the largest religion in the world, with over 2 billion followers. Christians believe in one God and follow the teachings of Jesus Christ, who they believe is the Son of God and the savior of humanity.\\n\\nLakewood Church is a nondenominational charismatic Christian megachurch located in Houston, Texas. It is the largest congregation in the United States, with an average attendance of about 52,000 people per week. The church is led by Joel Osteen, who is the son of John Hillery Osteen, the founder and first pastor of Lakewood Church.\\n\\nMexico is a federal republic in North America, bordered by the United States, Guatemala, Belize, the Caribbean Sea, and the Pacific Ocean. It is the most populous Spanish-speaking country in the world and the second most populous country in Latin America. Mexico is a federation comprising 31 states and a federal district that is also its capital and most populous city.\\n\\nJohn Hillery Osteen founded Lakewood Church in Houston, Texas, in 1959. He served as the first pastor until his death in 1999. His son, Joel Osteen, succeeded him as pastor and continues to lead the church today.\\n\\nHouston is the most populous city in Texas and the fourth-most populous city in the United States. It is located in Southeast Texas near the Gulf of Mexico and is the seat of Harris County. Houston is the principal city of the Houston metropolitan area, which is the fifth-most populated metropolitan area in the United States.\\n\\nIn summary, Christianity is a monotheistic religion based on the life and teachings of Jesus Christ, with over 2 billion followers worldwide. Lakewood Church in Houston, Texas, is the largest congregation in the United States, led by Joel Osteen, the son of John Hillery Osteen, the founder and first pastor of the church. Houston is the most populous city in Texas and the fourth-most populous city in the United States, and is the principal city of the Houston metropolitan area.\n",
    "        \n",
    "        Assistant: \n",
    "        \"\"\"\n",
    "        },\n",
    "        {\"role\": \"assistant\", \"content\": \"\"\"\n",
    "        The context states that John Hillery Osteen is the founder and first pastor of Lakewood Church in Houston, Texas. Even though Mexico is the most populous Spanish-speaking country in the world and the second most populous country in Latin America, we do not have evidence that the language is spanish. \n",
    "        The options 'greek', 'koine greek', 'nahuatl' do not figure in the context and are not proper answers. Thus the most reasonable option is 'english'.\n",
    "        \"\"\"\n",
    "        },\n",
    "        {\"role\": \"system\", \"content\": \"Now do the same for the following question:\"},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "\n",
    "    return messages\n",
    "\n",
    "def antithesisGeneration(query, merged, candidate, sources):\n",
    "    merged = ast.literal_eval(merged)\n",
    "    prompt = create_message_antithesis_ctx(query, candidate, merged, sources)\n",
    "    output = pipe(prompt, **generation_args)\n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "112dfadf-af0c-4400-ad71-0c5bbb743722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_message_presynthesis_original(question, candidate, suggestion, options, context):\n",
    "\n",
    "    user_content = \"Question: \" + question + \"\\n Options: \" + str(options) + \"\\n Candidate answer: \" + candidate + \"\\n Suggestion: \" + suggestion + \"\\n Context: \" + context + \"\\n Assistant: \\n\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"\n",
    "        You are an helpful AI assistant. You are asked to determine the most correct answer for a given question, provided a set of possible options.\n",
    "        You also have at disposal a first tentative answer and a suggestion on which is the correct answer.\n",
    "        Your goal is to decree which is the most correct answer to the question between the available options according to the context.\n",
    "\n",
    "        Here's an example of how to do it:\n",
    "        \"\"\"},\n",
    "        {\"role\": \"user\", \"content\": \"\"\"\n",
    "        Question: What is the sun?\n",
    "        Options: ['a star', 'a planet']\n",
    "        Candidate answer: a planet\n",
    "        Suggestion: a star is the correct option since the context clearly specifies that the Sun is the star at the center of the Solar System\n",
    "        Context: The Sun is the star at the center of the Solar System. It is a massive, nearly perfect sphere of hot plasma, heated to incandescence by nuclear fusion reactions in its core, radiating the energy from its surface mainly as visible light and infrared radiation with 10% at ultraviolet energies.\n",
    "\n",
    "        Assistant: the correct option is 'a star', since the suggestion is grounded in the context, even if the candidate answer does not agree.\n",
    "        \"\"\"\n",
    "        },\n",
    "        {\"role\": \"system\", \"content\": \"Now do the same for the following question:\"},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "\n",
    "    return messages\n",
    "\n",
    "def preSynthGeneration(query, candidate_answer, critique, merged, sources):\n",
    "    prompt = create_message_presynthesis_original(query, merged, candidate_answer, critique, sources)\n",
    "    output = pipe(prompt, **generation_args)\n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aa74550b-65da-4e0d-a35f-e9880f50a335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_message_presynthesis_ctx(question, candidate, suggestion, options, context):\n",
    "\n",
    "    user_content = \"Question: \" + question + \"\\n Options: \" + str(options) + \"\\n Candidate answer: \" + candidate + \"\\n Suggestion: \" + suggestion + \"\\n Context: \" + context + \"\\n Assistant: \"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"\n",
    "        You are an helpful AI assistant. You are asked to determine the most correct answer for a given question, provided a set of possible options.\n",
    "        You also have at disposal a first tentative answer and a suggestion on which is the correct answer.\n",
    "        Your goal is to decree which is the most correct answer to the question between the available options according to the context.\n",
    "\n",
    "        Here's a few examples on how to do it:\n",
    "        \"\"\"},\n",
    "        {\"role\": \"user\", \"content\": \"\"\"\n",
    "        Question: What languages did John Osteen speak or write?\n",
    "        Options: ['english', 'greek', 'koine greek', 'nahuatl', 'spanish']\n",
    "        Candidate answer: english\n",
    "        Suggestion:  The context states that John Hillery Osteen is the founder and first pastor of Lakewood Church in Houston, Texas. Even though Mexico is the most populous Spanish-speaking country in the world and the second most populous country in Latin America, we do not have evidence that the language is spanish. The options 'greek', 'koine greek', 'nahuatl' do not figure in the context and are not proper answers. Thus the most reasonable option is 'english'.\n",
    "        Context: Christianity is a monotheistic religion based on the life and teachings of Jesus Christ. It is the largest religion in the world, with over 2 billion followers. Christians believe in one God and follow the teachings of Jesus Christ, who they believe is the Son of God and the savior of humanity.\\n\\nLakewood Church is a nondenominational charismatic Christian megachurch located in Houston, Texas. It is the largest congregation in the United States, with an average attendance of about 52,000 people per week. The church is led by Joel Osteen, who is the son of John Hillery Osteen, the founder and first pastor of Lakewood Church.\\n\\nMexico is a federal republic in North America, bordered by the United States, Guatemala, Belize, the Caribbean Sea, and the Pacific Ocean. It is the most populous Spanish-speaking country in the world and the second most populous country in Latin America. Mexico is a federation comprising 31 states and a federal district that is also its capital and most populous city.\\n\\nJohn Hillery Osteen founded Lakewood Church in Houston, Texas, in 1959. He served as the first pastor until his death in 1999. His son, Joel Osteen, succeeded him as pastor and continues to lead the church today.\\n\\nHouston is the most populous city in Texas and the fourth-most populous city in the United States. It is located in Southeast Texas near the Gulf of Mexico and is the seat of Harris County. Houston is the principal city of the Houston metropolitan area, which is the fifth-most populated metropolitan area in the United States.\\n\\nIn summary, Christianity is a monotheistic religion based on the life and teachings of Jesus Christ, with over 2 billion followers worldwide. Lakewood Church in Houston, Texas, is the largest congregation in the United States, led by Joel Osteen, the son of John Hillery Osteen, the founder and first pastor of the church. Houston is the most populous city in Texas and the fourth-most populous city in the United States, and is the principal city of the Houston metropolitan area.\n",
    "        \n",
    "\n",
    "        Assistant: Both the candidate answer and the suggestion agree on the fact that the correct option is 'english'. Let's check on the context whether or not this is correct. 'greek', 'koine greek', 'nahuatl' are out of topic, 'spanish' is not mentioned in the context as a possible language spoken by John Osteen. What is sure instead is that the english church in Mexico has english-speakers. Therefore, the correct option is 'english'.\n",
    "        \"\"\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"\"\"\n",
    "        Question: Who directed the 'Forward March of the Hare' cartoon?\n",
    "        Options: ['chuck jones', 'friz freleng', 'robert mckimson', 'variety']\n",
    "        Candidate answer:\n",
    "        Suggestion: \n",
    "        Context: Foghorn Leghorn is a character from the \"Looney Tunes\" and \"Merrie Melodies\" series, created by Robert McKimson and Warren Foster. He starred in 28 cartoons from 1946 to 1963, all directed by McKimson. The \"Looney Tunes\" series, produced by Warner Bros. Pictures, introduced many famous characters, including Bugs Bunny, Daffy Duck, Porky Pig, and Foghorn Leghorn. Warner Bros. Cartoons, Inc. was responsible for these cartoons, which became iconic in American culture.\\n\\nOne of the most notable cartoons featuring Foghorn Leghorn is \"Forward March Hare,\" released in 1953. In this cartoon, Bugs Bunny accidentally enlists in the Army and causes chaos during basic training. The title refers to the March Hare, a character from Lewis Carroll\\'s \"Alice\\'s Adventures in Wonderland.\" This cartoon was released during the final months of the Korean War.\\n\\nThe \"Looney Tunes\" series, along with its sister series \"Merrie Melodies,\" were known for their comedic short films featuring various characters. These animated series were broadcast on television, shown in movie theaters, released direct-to-video, or on the internet. The series had a wide variety of genres and targeted different audiences, from children to adults.\\n\\nBugs Bunny, created by Leon Schlesinger Productions (later Warner Bros. Cartoons), became an American cultural icon and the official mascot of Warner Bros. Entertainment. The studio\\'s creative staff, including directors and animators like Chuck Jones, Friz Freleng, Robert McKimson, Tex Avery, Robert Clampett, and Frank Tashlin, are considered major figures in the art and history of traditional animation.\\n\\nIn summary, Foghorn Leghorn is a beloved character from the \"Looney Tunes\" series, known for his comedic antics and memorable cartoons. The \"Looney Tunes\" series, created by Warner Bros. Pictures, introduced many iconic characters and became a significant part of American animation history.\n",
    "        \n",
    "        Assistant: \n",
    "        \"\"\"},\n",
    "        {\"role\": \"system\", \"content\": \"Now do the same for the following question:\"},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "\n",
    "    return messages\n",
    "\n",
    "def preSynthGeneration(query, candidate_answer, critique, merged, sources):\n",
    "    prompt = create_message_presynthesis_ctx(query, merged, candidate_answer, critique, sources)\n",
    "    output = pipe(prompt, **generation_args)\n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abe2291b-5961-41dd-af73-1290d53da65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesisGeneration(query, merged, pre_answer, sources):\n",
    "    merged = ast.literal_eval(merged)\n",
    "    augmented_prompt = synthesis_chain.invoke({'question': query, \n",
    "                                            'options': merged,\n",
    "                                            'critique': pre_answer,\n",
    "                                            'context': sources})\n",
    "\n",
    "    normal_string = clean_text(augmented_prompt.text)\n",
    "    ans = new_model + normal_string + select(merged)\n",
    "    return str(ans)\n",
    "\n",
    "def extract_answer_synthesis(text):\n",
    "    # Trova l'indice in cui inizia il testo \"Why or why not the answer is correct:\"\n",
    "    start_index = text.find(\"Assistant:\\n\")\n",
    "\n",
    "    \n",
    "    # Se l'indice è stato trovato, estrai la risposta corretta\n",
    "    if start_index != -1:\n",
    "        start_index += len(\"Assistant:\\n\")\n",
    "        # Estrai il testo dopo \"Why or why not the answer is correct:\"\n",
    "        correct_answer_text = text[start_index:].strip()\n",
    "        return correct_answer_text\n",
    "    else:\n",
    "        return \"The correct answer could not be found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c1a1953-0234-4513-82bf-d7b24f3403dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9b5a62e81f47a3ad99f43893460d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", use_fast=False)\n",
    "new_model = models.Transformers(model, tokenizer, temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e679fc5c-4c26-418d-b891-f495e9b1b0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"do_sample\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99d00b0b-9cab-455a-b075-4f5240044ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../../wikihop_dataset/wikihop-merged-summarized.csv')\n",
    "\n",
    "# select a subset of the queries, just for test:\n",
    "first_queries = df['query']\n",
    "\n",
    "# same for correct answers and distractors:\n",
    "correct_answers = df['answer']\n",
    "possibilities = df['options']\n",
    "\n",
    "# and for the sources:\n",
    "sources = df['sum_supports']\n",
    "\n",
    "N_rows = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "93583acf-3392-4a3c-a93f-994361b43675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>[{&#x27;role&#x27;: &#x27;system&#x27;, &#x27;content&#x27;: &#x27;\\n        You are an helpful AI assistant. You have to provide helpful answers to the user’s questions based on the context: \\n         The Three Kingdoms period (220280) in China was marked by the competition for supremacy among the states of Wei, Shu, and Wu. Wu, also known as Eastern Wu, was founded by Sun Quan in 222 and declared independence from the Cao Wei state in 229. Sun Quan ruled as &quot;King of Wu&quot; until 229 and as &quot;Emperor of Wu&quot; until 252. The state\\&#x27;s capital was initially Jianye (present-day Nanjing, Jiangsu) but later moved to Wuchang (present-day Ezhou, Hubei). The state of Cao Wei was established by Cao Pi in 220, with its capital at Luoyang. Cao Wei\\&#x27;s authority weakened after the death of Cao Rui, leading to the rise of Sima Yi and his family. The state of Shu Han was founded by Liu Bei in 221, with its capital at Chengdu. The Yellow Turban Rebellion (184-205) marked a significant event in Taoism\\&#x27;s history due to its association with secret Taoist societies. Japan, located off the eastern coast of Asia, is an island nation in the Pacific Ocean. The Qin dynasty (221206 BC) was the first unified Chinese empire, founded by Qin Shi Huang after conquering the other states during the Warring States period. The Warring States period (481403 BC) was a time of conflict that ended with the Qin dynasty\\&#x27;s victory in 221 BC. Archimedes of Syracuse was a renowned mathematician and scientist. The concept of buoyancy, or Archimedes\\&#x27; principle, states that the upward force exerted on a submerged object is equal to the weight of the fluid displaced by the object. The state of Cao Wei was founded by Cao Cao, a warlord who rose to power during the late Eastern Han dynasty. Cao Cao\\&#x27;s son, Cao Pi, forced Emperor Xian to abdicate in his favor, establishing the Cao Wei state. Liu Bei declared himself emperor of Shu Han, while Sun&#x27;}, {&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &quot;Answer to the following question: What was the occupation of Cao Chong? providing one of these options as answer: [&#x27;academic&#x27;, &#x27;builder&#x27;, &#x27;chancellor&#x27;, &#x27;classics&#x27;, &#x27;confucian scholar&#x27;, &#x27;designer&#x27;, &#x27;duke&#x27;, &#x27;emperor&#x27;, &#x27;engineer&#x27;, &#x27;engineering&#x27;, &#x27;father&#x27;, &#x27;founder&#x27;, &#x27;general&#x27;, &#x27;king&#x27;, &#x27;leader&#x27;, &#x27;major&#x27;, &#x27;mathematician&#x27;, &#x27;military&#x27;, &#x27;official&#x27;, &#x27;peasant&#x27;, &#x27;physicist&#x27;, &#x27;physics&#x27;, &#x27;politician&#x27;, &#x27;prior&#x27;, &#x27;rebel&#x27;, &#x27;research&#x27;, &#x27;ruler&#x27;, &#x27;science&#x27;, &#x27;script&#x27;, &#x27;social reformer&#x27;, &#x27;socialist&#x27;, &#x27;sovereign&#x27;, &#x27;taiwan&#x27;]Assistant:&quot;}]<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>t</span>aiwan</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# THESIS\n",
    "answers = []\n",
    "for i in range(N_rows):\n",
    "    answers.append(extract_answer_thesis(thesisGeneration(first_queries[i], possibilities[i], sources[i])))\n",
    "\n",
    "\n",
    "# ANTITHESIS\n",
    "ant_answers = []\n",
    "for i in range(N_rows):\n",
    "    ant_answers.append(antithesisGeneration(first_queries[i], possibilities[i], answers[i], sources[i]))\n",
    "\n",
    "# SYNTHESIS\n",
    "pre_answers = []\n",
    "for i in range(N_rows):\n",
    "    pre_answers.append(preSynthGeneration(first_queries[i], possibilities[i], answers[i], ant_answers[i], sources[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6f43d422-3418-4354-9c30-139536a15c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" The context mentions that Juan Miguel Rossell Milanes won the gold medal in the men's beach team competition at the 2003 Pan American Games and represented Cuba at the 1996 and 2004 Summer Olympics. Therefore, the correct answer is '1996 summer olympics'. <|end|>\",\n",
       " \" The context does not provide any information about John Osteen's languages. Therefore, we cannot determine the correct answer from the given options. <|end|>\",\n",
       " \" The context mentions that reptiles are 'tetrapod animals with a bony skull and limbs or limb-like appendages'. This indicates that the parent taxon of Proaigialosaurus, which is a reptile, is 'tetrapoda'. Therefore, the correct answer is 'tetrapoda'. <|end|>\",\n",
       " \" The context states that Mekosuchinae was a subfamily of crocodiles that lived in Australia and the South Pacific. Australosuchus is described as an extinct genus of crocodylian that belonged to the Mekosuchinae subfamily. Therefore, the parent taxon of Australosuchus is 'crocodile'. <|end|>\",\n",
       " \" The context provided discusses various historical figures and states, including Cao Cao and his son Cao Pi, who founded the state of Cao Wei. The occupation of Cao Chong is not directly mentioned in the context. However, Cao Chong was a general in the state of Cao Wei during the Three Kingdoms period. Therefore, the correct answer from the given options is 'general'. <|end|>\"]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ant_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "002302cc-dbb1-4697-94d2-9cdb193580ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' The context provided gives us information about Juan Miguel Rossell Milanes\\' participation in various sporting events. It specifically mentions that he won the gold medal in the men\\'s beach team competition at the 2003 Pan American Games and represented Cuba at the 1996 and 2004 Summer Olympics. Given the options provided, the correct answer to the question \"What event did Juan Rossell participate in?\" is \\'1996 summer olympics\\'. This is because the context directly states that he represented Cuba at the 1996 Summer Olympics, which aligns with the candidate answer. The suggestion \\'olympic games\\' is a broader term that encompasses the 1996 Summer Olympics, and\\'sport\\' is too general to be the correct answer. Therefore, the most accurate and specific answer is \\'1996 summer olympics\\'. <|end|>',\n",
       " ' The context provided does not mention any languages spoken or written by John Osteen. Therefore, based on the given information, we cannot determine the correct answer from the options provided. <|end|>',\n",
       " ' The context provided gives a comprehensive overview of herpetology, the study of amphibians and reptiles. It describes the characteristics of reptiles, such as their bony skulls, limbs or limb-like appendages, and unique skin covered in scales or scutes. The context also mentions that reptiles are ectothermic, meaning they rely on external sources of heat to regulate their body temperature.\\n\\nThe parent taxon of Proaigialosaurus, as mentioned in the context, is \\'tetrapoda\\'. Tetrapoda is a superclass of animals that includes all species with four limbs or limb-like appendages. This includes amphibians, reptiles, birds, and mammals. Since Proaigialosaurus is a reptile, it falls under the category of tetrapoda.\\n\\nTherefore, the correct answer to the question \"What is the parent taxon of Proaigialosaurus?\" is \\'tetrapoda\\'. <|end|>',\n",
       " \" The candidate answer and the suggestion both point towards 'crocodile' as the parent taxon of Australosuchus. The context provided confirms that Mekosuchinae, the subfamily to which Australosuchus belongs, is a subfamily of crocodiles. Since taxonomic classification is hierarchical, the parent taxon of a subfamily is a higher taxonomic rank. In this case, the parent taxon of Mekosuchinae is Crocodylia, which is the order that includes all true crocodiles. Therefore, the correct answer is 'crocodile'. <|end|>\",\n",
       " \" The context provided does not mention the occupation of Cao Chong. However, Cao Chong was a general in the state of Cao Wei during the Three Kingdoms period. Therefore, the correct answer from the given options is 'general'.  <|end|>\"]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f93e0a2f-f9ef-48a4-96ca-87b37ed5f11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Foghorn Leghorn is a character from the \"Looney Tunes\" and \"Merrie Melodies\" series, created by Robert McKimson and Warren Foster. He starred in 28 cartoons from 1946 to 1963, all directed by McKimson. The \"Looney Tunes\" series, produced by Warner Bros. Pictures, introduced many famous characters, including Bugs Bunny, Daffy Duck, Porky Pig, and Foghorn Leghorn. Warner Bros. Cartoons, Inc. was responsible for these cartoons, which became iconic in American culture.\\n\\nOne of the most notable cartoons featuring Foghorn Leghorn is \"Forward March Hare,\" released in 1953. In this cartoon, Bugs Bunny accidentally enlists in the Army and causes chaos during basic training. The title refers to the March Hare, a character from Lewis Carroll\\'s \"Alice\\'s Adventures in Wonderland.\" This cartoon was released during the final months of the Korean War.\\n\\nThe \"Looney Tunes\" series, along with its sister series \"Merrie Melodies,\" were known for their comedic short films featuring various characters. These animated series were broadcast on television, shown in movie theaters, released direct-to-video, or on the internet. The series had a wide variety of genres and targeted different audiences, from children to adults.\\n\\nBugs Bunny, created by Leon Schlesinger Productions (later Warner Bros. Cartoons), became an American cultural icon and the official mascot of Warner Bros. Entertainment. The studio\\'s creative staff, including directors and animators like Chuck Jones, Friz Freleng, Robert McKimson, Tex Avery, Robert Clampett, and Frank Tashlin, are considered major figures in the art and history of traditional animation.\\n\\nIn summary, Foghorn Leghorn is a beloved character from the \"Looney Tunes\" series, known for his comedic antics and memorable cartoons. The \"Looney Tunes\" series, created by Warner Bros. Pictures, introduced many iconic characters and became a significant part of American animation history. <|end|>'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "73b4b58d-ecb9-447e-b4d7-9409f5a0ffa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['chuck jones', 'friz freleng', 'robert mckimson', 'variety']\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possibilities[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2643ef67-44ea-4415-b3eb-f7192f26b160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Who directed the 'Forward March of the Hare' cartoon?\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_queries[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9d647c71-781f-4856-bf5b-ef25dcef0ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chuck jones'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_answers[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc992c7-da7e-483c-b5df-5fef1070a149",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
