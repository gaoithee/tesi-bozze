{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ea60077-acd4-43aa-bcd8-241b93d97306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    ")\n",
    "from guidance import models, select\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from operator import itemgetter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f55b2c1-fdb9-476c-a39d-d24911fe9586",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "\n",
    "# Definisci una funzione di pulizia per rimuovere caratteri non validi\n",
    "def clean_text(text):\n",
    "    return re.sub(r\"[^\\w\\s.,!?\\-:;()]+\", '', text)\n",
    "\n",
    "# Definisci una funzione di pulizia per rimuovere caratteri non validi\n",
    "def clean_text_final(text):\n",
    "    text = re.sub(r'[^\\w\\s.,!?\\'\"\\-:;()]+', '', text)  # Rimuove i caratteri speciali\n",
    "    text = re.sub(r\"['\\\"-]\", '', text)  # Rimuove apostrofi, virgolette e trattini\n",
    "    text = text.lower()  # Converte in minuscolo\n",
    "    return text\n",
    "\n",
    "#############################################\n",
    "\n",
    "# prompts and similar things:\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# prompt augmentation for the (format of the) synthesis:\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"You are a multiple-choice question answering assistant.\n",
    "Choose the most proper option between {options} that best matches with the suggestion. \n",
    "\n",
    "Question: {question}\n",
    "Context: {critique}\n",
    "Sources: {context}\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    ")\n",
    "augmentation = {\"question\": itemgetter(\"question\"),\n",
    "                \"options\": itemgetter(\"options\"), \n",
    "                \"critique\": itemgetter(\"critique\"),\n",
    "                \"context\": itemgetter(\"context\"), }\n",
    "synthesis_chain = augmentation | prompt_template \n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "#############################################\n",
    "\n",
    "def create_message_thesis(question, options, context):\n",
    "    options_str = '\", \"'.join(options)\n",
    "    content = f\"\"\"\n",
    "\n",
    "    Now do the same for this question: \"{question}\", where options: [\"{options_str}\"]. Assistant:\n",
    "    \"\"\"\n",
    "\n",
    "    user_content = \"Answer to the following question: \" + question + \" providing one of these options as answer: \" + str(options) + \"Assistant:\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"\n",
    "        You are an helpful AI assistant. You have to provide helpful answers to the user’s questions based on the context: \n",
    "        \"\"\" + context},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "\n",
    "    return messages\n",
    "\n",
    "def extract_answer_thesis(text):\n",
    "    # Trova l'indice in cui inizia il testo \"Why or why not the answer is correct:\"\n",
    "    start_index = text.find(\"}]\")\n",
    "\n",
    "    \n",
    "    # Se l'indice è stato trovato, estrai la risposta corretta\n",
    "    if start_index != -1:\n",
    "        start_index += len(\"}]\")\n",
    "        # Estrai il testo dopo \"Why or why not the answer is correct:\"\n",
    "        correct_answer_text = text[start_index:].strip()\n",
    "        return correct_answer_text\n",
    "    else:\n",
    "        return \"The correct answer could not be found.\"\n",
    "\n",
    "def thesisGeneration(query, merged, sources):\n",
    "    merged = ast.literal_eval(merged)\n",
    "    augmented_prompt = create_message_thesis(query, merged, sources)\n",
    "    ans = new_model + str(augmented_prompt) + select(merged)\n",
    "    return str(ans)\n",
    "\n",
    "#############################################\n",
    "\n",
    "def create_message_antithesis(question, candidate, options, context):\n",
    "    options_str = '\", \"'.join(options)\n",
    "    content = f\"\"\"\n",
    "\n",
    "    Now do the same for this question: \"{question}\", where options: [\"{options_str}\"]. Assistant:\n",
    "    \"\"\"\n",
    "\n",
    "    user_content = \"Question: \" + question + \"\\n Options: \" + str(options) + \"\\n Candidate answer: \" + candidate + \"\\n Context: \" + context + \"\\n Assistant: \\n\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"\"\"\n",
    "        You are an helpful AI assistant. You are asked to determine the most correct answer for a given question, provided a set of possible options.\n",
    "        You also have at disposal a first tentative answer that you are required to check with respect to the question and the relevant context.\n",
    "        Your goal is to decree which is the most correct answer to the question between the available options.\n",
    "\n",
    "        Here's an example of how to do it:\n",
    "        Question: What is the sun, a star or a planet?\n",
    "        Options: ['a star', 'a planet']\n",
    "        Candidate answer: a planet\n",
    "        Context: The Sun is the star at the center of the Solar System. It is a massive, nearly perfect sphere of hot plasma, heated to incandescence by nuclear fusion reactions in its core, radiating the energy from its surface mainly as visible light and infrared radiation with 10% at ultraviolet energies.\n",
    "        \"\"\"\n",
    "        },\n",
    "        {\"role\": \"assistant\", \"content\": \"\"\"\n",
    "        Assistant: \\n Let's consider the options and check whether or not they are correct. The context clearly identifies the Sun as 'the star at the center of the Solar System', thus 'a star' is probably the correct option. On the opposite, 'a planet' is not mentioned in the context, thus it is unlikely to be the correct option. Therefore, the correct option is 'a star'.\n",
    "        \"\"\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Now do the same for the following question: \\n\" + user_content}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt\n",
    "\n",
    "def extract_antithesis(text):\n",
    "    pattern = re.compile(r'<start_of_turn>model(.*?)<end_of_turn>', re.DOTALL)\n",
    "    matches = pattern.findall(text)\n",
    "    \n",
    "    if matches:\n",
    "        # Prendi l'ultimo match\n",
    "        extracted_text = matches[-1]\n",
    "        # Rimuovi i simboli \"_\"\n",
    "        cleaned_text = extracted_text.replace('▁', '').strip()\n",
    "        return cleaned_text\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def antithesisGeneration(query, merged, candidate, sources):\n",
    "    merged = ast.literal_eval(merged)\n",
    "    prompt = create_message_antithesis(query, candidate, merged, sources)\n",
    "    inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=500)\n",
    "    return extract_antithesis(tokenizer.decode(outputs[0]))\n",
    "\n",
    "#############################################\n",
    "\n",
    "def create_message_presynthesis(question, candidate, suggestion, options, context):\n",
    "    user_content = \"Question: \" + question + \"\\n Options: \" + str(options) + \"\\n Candidate answer: \" + candidate + \"\\n Suggestion: \" + suggestion + \"\\n Context: \" + context \n",
    "    chat = [\n",
    "            {\"role\": \"user\", \"content\": \"\"\"\n",
    "            You are an helpful AI assistant. You are asked to determine the most correct answer for a given question, provided a set of possible options.\n",
    "            You also have at disposal a first tentative answer that you are required to check with respect to the question and the relevant context.\n",
    "            Your goal is to decree which is the most correct answer to the question between the available options.\n",
    "    \n",
    "            Here's an example of how to do it:\n",
    "            Question: What is the sun, a star or a planet?\n",
    "            Options: ['a star', 'a planet']\n",
    "            Candidate answer: a planet\n",
    "            Context: The Sun is the star at the center of the Solar System. It is a massive, nearly perfect sphere of hot plasma, heated to incandescence by nuclear fusion reactions in its core, radiating the energy from its surface mainly as visible light and infrared radiation with 10% at ultraviolet energies.\n",
    "            \"\"\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"\"\"\n",
    "            The correct answer should be 'a star' due to the fact that the context explicitly say so. On the opposite, the context never mentions the fact that the Sun could be a planet.\n",
    "            \"\"\"\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": \"Now do the same for the following question: \"+ user_content}\n",
    "        ]\n",
    "    prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt\n",
    "\n",
    "def preSynthGeneration(query, candidate_answer, critique, merged, sources):\n",
    "    prompt = create_message_presynthesis(query, merged, candidate_answer, critique, sources)\n",
    "    inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=500)\n",
    "    return extract_antithesis(tokenizer.decode(outputs[0]))\n",
    "    \n",
    "#############################################\n",
    "\n",
    "def synthesisGeneration(query, merged, pre_answer, sources):\n",
    "    merged = ast.literal_eval(merged)\n",
    "    augmented_prompt = synthesis_chain.invoke({'question': query, \n",
    "                                            'options': merged,\n",
    "                                            'critique': pre_answer,\n",
    "                                            'context': sources})\n",
    "\n",
    "    normal_string = clean_text(augmented_prompt.text)\n",
    "    ans = new_model + normal_string + select(merged)\n",
    "    return str(ans)\n",
    "\n",
    "def extract_answer_synthesis(text):\n",
    "    # Trova l'indice in cui inizia il testo \"Why or why not the answer is correct:\"\n",
    "    start_index = text.find(\"Assistant:\\n\")\n",
    "\n",
    "    \n",
    "    # Se l'indice è stato trovato, estrai la risposta corretta\n",
    "    if start_index != -1:\n",
    "        start_index += len(\"Assistant:\\n\")\n",
    "        # Estrai il testo dopo \"Why or why not the answer is correct:\"\n",
    "        correct_answer_text = text[start_index:].strip()\n",
    "        return correct_answer_text\n",
    "    else:\n",
    "        return \"The correct answer could not be found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eea0b5ab-6f89-48bf-a1ad-5de093d15fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a82f01c0b2455da9e5de9c38af995c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\", token = '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', use_fast = False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    token = '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', \n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "new_model = models.Transformers(model, tokenizer, temperature=0.0)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"do_sample\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6544a31-71a0-4eaa-a3e1-d8e9b3e26986",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "\n",
    "dataset = load_dataset('saracandu/hotpotQA_nli', split=\"train\", trust_remote_code=True)\n",
    "\n",
    "# select a subset of the queries, just for test:\n",
    "first_queries = dataset['question']\n",
    "\n",
    "# same for correct answers and distractors:\n",
    "correct_answers = dataset['answer']\n",
    "possibilities = dataset['options']\n",
    "\n",
    "# and for the sources:\n",
    "sources = dataset['passages']\n",
    "\n",
    "#nli\n",
    "first_nli = dataset['first nli']\n",
    "second_nli = dataset['second nli']\n",
    "\n",
    "bart1 = dataset['BART1']\n",
    "bart2 = dataset['BART2']\n",
    "\n",
    "rob1 = dataset['ROBERTA1']\n",
    "rob2 = dataset['ROBERTA2']\n",
    "\n",
    "N_rows = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1643e71-40b8-4318-98ed-0e64ca7ec0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>[{&#x27;role&#x27;: &#x27;system&#x27;, &#x27;content&#x27;: &quot;\\n        You are an helpful AI assistant. You have to provide helpful answers to the user’s questions based on the context: \\n        Henri Leconte (born 4 July 1963) is a former French professional tennis player. He reached the men&#x27;s singles final at the French Open in 1988, won the French Open men&#x27;s doubles title in 1984, and helped France win the Davis Cup in 1991. Leconte&#x27;s career-high singles ranking was world No. 5. Jonathan Stark (born April 3, 1971) is a former professional tennis player from the United States. During his career he won two Grand Slam doubles titles (the 1994 French Open Men&#x27;s Doubles and the 1995 Wimbledon Championships Mixed Doubles). Stark reached the World No. 1 doubles ranking in 1994.&quot;}, {&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &quot;Answer to the following question: Which tennis player won more Grand Slam titles, Henri Leconte or Jonathan Stark? providing one of these options as answer: [&#x27;Henri Leconte&#x27;, &#x27;Jonathan Stark&#x27;]Assistant:&quot;}]<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>Henri</span> Leconte</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############################################\n",
    "\n",
    "# THESIS\n",
    "answers = []\n",
    "for i in range(N_rows):\n",
    "    answers.append(extract_answer_thesis(thesisGeneration(first_queries[i], possibilities[i], sources[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0a0e842-7041-441d-9392-b423219534aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANTITHESIS\n",
    "ant_answers = []\n",
    "for i in range(N_rows):\n",
    "    ant_answers.append(antithesisGeneration(first_queries[i], possibilities[i], answers[i], sources[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc554b3f-61f9-4924-94c9-75abdcd4a123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Assistant: \\n\\nThe context states that \"First for Women\" was started in 1989.Arthur\\'s Magazine was published from 1844 to 1846.Therefore, **Arthur\\'s Magazine** was started first.',\n",
       " \"Assistant: \\n\\nThe context provides information about both players' careers, but it doesn't explicitly state who won more Grand Slam titles.\\n\\nHowever, we can infer that Henri Leconte likely won more Grand Slam titles based on the information provided.\\n\\nTherefore, the most correct answer is **Henri Leconte**.\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ant_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bbec5bd-4039-4ac0-b0eb-b079933f6999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SYNTHESIS\n",
    "pre_answers = []\n",
    "for i in range(N_rows):\n",
    "    pre_answers.append(preSynthGeneration(first_queries[i], possibilities[i], answers[i], ant_answers[i], sources[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1949670-b9ca-4f17-9fd5-9a2b3a71b690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The correct answer is **Arthur\\'s Magazine**. \\n\\nHere\\'s why:\\n\\n* The context clearly states that Arthur\\'s Magazine was published from 1844 to 1846.\\n* It also mentions that \"First for Women\" was started in 1989. \\n\\nTherefore, Arthur\\'s Magazine was published before \"First for Women\". \\n\\n\\n**Suggestion:** [\"Arthur\\'s Magazine\", \\'First for Women\\']',\n",
       " \"The most correct answer is **Henri Leconte**. \\n\\nHere's why:\\n\\n* **The context focuses on their careers and achievements.**While it mentions both players' successes, it doesn't explicitly state who won more Grand Slam titles.\\n* **Henri Leconte's career highlights** are mentioned, including reaching the French Open final and winning the French Open doubles title. \\n* **The context doesn't provide enough information to definitively say who won more.**We can infer that Leconte likely won more based on the information provided. \\n\\n\\nLet me know if you'd like to explore other questions!\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7983d9e5-424d-4e4b-a722-439d2df9400d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>You are a multiple-choice question answering assistant.\n",
       "Choose the most proper option between Henri Leconte, Jonathan Stark that best matches with the suggestion. \n",
       "\n",
       "Question: Which tennis player won more Grand Slam titles, Henri Leconte or Jonathan Stark?\n",
       "Context: The most correct answer is Henri Leconte. \n",
       "\n",
       "Heres why:\n",
       "\n",
       " The context focuses on their careers and achievements.While it mentions both players successes, it doesnt explicitly state who won more Grand Slam titles.\n",
       " Henri Lecontes career highlights are mentioned, including reaching the French Open final and winning the French Open doubles title. \n",
       " The context doesnt provide enough information to definitively say who won more.We can infer that Leconte likely won more based on the information provided. \n",
       "\n",
       "\n",
       "Let me know if youd like to explore other questions!\n",
       "Sources: Henri Leconte (born 4 July 1963) is a former French professional tennis player. He reached the mens singles final at the French Open in 1988, won the French Open mens doubles title in 1984, and helped France win the Davis Cup in 1991. Lecontes career-high singles ranking was world No. 5. Jonathan Stark (born April 3, 1971) is a former professional tennis player from the United States. During his career he won two Grand Slam doubles titles (the 1994 French Open Mens Doubles and the 1995 Wimbledon Championships Mixed Doubles). Stark reached the World No. 1 doubles ranking in 1994.\n",
       "\n",
       "Assistant:\n",
       "<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>Henri</span> Leconte</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# format synthesis\n",
    "syn_answers = []\n",
    "for i in range(N_rows):\n",
    "    syn_answers.append(extract_answer_synthesis(\n",
    "        synthesisGeneration(\n",
    "            first_queries[i], possibilities[i], \n",
    "            pre_answers[i], sources[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea359faf-8b43-4e76-9ef9-c3715159d833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Arthur's Magazine\", 'Henri Leconte']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7728b9aa-dbec-4891-bd45-db21600aa166",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
