{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a268f4e1-3a6d-464d-885b-2053e3143a2f",
   "metadata": {},
   "source": [
    "# Dataset, documents, FAISS; retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96db4863-a54b-4d3a-87e2-74c645fcc018",
   "metadata": {},
   "source": [
    "## üîπ Load the dataset containing the tuples `(query, correct_answer, distractor_1, distractor_2)` and the one containing the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ca6be79-c73c-4254-b85c-e21ac287b0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "094ab87a-0ba8-4878-991e-01515de302e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'options', 'answer', 'type', 'level', 'selected_passages'],\n",
       "    num_rows: 352\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('saracandu/filtered_hotpotQA', split=\"train\", trust_remote_code=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3232ca1-3306-450e-9654-a9a47738f61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Who was inducted into the Rock and Roll Hall of Fame, David Lee Roth or Cia Berg?',\n",
       " 'options': \"['Cia Berg', 'David Lee Roth']\",\n",
       " 'answer': 'David Lee Roth',\n",
       " 'type': 'comparison',\n",
       " 'level': 'medium',\n",
       " 'selected_passages': 'Cia Berg (born 2 December 1963), now known as Cia Soro, is a Swedish television presenter and singer. She was at one time the lead singer of the Swedish rock band Whale, who released the single \"Hobo Humpin\\' Slobo Babe\". David Lee Roth (born October 10, 1954) is an American rock vocalist, musician, songwriter, actor, author, and former radio personality. In 2007, he was inducted into the Rock and Roll Hall of Fame.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926bb532-fa37-4cfe-89fa-731ad140276d",
   "metadata": {},
   "source": [
    "# Model loading and dataset selection (for testing purposes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86132ec0-f0d0-4316-b71e-92af9861cda2",
   "metadata": {},
   "source": [
    "## ‚ñ™Ô∏è Upload the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ce89a37-b6e1-48ff-b8fb-621e456392c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e886e221a442d7b06ed6bdc08a052c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# do not run this unless necessary!\n",
    "\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f49a397f-4014-4dbc-9926-7621a862f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cb7d9b3-c789-4d50-a990-b10f3538330c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5864a53bc4e4d03a07e2c6b360a3391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "#################################################################\n",
    "# Tokenizer\n",
    "#################################################################\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "#################################################################\n",
    "# bitsandbytes parameters\n",
    "#################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "#################################################################\n",
    "# Set up quantization config\n",
    "#################################################################\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "#################################################################\n",
    "# Load pre-trained config\n",
    "#################################################################\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    quantization_config=bnb_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d17128d-7b94-4e6a-9ba2-95a8dc2d4107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from guidance import models, select\n",
    "\n",
    "new_model = models.Transformers(model, tokenizer, temperature=0.0)\n",
    "\n",
    "# new_model + f'Do you want a joke or a poem? A ' + select(['joke', 'poem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3df2428-4a0c-4d13-9455-05cc97aaf45e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ‚ñ™Ô∏è Test with `guidance`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0545fee1-7acd-4c85-8f01-5cff7fdc7556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>You are a multiple-choice question answering assistant.\n",
       "Choose which of the following options: a star, a planet, a galaxy is the object below.\n",
       "\n",
       "Object: the sun\n",
       "a<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> star</span></pre>"
      ],
      "text/plain": [
       "<guidance.models.transformers._transformers.Transformers at 0x7fd8e0502ac0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message = \"\"\"You are a multiple-choice question answering assistant.\n",
    "Choose which of the following options: a star, a planet, a galaxy is the object below.\n",
    "\n",
    "Object: the sun\n",
    "\"\"\"\n",
    "\n",
    "new_model + system_message + select([\"a planet\", \"a galaxy\", \"a star\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "580d12a6-e140-4675-8721-6577a8e5939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from operator import itemgetter\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"You are a multiple-choice question answering assistant.\n",
    "You have a suggestion on which answer is the most appropriate, that is treated as context. Use the suggestion to choose the most proper option.\n",
    "You also have an attempt of answer that you are suggested to neglect. \n",
    "\n",
    "Question: {question}\n",
    "Attempt: {candidate_answer}\n",
    "Context: {critique}\n",
    "\n",
    "The most proper option between {options} is:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "augmentation = {\"question\": itemgetter(\"question\"),\n",
    "                \"options\": itemgetter(\"options\"), \n",
    "                \"candidate_answer\": itemgetter(\"candidate_answer\"),\n",
    "                \"critique\": itemgetter(\"critique\"),\n",
    "                \"context\": itemgetter(\"context\"), }\n",
    "\n",
    "synthesis_chain = augmentation | prompt_template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25fe594c-9c67-43cf-81ba-a4a26636b77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffles the order of the vector containing the correct answer and the two distractors\n",
    "# returns another vector, shuffled\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Definisci una funzione di pulizia per rimuovere caratteri non validi\n",
    "def clean_text(text):\n",
    "    return re.sub(r\"[^\\w\\s.,!?\\-:;()]+\", '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e54e88d4-401d-46f4-92a4-a8c55fc6984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesisGeneration(query, prompt_template, merged, candidate_answer, critique, sources):\n",
    "    # merged = ast.literal_eval(merged)\n",
    "    augmented_prompt = synthesis_chain.invoke({'question': query, \n",
    "                                            'options': merged,\n",
    "                                            'candidate_answer': candidate_answer,\n",
    "                                            'critique': critique,\n",
    "                                            'context': sources})\n",
    "\n",
    "    normal_string = clean_text(augmented_prompt.text)\n",
    "    new_model + normal_string + select(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ae4f975-7901-4eb9-9874-74d14f5b36f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>You are a multiple-choice question answering assistant.\n",
       "You have a suggestion on which answer is the most appropriate, that is treated as context. Use the suggestion to choose the most proper option.\n",
       "You also have an attempt of answer that you are suggested to neglect. \n",
       "\n",
       "Question: what is the sun\n",
       "Attempt: planet\n",
       "Context: the correct answer is: a star since bot an asteroid and a planet are inadequate and not supported by the context\n",
       "\n",
       "The most proper option between star, planet is:\n",
       "<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>s</span>tar</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "synthesisGeneration('what is the sun', prompt_template, ['star', 'planet'], 'planet', \n",
    "                    'the correct answer is: a star since bot an asteroid and a planet are inadequate and not supported by the context', \n",
    "                    'The Sun is the star at the center of the Solar System. It is a massive, nearly perfect sphere of hot plasma, heated to incandescence by nuclear fusion reactions in its core, radiating the energy from its surface mainly as visible light and infrared radiation with 10% at ultraviolet energies.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c952a714-1f7c-48d0-8538-10bc2717091e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>You are a multiple-choice question answering assistant.\n",
       "You have a suggestion on which answer is the most appropriate, that is treated as context. Use the suggestion to choose the most proper option.\n",
       "You also have an attempt of answer that you are suggested to neglect. \n",
       "\n",
       "Question: Which magazine was started first Arthurs Magazine or First for Women?\n",
       "Attempt: First for Women\n",
       "Context: the correct answer is Arthurs Magazine and the context agrees\n",
       "\n",
       "The most proper option between First for Women, Arthurs Magazine is:\n",
       "<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>A</span>rthur&#x27;s Magazine</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "source = \"\"\" Arthur\\'s Magazine (1844‚Äì1846) was an American literary periodical published in Philadelphia in the 19th century. \n",
    "Edited by T.S. Arthur, it featured work by Edgar A. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G. Spear, and others. \n",
    "In May 1846 it was merged into \"Godey\\'s Lady\\'s Book. First for Women is a woman\\'s magazine published by Bauer Media Group in the USA.  \n",
    "The magazine was started in 1989. It is based in Englewood Cliffs, New Jersey. In 2011 the circulation of the magazine was 1,310,696 copies.\n",
    "    \"\"\"\n",
    "\n",
    "synthesisGeneration('Which magazine was started first Arthur\\'s Magazine or First for Women?', \n",
    "                    prompt_template, ['First for Women', 'Arthur\\'s Magazine'], 'First for Women', \n",
    "                    'the correct answer is Arthur\\'s Magazine and the context agrees', \n",
    "                    source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a80b5f8-3ff3-4531-a8e8-bdaf5f167c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>You are a multiple-choice question answering assistant.\n",
       "You have a suggestion on which answer is the most appropriate, that is treated as context. Use the suggestion to choose the most proper option.\n",
       "You also have an attempt of answer that you are suggested to neglect. \n",
       "\n",
       "Question: The Oberoi family is part of a hotel company that has a head office in what city?\n",
       "Attempt: Sammardenchia\n",
       "Context: Delhi is correct\n",
       "\n",
       "The most proper option between Delhi, Sammardenchia is:\n",
       "<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>S</span>ammardenchia</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "source = \"\"\" \"The Oberoi family is an Indian family that is famous for its involvement in hotels, namely through The Oberoi Group. \n",
    "The Oberoi Group is a hotel company with its head office in Delhi. \n",
    "Founded in 1934, the company owns and/or operates 30+ luxury hotels and two river cruise ships in six countries, primarily under its Oberoi Hotels & Resorts and Trident Hotels brands.\".\n",
    "    \"\"\"\n",
    "\n",
    "synthesisGeneration('The Oberoi family is part of a hotel company that has a head office in what city?', \n",
    "                    prompt_template, ['Delhi', 'Sammardenchia'], 'Sammardenchia', \n",
    "                    'Delhi is correct', \n",
    "                    source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e819edc2-c138-472e-bd94-31559555e63b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>You are a multiple-choice question answering assistant.\n",
       "You have a suggestion on which answer is the most appropriate, that is treated as context. Use the suggestion to choose the most proper option.\n",
       "You also have an attempt of answer that you are suggested to neglect. \n",
       "\n",
       "Question: Musician and satirist Allie Goertz wrote a song about the The Simpsons character Milhouse, who Matt Groening named after who?\n",
       "Attempt: Obama\n",
       "Context: the correct answer is Nixon\n",
       "\n",
       "The most proper option between Nixon, Obama is:\n",
       "<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>O</span>bama</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "source = \"\"\" Allison Beth Allie Goertz (born March 2, 1991) is an American musician. \n",
    "Goertz is known for her satirical songs based on various pop culture topics. Her videos are posted on YouTube under the name of Cossbysweater. \n",
    "Subjects of her songs have included the film The Room, the character Milhouse from the television show The Simpsons, and the game Dungeons & Dragons. \n",
    "Her style has been compared to that of Bo Burnham. In December 2015, Goertz released a concept album based on the Adult Swim series Rick and Morty, Sad Dance Songs, \n",
    "with the album\\'s cover emulating the animation and logo of the series.  The album was made possible through Kickstarter. \n",
    "She is co-host of Everything's Coming Up Podcast, a Simpsons-focused podcast along with Julia Prescott. \n",
    "Milhouse Mussolini van Houten is a fictional character featured in the animated television series The Simpsons, voiced by Pamela Hayden, and created by Matt Groening \n",
    "who named the character after President Richard Nixon\\'s middle name. Later in the series, it is revealed that Milhouse\\'s middle name is Mussolini. \"\n",
    "    \"\"\"\n",
    "\n",
    "synthesisGeneration('Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?', \n",
    "                    prompt_template, ['Nixon', 'Obama'], 'Obama', \n",
    "                    'the correct answer is Nixon', \n",
    "                    source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84b184cf-41a6-4f5a-9fa5-d9f547fe8243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>You are a multiple-choice question answering assistant.\n",
       "You have a suggestion on which answer is the most appropriate, that is treated as context. Use the suggestion to choose the most proper option.\n",
       "You also have an attempt of answer that you are suggested to neglect. \n",
       "\n",
       "Question: What profession does Nicholas Ray and Elia Kazan have in common?\n",
       "Attempt: director\n",
       "Context: the correct answer is writer\n",
       "\n",
       "The most proper option between director, writer is:\n",
       "<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>d</span>irector</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "source = \"\"\" \n",
    "Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 ‚Äì June 16, 1979) was an American film director best known for the movie Rebel Without a Cause. \n",
    "Elia Kazan (born Elias Kazantzoglou September 7, 1909 ‚Äì September 28, 2003) was a Greek-American director, producer, writer and actor, described by The New York Times as one of the most honored and influential directors in Broadway and Hollywood history.\n",
    "    \"\"\"\n",
    "\n",
    "synthesisGeneration('What profession does Nicholas Ray and Elia Kazan have in common?', \n",
    "                    prompt_template, ['director', 'writer'], 'director', \n",
    "                    'the correct answer is writer', \n",
    "                    source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702cc29e-5f6a-440e-a5fd-8a5bee3050f6",
   "metadata": {},
   "source": [
    "## ‚ñ™Ô∏è Select a subset of the true dataset as a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dccbd160-6698-430a-b1db-feafcd5799d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffles the order of the vector containing the correct answer and the two distractors\n",
    "# returns another vector, shuffled\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Definisci una funzione di pulizia per rimuovere caratteri non validi\n",
    "def clean_text(text):\n",
    "    return re.sub(r\"[^\\w\\s.,!?\\-:;()]+\", '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca2a69e4-be19-4f96-b4de-ab80ee00a8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_examples = 5\n",
    "# select a subset of the queries, just for test:\n",
    "first_queries = dataset['question']\n",
    "\n",
    "# same for correct answers and distractors:\n",
    "correct_answers = dataset['answer']\n",
    "possibilities = dataset['options']\n",
    "# and for the sources:\n",
    "sources = dataset['selected_passages']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b312ecad-c2c4-4791-bc82-bfaae5f5fa04",
   "metadata": {},
   "source": [
    "# Thesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f722aa12-2d2a-4868-83ef-96f01617d4d7",
   "metadata": {},
   "source": [
    "## üîπ PromptTemplate definition and a LLMChain for the **thesis** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e22fdd5d-906e-40c2-9a3e-f2063518153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_message(question, options, context):\n",
    "    options_str = '\", \"'.join(options)\n",
    "    content = f\"\"\"\n",
    "\n",
    "    Now do the same for this question: \"{question}\", where options: [\"{options_str}\"]. Assistant:\n",
    "    \"\"\"\n",
    "\n",
    "    user_content = \"Answer to the following question: \" + question + \" providing one of these options as answer: \" + str(options) + \"Assistant:\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"\n",
    "        You are an helpful AI assistant. You have to provide helpful answers to the user‚Äôs questions based on the context: \n",
    "        \"\"\" + context},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a92d91e-d18d-4de8-be05-86daff3b0532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': '\\n        You are an helpful AI assistant. You have to provide helpful answers to the user‚Äôs questions based on the context: \\n        Arthur\\'s Magazine (1844‚Äì1846) was an American literary periodical published in Philadelphia in the 19th century. Edited by T.S. Arthur, it featured work by Edgar A. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G. Spear, and others. In May 1846 it was merged into \"Godey\\'s Lady\\'s Book\". First for Women is a woman\\'s magazine published by Bauer Media Group in the USA. The magazine was started in 1989. It is based in Englewood Cliffs, New Jersey. In 2011 the circulation of the magazine was 1,310,696 copies.'},\n",
       " {'role': 'user',\n",
       "  'content': 'Answer to the following question: Which magazine was started first, Arthur\\'s Magazine or First for Women? providing one of these options as answer: [\"Arthur\\'s Magazine\", \\'First for Women\\']Assistant:'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_message(first_queries[0], possibilities[0], sources[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcc84e0-fa8c-404d-85b4-96be5448a81d",
   "metadata": {},
   "source": [
    "## üîπ Function that generates the output given the prompt, the question and the set of options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18ca079d-d85c-491f-87ab-2a0dfd626894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def thesisGeneration(query, merged, sources):\n",
    "    merged = ast.literal_eval(merged)\n",
    "    augmented_prompt = create_message(query, merged, sources)\n",
    "    # normal_string = clean_text(augmented_prompt)\n",
    "    ans = new_model + str(augmented_prompt) + select([clean_text(merged[0]), clean_text(merged[1])])\n",
    "    return str(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc34eb1d-d402-4373-b8c5-e21cd51902b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(text):\n",
    "    # Trova l'indice in cui inizia il testo \"Why or why not the answer is correct:\"\n",
    "    start_index = text.find(\"}]\")\n",
    "\n",
    "    \n",
    "    # Se l'indice √® stato trovato, estrai la risposta corretta\n",
    "    if start_index != -1:\n",
    "        start_index += len(\"}]\")\n",
    "        # Estrai il testo dopo \"Why or why not the answer is correct:\"\n",
    "        correct_answer_text = text[start_index:].strip()\n",
    "        return correct_answer_text\n",
    "    else:\n",
    "        return \"The correct answer could not be found.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecf177e-3d1d-4e08-96d3-c9bf9144c986",
   "metadata": {},
   "source": [
    "## üîπ Test: how well the thesis alone is able to perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4cf535d-2517-452e-8ef3-821dc8524cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>[{&#x27;role&#x27;: &#x27;system&#x27;, &#x27;content&#x27;: &#x27;\\n        You are an helpful AI assistant. You have to provide helpful answers to the user‚Äôs questions based on the context: \\n        Kings of Leon is an American rock band that formed in Nashville, Tennessee, in 1999. The band is composed of brothers Caleb Followill (b. January 14, 1982, lead vocals, rhythm guitar), Nathan Followill (b. June 26, 1979, drums, percussion, backing vocals) and Jared Followill (b. November 20, 1986, bass guitar, backing vocals), with their cousin Matthew Followill (b. September 10, 1984, lead guitar, backing vocals). The New Pornographers is a Canadian indie rock band formed in 1997 in Vancouver, British Columbia. Presented as a musical collective of singer-songwriters and musicians from multiple projects, the band has released seven studio albums to critical acclaim for their use of multiple vocalists and elements of power pop incorporated into their music.&#x27;}, {&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &quot;Answer to the following question: Are both The New Pornographers and Kings of Leon American rock bands? providing one of these options as answer: [&#x27;yes&#x27;, &#x27;no&#x27;]Assistant:&quot;}]<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>y</span>es</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answers = []\n",
    "for i in range(N_examples):\n",
    "    answers.append(extract_answer(thesisGeneration(first_queries[i], possibilities[i], sources[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6af99876-ef99-449f-8b1f-edf219e2537b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Arthurs Magazine', 'Henri Leconte', 'The Wolfhounds', 'yes', 'yes']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f28ff4-de3a-4c41-b8d9-0ff10b7a13f9",
   "metadata": {},
   "source": [
    "# Antithesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f12915eb-c557-4f53-8d85-84870e4fd3fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400b7f9b37a34d5e8e795b4d989d540f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "# messages = [\n",
    "#    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "#    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "#]\n",
    "\n",
    "# outputs = pipeline(\n",
    "#    messages,\n",
    "#    max_new_tokens=500,\n",
    "#    eos_token_id=terminators,\n",
    "#    do_sample=False,\n",
    "#)\n",
    "\n",
    "#print(outputs[0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4c6296c-e2ef-4906-a7a0-3b3508f0da7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_message(question, candidate, options, context):\n",
    "    options_str = '\", \"'.join(options)\n",
    "    content = f\"\"\"\n",
    "\n",
    "    Now do the same for this question: \"{question}\", where options: [\"{options_str}\"]. Assistant:\n",
    "    \"\"\"\n",
    "\n",
    "    user_content = \"Question: \" + question + \"\\n Options: \" + str(options) + \"\\n Candidate answer: \" + candidate + \"\\n Context: \" + context + \"\\n Assistant: \\n\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"\n",
    "        You are an helpful AI assistant. You are asked to determine the most correct answer for a given question, provided a set of possible options.\n",
    "        You also have at disposal a first tentative answer that you are required to check with respect to the question and the relevant context.\n",
    "        Your goal is to decree which is the most correct answer to the question between the available options.\n",
    "\n",
    "        Here's an example of how to do it:\n",
    "        \"\"\"},\n",
    "        {\"role\": \"user\", \"content\": \"\"\"\n",
    "        Question: What is the sun, a star or a planet?\n",
    "        Options: ['a star', 'a planet']\n",
    "        Candidate answer: a planet\n",
    "        Context: The Sun is the star at the center of the Solar System. It is a massive, nearly perfect sphere of hot plasma, heated to incandescence by nuclear fusion reactions in its core, radiating the energy from its surface mainly as visible light and infrared radiation with 10% at ultraviolet energies.\n",
    "\n",
    "        Assistant: The correct answer should be 'a star' due to the fact that the context explicitly say so. On the opposite, the context never mentions the fact that the Sun could be a planet.\n",
    "        \"\"\"\n",
    "        },\n",
    "        {\"role\": \"system\", \"content\": \"Now do the same for the following question:\"},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a643ed8f-0d35-44b4-84cd-293816c40ed4",
   "metadata": {},
   "source": [
    "## üî∏ Function to generate the antithesis given the question, the thesis, the context and the options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5ffee32-0ea6-439d-9ec5-1440dd58b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def antithesisGeneration(query, merged, candidate, sources):\n",
    "    merged = ast.literal_eval(merged)\n",
    "    prompt = create_message(query, candidate, merged, sources)\n",
    "    outputs = pipeline(prompt, max_new_tokens=500, eos_token_id=terminators, do_sample=False)\n",
    "    return outputs[0][\"generated_text\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec71c35b-0598-429f-a99d-df9e82da06b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "ant_answers = []\n",
    "for i in range(N_examples):\n",
    "    ant_answers.append(antithesisGeneration(first_queries[i], possibilities[i], answers[i], sources[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93695a06-214d-4bc0-9f41-219192027cac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': \"You are an helpful AI assistant. You are asked to determine the most correct answer for a given question, provided a set of possible options.\\n        You also have at disposal a first tentative answer that you are required to check with respect to the question and the relevant context.\\n        Your goal is to decree which is the most correct answer to the question between the available options.\\n\\n        Here's an example of how to do it:\"}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ant_answers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22c886d-a2d7-430b-8841-baa40155dc24",
   "metadata": {},
   "source": [
    "# Pre Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e2e7ee8-9b58-4b42-b5e9-35bfaaee840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_message(question, candidate, suggestion, options, context):\n",
    "\n",
    "    user_content = \"Question: \" + question + \"\\n Options: \" + str(options) + \"\\n Candidate answer: \" + candidate + \"\\n Suggestion: \" + suggestion + \"\\n Context: \" + context + \"\\n Assistant: \\n\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"\n",
    "        You are an helpful AI assistant. You are asked to determine the most correct answer for a given question, provided a set of possible options.\n",
    "        You also have at disposal a first tentative answer and a suggestion on which is the correct answer.\n",
    "        Your goal is to decree which is the most correct answer to the question between the available options according to the context.\n",
    "\n",
    "        Here's an example of how to do it:\n",
    "        \"\"\"},\n",
    "        {\"role\": \"user\", \"content\": \"\"\"\n",
    "        Question: What is the sun, a star or a planet?\n",
    "        Options: ['a star', 'a planet']\n",
    "        Candidate answer: a planet\n",
    "        Suggestion: a star is the correct option since the context clearly specifies that the Sun is the star at the center of the Solar System\n",
    "        Context: The Sun is the star at the center of the Solar System. It is a massive, nearly perfect sphere of hot plasma, heated to incandescence by nuclear fusion reactions in its core, radiating the energy from its surface mainly as visible light and infrared radiation with 10% at ultraviolet energies.\n",
    "        \"\"\"\n",
    "        },\n",
    "        {\"role\": \"assistant\", \"content\": \"\"\"\n",
    "        the correct option is 'a star', since the suggestion is grounded in the context, even if the candidate answer does not agree.\n",
    "        \"\"\"\n",
    "        },\n",
    "        {\"role\": \"system\", \"content\": \"Now do the same for the following question:\"},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad263e0f-acf1-4974-a7e0-3ce08a6f131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preSynthGeneration(query, candidate_answer, critique, merged, sources):\n",
    "    prompt = create_message(query, merged, candidate_answer, critique, sources)\n",
    "    output = pipeline(prompt, max_new_tokens=500, eos_token_id=terminators, do_sample=False)\n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "799c7103-f3fb-44c3-941f-e2dae194fcd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "pre_answers = []\n",
    "for i in range(N_examples):\n",
    "    pre_answers.append(preSynthGeneration(first_queries[i], answers[i], ant_answers[i], possibilities[i], sources[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "467d672f-2122-49b8-ac76-4f141550533d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'role': 'system',\n",
       "   'content': \"\\n        You are an helpful AI assistant. You are asked to determine the most correct answer for a given question, provided a set of possible options.\\n        You also have at disposal a first tentative answer and a suggestion on which is the correct answer.\\n        Your goal is to decree which is the most correct answer to the question between the available options according to the context.\\n\\n        Here's an example of how to do it:\\n        \"},\n",
       "  {'role': 'user',\n",
       "   'content': \"\\n        Question: What is the sun, a star or a planet?\\n        Options: ['a star', 'a planet']\\n        Candidate answer: a planet\\n        Suggestion: a star is the correct option since the context clearly specifies that the Sun is the star at the center of the Solar System\\n        Context: The Sun is the star at the center of the Solar System. It is a massive, nearly perfect sphere of hot plasma, heated to incandescence by nuclear fusion reactions in its core, radiating the energy from its surface mainly as visible light and infrared radiation with 10% at ultraviolet energies.\\n        \"},\n",
       "  {'role': 'assistant',\n",
       "   'content': \"\\n        the correct option is 'a star', since the suggestion is grounded in the context, even if the candidate answer does not agree.\\n        \"},\n",
       "  {'role': 'system', 'content': 'Now do the same for the following question:'},\n",
       "  {'role': 'user',\n",
       "   'content': 'Question: Which magazine was started first, Arthur\\'s Magazine or First for Women?\\n Options: {\\'role\\': \\'assistant\\', \\'content\\': \"You are an helpful AI assistant. You are asked to determine the most correct answer for a given question, provided a set of possible options.\\\\n        You also have at disposal a first tentative answer that you are required to check with respect to the question and the relevant context.\\\\n        Your goal is to decree which is the most correct answer to the question between the available options.\\\\n\\\\n        Here\\'s an example of how to do it:\"}\\n Candidate answer: [\"Arthur\\'s Magazine\", \\'First for Women\\']\\n Suggestion: Arthurs Magazine\\n Context: Arthur\\'s Magazine (1844‚Äì1846) was an American literary periodical published in Philadelphia in the 19th century. Edited by T.S. Arthur, it featured work by Edgar A. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G. Spear, and others. In May 1846 it was merged into \"Godey\\'s Lady\\'s Book\". First for Women is a woman\\'s magazine published by Bauer Media Group in the USA. The magazine was started in 1989. It is based in Englewood Cliffs, New Jersey. In 2011 the circulation of the magazine was 1,310,696 copies.\\n Assistant: \\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': \"You are an helpful AI assistant. You are asked to determine the most correct answer for a given question, provided a set of possible options.\\n        You also have at disposal a first tentative answer and a suggestion on which is the correct answer.\\n        Your goal is to decree which is the most correct answer to the question between the available options according to the context.\\n\\n        Here's an example of how to do it:\"}],\n",
       " [{'role': 'system',\n",
       "   'content': \"\\n        You are an helpful AI assistant. You are asked to determine the most correct answer for a given question, provided a set of possible options.\\n        You also have at disposal a first tentative answer and a suggestion on which is the correct answer.\\n        Your goal is to decree which is the most correct answer to the question between the available options according to the context.\\n\\n        Here's an example of how to do it:\\n        \"},\n",
       "  {'role': 'user',\n",
       "   'content': \"\\n        Question: What is the sun, a star or a planet?\\n        Options: ['a star', 'a planet']\\n        Candidate answer: a planet\\n        Suggestion: a star is the correct option since the context clearly specifies that the Sun is the star at the center of the Solar System\\n        Context: The Sun is the star at the center of the Solar System. It is a massive, nearly perfect sphere of hot plasma, heated to incandescence by nuclear fusion reactions in its core, radiating the energy from its surface mainly as visible light and infrared radiation with 10% at ultraviolet energies.\\n        \"},\n",
       "  {'role': 'assistant',\n",
       "   'content': \"\\n        the correct option is 'a star', since the suggestion is grounded in the context, even if the candidate answer does not agree.\\n        \"},\n",
       "  {'role': 'system', 'content': 'Now do the same for the following question:'},\n",
       "  {'role': 'user',\n",
       "   'content': 'Question: Which tennis player won more Grand Slam titles, Henri Leconte or Jonathan Stark?\\n Options: {\\'role\\': \\'assistant\\', \\'content\\': \"You are an helpful AI assistant. You are asked to determine the most correct answer for a given question, provided a set of possible options.\\\\n        You also have at disposal a first tentative answer that you are required to check with respect to the question and the relevant context.\\\\n        Your goal is to decree which is the most correct answer to the question between the available options.\\\\n\\\\n        Here\\'s an example of how to do it:\"}\\n Candidate answer: [\\'Henri Leconte\\', \\'Jonathan Stark\\']\\n Suggestion: Henri Leconte\\n Context: Henri Leconte (born 4 July 1963) is a former French professional tennis player. He reached the men\\'s singles final at the French Open in 1988, won the French Open men\\'s doubles title in 1984, and helped France win the Davis Cup in 1991. Leconte\\'s career-high singles ranking was world No. 5. Jonathan Stark (born April 3, 1971) is a former professional tennis player from the United States. During his career he won two Grand Slam doubles titles (the 1994 French Open Men\\'s Doubles and the 1995 Wimbledon Championships Mixed Doubles). Stark reached the World No. 1 doubles ranking in 1994.\\n Assistant: \\n'},\n",
       "  {'role': 'assistant',\n",
       "   'content': \"You are an helpful AI assistant. You are asked to determine the most correct answer for a given question, provided a set of possible options.\\n        You also have at disposal a first tentative answer and a suggestion on which is the correct answer.\\n        Your goal is to decree which is the most correct answer to the question between the available options according to the context.\\n\\n        Here's an example of how to do it:\"}]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_answers[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e584aa8e-354e-4a45-9d33-3925da279be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_last_assistant_content(pre_answers):\n",
    "    # Filtra gli elementi con 'role' uguale a 'assistant'\n",
    "    assistant_responses = [entry['content'] for entry in pre_answers if entry['role'] == 'assistant']\n",
    "    # Restituisci il 'content' dell'ultimo elemento filtrato\n",
    "    return assistant_responses[-1] if assistant_responses else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07aae555-c57d-4307-96b1-e42415ac6a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(N_examples):\n",
    "    pre_answers[i] = extract_last_assistant_content(pre_answers[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f600cfb8-6a35-4baa-8236-c02c2909868c",
   "metadata": {},
   "source": [
    "# Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1edb95-c057-4d4b-8a40-c24f578e6c6b",
   "metadata": {},
   "source": [
    "## üî∫ PromptTemplate definition and a LLMChain for the **synthesis** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91fc7b5-1837-4c1d-b8ef-a41b366c933a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from operator import itemgetter\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"You are a multiple-choice question answering assistant.\n",
    "Choose the most proper answer between {options} that best matches with the suggestion. \n",
    "\n",
    "Question: {question}\n",
    "Suggestion: {critique}\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "augmentation = {\"question\": itemgetter(\"question\"),\n",
    "                \"options\": itemgetter(\"options\"), \n",
    "                \"critique\": itemgetter(\"critique\"),\n",
    "                \"context\": itemgetter(\"context\"), }\n",
    "\n",
    "synthesis_chain = augmentation | prompt_template "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8170bc0d-1182-4c31-b33d-65fe64e1c385",
   "metadata": {},
   "source": [
    "## üî∫ Function to generate the synthesis given literally everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bef727-d240-4511-a556-0d2c19db4fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesisGeneration(query, prompt_template, merged, pre_answer, sources):\n",
    "    merged = ast.literal_eval(merged)\n",
    "    augmented_prompt = synthesis_chain.invoke({'question': query, \n",
    "                                            'options': merged,\n",
    "                                            'critique': pre_answer,\n",
    "                                            'context': sources})\n",
    "\n",
    "    normal_string = clean_text(augmented_prompt.text)\n",
    "    ans = new_model + normal_string + select([clean_text(merged[0]), clean_text(merged[1])])\n",
    "    return str(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9185be49-f36a-45e7-91a6-d0988fddd1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(text):\n",
    "    # Trova l'indice in cui inizia il testo \"Why or why not the answer is correct:\"\n",
    "    start_index = text.find(\"Assistant:\\n\")\n",
    "\n",
    "    \n",
    "    # Se l'indice √® stato trovato, estrai la risposta corretta\n",
    "    if start_index != -1:\n",
    "        start_index += len(\"Assistant:\\n\")\n",
    "        # Estrai il testo dopo \"Why or why not the answer is correct:\"\n",
    "        correct_answer_text = text[start_index:].strip()\n",
    "        return correct_answer_text\n",
    "    else:\n",
    "        return \"The correct answer could not be found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5856d161-623d-4258-bd8c-fbe224f1b286",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_answers = []\n",
    "for i in range(N_examples):\n",
    "    syn_answers.append(extract_answer(\n",
    "        synthesisGeneration(\n",
    "            first_queries[i], prompt_template, possibilities[i], \n",
    "            ant_answers[i], sources[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "538c897d-183c-40eb-a9e2-c05e53addd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def_answers = [\"the correct option is \" + clean_text(correct_answer)\n",
    "               + \" since the other options is not mentioned in the context\" for correct_answer in correct_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c195ce-2955-4454-af82-322872620217",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_answers[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b81cea-bebb-4fc9-9787-5b2353af1113",
   "metadata": {},
   "source": [
    "# Dataset conversion and performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec0231d-9fec-429b-8d6d-b265dbae7af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {\n",
    "    'query': first_queries,\n",
    "    'correct': correct_answers,\n",
    "    'thesis': answers,\n",
    "    'antithesis': ant_answers,\n",
    "    'pre-synthesis': pre_answers,\n",
    "    'synthesis': syn_answers,\n",
    "    'context': sources\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63baefa-86bc-41b3-a38a-807bc973b8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61349820-ad5d-4dff-a1d0-a08547f727d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('baseline-true.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d280eeea-c69b-42d2-856b-1651fc77b083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per rimuovere le quadre e ottenere solo il contenuto\n",
    "def remove_brackets(s):\n",
    "    return s.strip(\"[] \")\n",
    "\n",
    "# Definisci una funzione di pulizia per rimuovere caratteri non validi\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s.,!?\\'\"\\-:;()]+', '', text)  # Rimuove i caratteri speciali\n",
    "    text = re.sub(r\"['\\\"-]\", '', text)  # Rimuove apostrofi, virgolette e trattini\n",
    "    text = text.lower()  # Converte in minuscolo\n",
    "    return text\n",
    "\n",
    "# Applica la funzione alla colonna 'correct answer'\n",
    "df['correct'] = df['correct'].apply(clean_text)\n",
    "df['thesis'] = df['thesis'].apply(clean_text)\n",
    "df['synthesis'] = df['synthesis'].apply(clean_text)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e03a0e-1c19-41e0-9ae3-d8348bcdc3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conta quante righe combaciano e quante no\n",
    "matches = 0\n",
    "non_matches = 0\n",
    "which_ones = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    correct_answer = str(row['correct']).strip()\n",
    "    thesis = str(row['thesis']).strip()\n",
    "    \n",
    "    if correct_answer == thesis:\n",
    "        matches += 1\n",
    "    else:\n",
    "        non_matches += 1\n",
    "        which_ones.append(\"thesis: {}, Correct: {}\".format(thesis, correct_answer))\n",
    "\n",
    "print(f\"Number of matches: {matches}\")\n",
    "print(f\"Number of non-matches: {non_matches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0614ea-7259-4f87-943c-eaa830e211b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conta quante righe combaciano e quante no\n",
    "matches = 0\n",
    "non_matches = 0\n",
    "which_ones_syn = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    correct_answer = str(row['correct']).strip()\n",
    "    thesis = str(row['synthesis']).strip()\n",
    "    \n",
    "    if correct_answer == thesis:\n",
    "        matches += 1\n",
    "    else:\n",
    "        # print(\"Synthesis: {}, Correct: {}\".format(thesis, correct_answer))\n",
    "        non_matches += 1\n",
    "        which_ones_syn.append(\"Synthesis: {}, Correct: {}\".format(thesis, correct_answer))\n",
    "\n",
    "print(f\"Number of matches: {matches}\")\n",
    "print(f\"Number of non-matches: {non_matches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "37c10978-bd9b-464b-8c3b-aa0e2437201a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>correct</th>\n",
       "      <th>thesis</th>\n",
       "      <th>antithesis</th>\n",
       "      <th>pre-synthesis</th>\n",
       "      <th>synthesis</th>\n",
       "      <th>context</th>\n",
       "      <th>goat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which magazine was started first, Arthur's Mag...</td>\n",
       "      <td>arthurs magazine</td>\n",
       "      <td>first for women</td>\n",
       "      <td>The correct answer should be 'Arthur's Magazi...</td>\n",
       "      <td>The correct answer is 'Arthur's Magazine' as ...</td>\n",
       "      <td>arthurs magazine</td>\n",
       "      <td>Arthur's Magazine (1844‚Äì1846) was an American ...</td>\n",
       "      <td>arthurs magazine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which tennis player won more Grand Slam titles...</td>\n",
       "      <td>jonathan stark</td>\n",
       "      <td>henri leconte</td>\n",
       "      <td>The correct answer should be 'Jonathan Stark'...</td>\n",
       "      <td>The correct answer is 'Jonathan Stark' as he ...</td>\n",
       "      <td>jonathan stark</td>\n",
       "      <td>Henri Leconte (born 4 July 1963) is a former F...</td>\n",
       "      <td>jonathan stark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which band was founded first, Hole (the rock b...</td>\n",
       "      <td>the wolfhounds</td>\n",
       "      <td>the wolfhounds</td>\n",
       "      <td>The correct answer should be 'The Wolfhounds'...</td>\n",
       "      <td>The correct answer is 'The Wolfhounds'. The s...</td>\n",
       "      <td>the wolfhounds</td>\n",
       "      <td>The Wolfhounds are an indie pop/noise pop band...</td>\n",
       "      <td>the wolfhounds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Were Pavel Urysohn and Leonid Levin known for ...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>The correct answer should be 'no' because Pav...</td>\n",
       "      <td>The correct answer is 'no', as the context cl...</td>\n",
       "      <td>no</td>\n",
       "      <td>Leonid Anatolievich Levin ( ; Russian: –õ–µ–æ–Ω–∏ÃÅ–¥...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Are both The New Pornographers and Kings of Le...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>The correct answer should be 'no' because The...</td>\n",
       "      <td>The correct answer is 'no'. The New Pornograp...</td>\n",
       "      <td>no</td>\n",
       "      <td>Kings of Leon is an American rock band that fo...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query           correct  \\\n",
       "0  Which magazine was started first, Arthur's Mag...  arthurs magazine   \n",
       "1  Which tennis player won more Grand Slam titles...    jonathan stark   \n",
       "2  Which band was founded first, Hole (the rock b...    the wolfhounds   \n",
       "3  Were Pavel Urysohn and Leonid Levin known for ...                no   \n",
       "4  Are both The New Pornographers and Kings of Le...               yes   \n",
       "\n",
       "            thesis                                         antithesis  \\\n",
       "0  first for women   The correct answer should be 'Arthur's Magazi...   \n",
       "1    henri leconte   The correct answer should be 'Jonathan Stark'...   \n",
       "2   the wolfhounds   The correct answer should be 'The Wolfhounds'...   \n",
       "3               no   The correct answer should be 'no' because Pav...   \n",
       "4              yes   The correct answer should be 'no' because The...   \n",
       "\n",
       "                                       pre-synthesis         synthesis  \\\n",
       "0   The correct answer is 'Arthur's Magazine' as ...  arthurs magazine   \n",
       "1   The correct answer is 'Jonathan Stark' as he ...    jonathan stark   \n",
       "2   The correct answer is 'The Wolfhounds'. The s...    the wolfhounds   \n",
       "3   The correct answer is 'no', as the context cl...                no   \n",
       "4   The correct answer is 'no'. The New Pornograp...                no   \n",
       "\n",
       "                                             context              goat  \n",
       "0  Arthur's Magazine (1844‚Äì1846) was an American ...  arthurs magazine  \n",
       "1  Henri Leconte (born 4 July 1963) is a former F...    jonathan stark  \n",
       "2  The Wolfhounds are an indie pop/noise pop band...    the wolfhounds  \n",
       "3  Leonid Anatolievich Levin ( ; Russian: –õ–µ–æ–Ω–∏ÃÅ–¥...                no  \n",
       "4  Kings of Leon is an American rock band that fo...               yes  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['goat'] = syn_answers\n",
    "df['goat'] = df['goat'].apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e6e02d2b-1f49-454b-a60b-8cdd5afb0846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 314\n",
      "Number of non-matches: 38\n"
     ]
    }
   ],
   "source": [
    "# Conta quante righe combaciano e quante no\n",
    "matches = 0\n",
    "non_matches = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    correct_answer = str(row['correct']).strip()\n",
    "    thesis = str(row['goat']).strip()\n",
    "    \n",
    "    if correct_answer == thesis:\n",
    "        matches += 1\n",
    "    else:\n",
    "        # print(\"Synthesis: {}, Correct: {}\".format(thesis, correct_answer))\n",
    "        non_matches += 1\n",
    "        which_ones_syn.append(\"GOAT: {}, Correct: {}\".format(thesis, correct_answer))\n",
    "\n",
    "print(f\"Number of matches: {matches}\")\n",
    "print(f\"Number of non-matches: {non_matches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d97283d4-811a-417a-947a-18ea10059e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('phi-mini-baseline.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdba7682-6d15-4733-9310-c482b6794771",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('saracandu/hotpotQA_nli', split=\"train\", trust_remote_code=True)\n",
    "\n",
    "# select a subset of the queries, just for test:\n",
    "first_queries = dataset['question']\n",
    "\n",
    "# same for correct answers and distractors:\n",
    "correct_answers = dataset['answer']\n",
    "possibilities = dataset['options']\n",
    "\n",
    "# and for the sources:\n",
    "sources = dataset['passages']\n",
    "\n",
    "#nli\n",
    "first_nli = dataset['first nli']\n",
    "second_nli = dataset['second nli']\n",
    "\n",
    "bart1 = dataset['BART1']\n",
    "bart2 = dataset['BART2']\n",
    "\n",
    "rob1 = dataset['ROBERTA1']\n",
    "rob2 = dataset['ROBERTA2']\n",
    "\n",
    "N_rows = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60794052-fd6f-4e29-9fa2-5768ce3bde31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0feca9b46c634f7f84f9f175b8fbcf56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0218f800e6834fab81c7d440f74db680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "193751f5fdbb478ea3aa5185ef8ff944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77596a35571a4acbb1f05a74fdcc5d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7dee3e131ee4d529bb41b878a445320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a27b33e75349ebb0a2dae3ba98061d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f01edc34084008af216e6ea94053bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d04370bde6476ab657532ed53f667d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd85b1726a63457e9f49cf401d341b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad23fc95372428fa1bcd867299fb29b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073a706d3ad64cdebb4de4a62a20e012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    ")\n",
    "from guidance import models, select\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from operator import itemgetter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#############################################\n",
    "\n",
    "# Definisci una funzione di pulizia per rimuovere caratteri non validi\n",
    "def clean_text(text):\n",
    "    return re.sub(r\"[^\\w\\s.,!?\\-:;()]+\", '', text)\n",
    "\n",
    "# Definisci una funzione di pulizia per rimuovere caratteri non validi\n",
    "def clean_text_final(text):\n",
    "    text = re.sub(r'[^\\w\\s.,!?\\'\"\\-:;()]+', '', text)  # Rimuove i caratteri speciali\n",
    "    text = re.sub(r\"['\\\"-]\", '', text)  # Rimuove apostrofi, virgolette e trattini\n",
    "    text = text.lower()  # Converte in minuscolo\n",
    "    return text\n",
    "\n",
    "#############################################\n",
    "\n",
    "# prompts and similar things:\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# prompt augmentation for the (format of the) synthesis:\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"You are a multiple-choice question answering assistant.\n",
    "Choose the most proper option between {options} that best matches with the suggestion. \n",
    "\n",
    "Question: {question}\n",
    "Context: {critique}\n",
    "Sources: {context}\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    ")\n",
    "augmentation = {\"question\": itemgetter(\"question\"),\n",
    "                \"options\": itemgetter(\"options\"), \n",
    "                \"critique\": itemgetter(\"critique\"),\n",
    "                \"context\": itemgetter(\"context\"), }\n",
    "synthesis_chain = augmentation | prompt_template \n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# for generating the 'thought' of the synthesis\n",
    "system_message = \"\"\"\n",
    "    You are an helpful AI assistant.\n",
    "    You are asked to determine the most correct answer for a given question.\n",
    "    You have at disposal a first tentative answer (a candidate answer) and another opinion on which should be the correct option according to context (a suggestion).\n",
    "    \n",
    "    They could agree on the correct option; in this case, directly output the option on which they agree.\n",
    "    If instead they disagree, use the context to determine the correct answer for the question, given the set of possible options.\n",
    "    \n",
    "    The goal of the assistant is to decree which is the most correct answer to the question between the available options. \n",
    "    Answer by explicitly reporting the correct answer to you.\n",
    "\"\"\"\n",
    "user_message = \"\"\"\n",
    "    Question: {question}\n",
    "    Options: {options}\n",
    "    Candidate answer: {candidate_answer}\n",
    "    Suggestion: {critique}\n",
    "    Which of the candidate answers {options} is the most proper answer for the question?\n",
    "\"\"\"\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "#############################################\n",
    "\n",
    "def create_message_thesis(question, options, context):\n",
    "    options_str = '\", \"'.join(options)\n",
    "    content = f\"\"\"\n",
    "\n",
    "    Now do the same for this question: \"{question}\", where options: [\"{options_str}\"]. Assistant:\n",
    "    \"\"\"\n",
    "\n",
    "    user_content = \"Answer to the following question: \" + question + \" providing one of these options as answer: \" + str(options) + \"Assistant:\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"\n",
    "        You are an helpful AI assistant. You have to provide helpful answers to the user‚Äôs questions based on the context: \n",
    "        \"\"\" + context},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "\n",
    "    return messages\n",
    "\n",
    "def extract_answer_thesis(text):\n",
    "    # Trova l'indice in cui inizia il testo \"Why or why not the answer is correct:\"\n",
    "    start_index = text.find(\"}]\")\n",
    "\n",
    "    \n",
    "    # Se l'indice √® stato trovato, estrai la risposta corretta\n",
    "    if start_index != -1:\n",
    "        start_index += len(\"}]\")\n",
    "        # Estrai il testo dopo \"Why or why not the answer is correct:\"\n",
    "        correct_answer_text = text[start_index:].strip()\n",
    "        return correct_answer_text\n",
    "    else:\n",
    "        return \"The correct answer could not be found.\"\n",
    "\n",
    "def thesisGeneration(query, merged, sources):\n",
    "    merged = ast.literal_eval(merged)\n",
    "    augmented_prompt = create_message_thesis(query, merged, sources)\n",
    "    ans = new_model + str(augmented_prompt) + select(merged)\n",
    "    return str(ans)\n",
    "\n",
    "#############################################\n",
    "\n",
    "def create_message_antithesis(question, candidate, options, context):\n",
    "    options_str = '\", \"'.join(options)\n",
    "    content = f\"\"\"\n",
    "\n",
    "    Now do the same for this question: \"{question}\", where options: [\"{options_str}\"]. Assistant:\n",
    "    \"\"\"\n",
    "\n",
    "    user_content = \"Question: \" + question + \"\\n Options: \" + str(options) + \"\\n Candidate answer: \" + candidate + \"\\n Context: \" + context + \"\\n Assistant: \\n\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"\n",
    "        You are an helpful AI assistant. You are asked to determine the most correct answer for a given question, provided a set of possible options.\n",
    "        You also have at disposal a first tentative answer that you are required to check with respect to the question and the relevant context.\n",
    "        Your goal is to decree which is the most correct answer to the question between the available options.\n",
    "\n",
    "        Here's an example of how to do it:\n",
    "        \"\"\"},\n",
    "        {\"role\": \"user\", \"content\": \"\"\"\n",
    "        Question: What is the sun, a star or a planet?\n",
    "        Options: ['a star', 'a planet']\n",
    "        Candidate answer: a planet\n",
    "        Context: The Sun is the star at the center of the Solar System. It is a massive, nearly perfect sphere of hot plasma, heated to incandescence by nuclear fusion reactions in its core, radiating the energy from its surface mainly as visible light and infrared radiation with 10% at ultraviolet energies.\n",
    "\n",
    "        Assistant: The correct answer should be 'a star' due to the fact that the context explicitly say so. On the opposite, the context never mentions the fact that the Sun could be a planet.\n",
    "        \"\"\"\n",
    "        },\n",
    "        {\"role\": \"system\", \"content\": \"Now do the same for the following question:\"},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "\n",
    "    return messages\n",
    "\n",
    "def antithesisGeneration(query, merged, candidate, sources):\n",
    "    merged = ast.literal_eval(merged)\n",
    "    prompt = create_message_antithesis(query, candidate, merged, sources)\n",
    "    output = pipe(prompt, **generation_args)\n",
    "    return output[0]['generated_text']\n",
    "\n",
    "#############################################\n",
    "\n",
    "def create_message_presynthesis(question, candidate, suggestion, options, context):\n",
    "\n",
    "    user_content = \"Question: \" + question + \"\\n Options: \" + str(options) + \"\\n Candidate answer: \" + candidate + \"\\n Suggestion: \" + suggestion + \"\\n Context: \" + context + \"\\n Assistant: \\n\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"\n",
    "        You are an helpful AI assistant. You are asked to determine the most correct answer for a given question, provided a set of possible options.\n",
    "        You also have at disposal a first tentative answer and a suggestion on which is the correct answer.\n",
    "        Your goal is to decree which is the most correct answer to the question between the available options according to the context.\n",
    "\n",
    "        Here's an example of how to do it:\n",
    "        \"\"\"},\n",
    "        {\"role\": \"user\", \"content\": \"\"\"\n",
    "        Question: What is the sun, a star or a planet?\n",
    "        Options: ['a star', 'a planet']\n",
    "        Candidate answer: a planet\n",
    "        Suggestion: a star is the correct option since the context clearly specifies that the Sun is the star at the center of the Solar System\n",
    "        Context: The Sun is the star at the center of the Solar System. It is a massive, nearly perfect sphere of hot plasma, heated to incandescence by nuclear fusion reactions in its core, radiating the energy from its surface mainly as visible light and infrared radiation with 10% at ultraviolet energies.\n",
    "\n",
    "        Assistant: the correct option is 'a star', since the suggestion is grounded in the context, even if the candidate answer does not agree.\n",
    "        \"\"\"\n",
    "        },\n",
    "        {\"role\": \"system\", \"content\": \"Now do the same for the following question:\"},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "\n",
    "    return messages\n",
    "\n",
    "def preSynthGeneration(query, candidate_answer, critique, merged, sources):\n",
    "    prompt = create_message_presynthesis(query, merged, candidate_answer, critique, sources)\n",
    "    output = pipe(prompt, **generation_args)\n",
    "    return output[0]['generated_text']\n",
    "\n",
    "#############################################\n",
    "\n",
    "def synthesisGeneration(query, merged, pre_answer, sources):\n",
    "    merged = ast.literal_eval(merged)\n",
    "    augmented_prompt = synthesis_chain.invoke({'question': query, \n",
    "                                            'options': merged,\n",
    "                                            'critique': pre_answer,\n",
    "                                            'context': sources})\n",
    "\n",
    "    normal_string = clean_text(augmented_prompt.text)\n",
    "    ans = new_model + normal_string + select(merged)\n",
    "    return str(ans)\n",
    "\n",
    "#############################################\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", use_fast=False)\n",
    "new_model = models.Transformers(model, tokenizer, temperature=0.0)\n",
    "\n",
    "def query_model(\n",
    "        system_message,\n",
    "        user_message,\n",
    "        temperature = 0.0,\n",
    "        max_length=1024\n",
    "        ):\n",
    "\n",
    "    user_message = \"Question: \" + user_message + \" Correct answer:\"\n",
    "    messages = [\n",
    "        {\"role\": \"System\", \"content\": system_message},\n",
    "        {\"role\": \"User\", \"content\": user_message},\n",
    "        ]\n",
    "    prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "        )\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    sequences = pipeline(\n",
    "        prompt,\n",
    "        top_p=0.0,\n",
    "        temperature=temperature,\n",
    "        #num_return_sequences=1,\n",
    "        eos_token_id=terminators,\n",
    "        max_new_tokens=max_length,\n",
    "        return_full_text=False,\n",
    "        pad_token_id=pipeline.model.config.eos_token_id\n",
    "    )\n",
    "\n",
    "    answer = sequences[0]['generated_text']\n",
    "    return answer \n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"do_sample\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94b2142e-0792-47f3-b676-9a8f80cd2270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer_synthesis(text):\n",
    "    # Trova l'indice in cui inizia il testo \"Why or why not the answer is correct:\"\n",
    "    start_index = text.find(\"Assistant:\\n\")\n",
    "\n",
    "    \n",
    "    # Se l'indice √® stato trovato, estrai la risposta corretta\n",
    "    if start_index != -1:\n",
    "        start_index += len(\"Assistant:\\n\")\n",
    "        # Estrai il testo dopo \"Why or why not the answer is correct:\"\n",
    "        correct_answer_text = text[start_index:].strip()\n",
    "        return correct_answer_text\n",
    "    else:\n",
    "        return \"The correct answer could not be found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee1b1428-06ed-4e28-b222-4f8a20ac0f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>You are a multiple-choice question answering assistant.\n",
       "Choose the most proper option between yes, no that best matches with the suggestion. \n",
       "\n",
       "Question: Are both The New Pornographers and Kings of Leon American rock bands?\n",
       "Context: The correct option is no, since the context explicitly states that The New Pornographers is a Canadian indie rock band, while Kings of Leon is an American rock band. This indicates that the two bands have different nationalities, making the answer no the most correct choice. The suggestion yes is incorrect because it contradicts the information provided in the context.\n",
       "Sources: Kings of Leon is an American rock band that formed in Nashville, Tennessee, in 1999. The band is composed of brothers Caleb Followill (b. January 14, 1982, lead vocals, rhythm guitar), Nathan Followill (b. June 26, 1979, drums, percussion, backing vocals) and Jared Followill (b. November 20, 1986, bass guitar, backing vocals), with their cousin Matthew Followill (b. September 10, 1984, lead guitar, backing vocals). The New Pornographers is a Canadian indie rock band formed in 1997 in Vancouver, British Columbia. Presented as a musical collective of singer-songwriters and musicians from multiple projects, the band has released seven studio albums to critical acclaim for their use of multiple vocalists and elements of power pop incorporated into their music.\n",
       "\n",
       "Assistant:\n",
       "<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>no</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# THESIS\n",
    "answers = []\n",
    "for i in range(N_rows):\n",
    "    answers.append(extract_answer_thesis(thesisGeneration(first_queries[i], possibilities[i], sources[i])))\n",
    "\n",
    "\n",
    "# ANTITHESIS\n",
    "ant_answers = []\n",
    "for i in range(N_rows):\n",
    "    ant_answers.append(antithesisGeneration(first_queries[i], possibilities[i], answers[i], sources[i]))\n",
    "\n",
    "# SYNTHESIS\n",
    "pre_answers = []\n",
    "for i in range(N_rows):\n",
    "    pre_answers.append(preSynthGeneration(first_queries[i], possibilities[i], answers[i], ant_answers[i], sources[i]))\n",
    "\n",
    "\n",
    "# format synthesis\n",
    "syn_answers = []\n",
    "for i in range(N_rows):\n",
    "    syn_answers.append(extract_answer_synthesis(\n",
    "        synthesisGeneration(\n",
    "            first_queries[i], possibilities[i], \n",
    "            pre_answers[i], sources[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a676aa25-b7fa-4c56-b182-5a4d9898832b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Arthur's Magazine\", 'Jonathan Stark', 'The Wolfhounds', 'yes', 'no']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f50b24-061d-4116-bf83-54175d5dadb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
