{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a268f4e1-3a6d-464d-885b-2053e3143a2f",
   "metadata": {},
   "source": [
    "# Dataset, documents, FAISS; retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96db4863-a54b-4d3a-87e2-74c645fcc018",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üîπ Load the dataset containing the tuples `(query, correct_answer, distractor_1, distractor_2)` and the one containing the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "094ab87a-0ba8-4878-991e-01515de302e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers', 'correct_answer', 'distractor_1', 'distractor_2'],\n",
       "    num_rows: 82326\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import ast\n",
    "\n",
    "dataset = load_dataset('saracandu/msmarco_modified', split=\"train\", trust_remote_code=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5678034-5b6e-4052-a330-28f255f97efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_passages = []\n",
    "\n",
    "for row in dataset:\n",
    "    passages_data = ast.literal_eval(row['passages'])\n",
    "    try:\n",
    "        selected_index = passages_data['is_selected'].index(1)\n",
    "        selected_passage = {\n",
    "            'is_selected': 1,\n",
    "            'passage_text': passages_data['passage_text'][selected_index],\n",
    "        }\n",
    "        selected_passages.append(selected_passage)\n",
    "    except ValueError:\n",
    "        # Aggiungi un passaggio vuoto se non c'√® nessun passaggio selezionato\n",
    "        selected_passages.append({'is_selected': 0, 'passage_text': ''})\n",
    "\n",
    "# Assicurati che la lunghezza dei passaggi selezionati corrisponda alla lunghezza del dataset originale\n",
    "assert len(selected_passages) == len(dataset), \"Errore: lunghezza dei passaggi selezionati non corrisponde alla lunghezza del dataset originale\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffcdd7a5-9083-470c-893d-c0fbf75ba3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Creazione di un nuovo dizionario con i dati desiderati\n",
    "new_dataset = {\n",
    "    'answers': dataset['answers'],\n",
    "    'query': dataset['query'],\n",
    "    'query_id': dataset['query_id'],\n",
    "    'query_type': dataset['query_type'],\n",
    "    'wellFormedAnswers': dataset['wellFormedAnswers'],\n",
    "    'correct_answer': dataset['correct_answer'],\n",
    "    'distractor_1': dataset['distractor_1'],\n",
    "    'distractor_2': dataset['distractor_2'],\n",
    "    'selected_passages': selected_passages\n",
    "}\n",
    "\n",
    "# Creazione del nuovo dataset\n",
    "new_dataset = Dataset.from_dict(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20df4e88-959e-4c27-8756-15923bb36567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "380924aa92bf4175b13af32325c52ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/82326 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['answers', 'query', 'query_id', 'query_type', 'wellFormedAnswers', 'correct_answer', 'distractor_1', 'distractor_2', 'selected_passages'],\n",
       "    num_rows: 80143\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_correct_answer(example):\n",
    "    return example['correct_answer'] != '[]'\n",
    "\n",
    "# Applica la funzione di filtro\n",
    "new_dataset = new_dataset.filter(filter_correct_answer)\n",
    "\n",
    "# Visualizza le prime righe del dataset filtrato per verificare il risultato\n",
    "new_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926bb532-fa37-4cfe-89fa-731ad140276d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Model loading and dataset selection (for testing purposes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86132ec0-f0d0-4316-b71e-92af9861cda2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ‚ñ™Ô∏è Upload the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce89a37-b6e1-48ff-b8fb-621e456392c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not run this unless necessary!\n",
    "\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cc6eb0bf-fb11-4952-b14e-809b77898a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a41f5adcce4aefaa3f83c48e93fef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "#################################################################\n",
    "# Tokenizer\n",
    "#################################################################\n",
    "\n",
    "model_name=\"nvidia/Llama3-ChatQA-1.5-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "#################################################################\n",
    "# bitsandbytes parameters\n",
    "#################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "#################################################################\n",
    "# Set up quantization config\n",
    "#################################################################\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "#################################################################\n",
    "# Load pre-trained config\n",
    "#################################################################\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2d17128d-7b94-4e6a-9ba2-95a8dc2d4107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from outlines import models\n",
    "\n",
    "new_model = models.Transformers(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0545fee1-7acd-4c85-8f01-5cff7fdc7556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import outlines\n",
    "\n",
    "system_message = \"\"\"You are a sentiment-labelling assistant.\n",
    "Is the following review positive or negative?\n",
    "\n",
    "Review: This restaurant is just awesome!\n",
    "\"\"\"\n",
    "\n",
    "generator = outlines.generate.choice(new_model, [\"Positive\", \"Negative\"])\n",
    "answer = generator(system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "021dac4c-df3d-4e50-b09d-5314ee8bd317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702cc29e-5f6a-440e-a5fd-8a5bee3050f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ‚ñ™Ô∏è Select a subset of the true dataset as a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bd1087d9-3e1d-444a-a250-b57fca3862d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_examples = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2e058aac-3fa5-42b7-95db-809669ac8524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a subset of the queries, just for test:\n",
    "first_queries = new_dataset['query'][:N_examples]\n",
    "\n",
    "# same for correct answers and distractors:\n",
    "correct_answers = new_dataset['correct_answer'][:N_examples]\n",
    "distractors_1 = new_dataset['distractor_1'][:N_examples]\n",
    "distractors_2 = new_dataset['distractor_2'][:N_examples]\n",
    "# and for the sources:\n",
    "sources = new_dataset['selected_passages'][:N_examples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7e39db-091e-4510-800a-0a16f2b89e76",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ‚ñ™Ô∏è Merge the true answer and the distractors into a vector, shuffling the order of the elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cab974ea-41f0-44b3-8167-592bbd323320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffles the order of the vector containing the correct answer and the two distractors\n",
    "# returns another vector, shuffled\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Definisci una funzione di pulizia per rimuovere caratteri non validi\n",
    "def clean_text(text):\n",
    "    return re.sub(r'[^\\w\\s.,!?\\'\"\\-:;()]+', '', text)\n",
    "\n",
    "# Funzione per mescolare le risposte\n",
    "def shuffleAnswers(correct_answer, distractor_1, distractor_2):\n",
    "    # Applica la pulizia a ciascun elemento\n",
    "    correct_answer_cleaned = clean_text(correct_answer)\n",
    "    distractor_1_cleaned = clean_text(distractor_1)\n",
    "    distractor_2_cleaned = clean_text(distractor_2)\n",
    "    \n",
    "    # Unisci le opzioni pulite\n",
    "    merge_options = [correct_answer_cleaned, distractor_1_cleaned, distractor_2_cleaned]\n",
    "    \n",
    "    # Mescola le opzioni\n",
    "    random.shuffle(merge_options)\n",
    "    \n",
    "    return merge_options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b312ecad-c2c4-4791-bc82-bfaae5f5fa04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "5175f4e8-bacd-4a63-8024-9b487f80669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_options = []\n",
    "for i in range(N_examples):\n",
    "    merged_options.append(shuffleAnswers(correct_answers[i], distractors_1[i], distractors_2[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f722aa12-2d2a-4868-83ef-96f01617d4d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üîπ PromptTemplate definition and a LLMChain for the **thesis** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "34445f5e-870a-4cbc-a1f6-1b004027426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template definition\n",
    "# requires question, options (a string containing the possible options) and the context as input variables!\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "    System: This is a chat between a user and an AI assistant. The assistant gives helpful, detailed, and polite answers to the user‚Äôs questions based on the context. \n",
    "    {context}\n",
    "    User: {question}\n",
    "    Possible options: {option_a}, {option_b}, {option_c}.\n",
    "    Assistant:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcc84e0-fa8c-404d-85b4-96be5448a81d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üîπ Function that generates the output given the prompt, the question and the set of options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "07b5e395-05a8-4ba7-a960-07e6b496aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM chain definition\n",
    "from operator import itemgetter\n",
    "\n",
    "augmentation = {\"question\": itemgetter(\"question\"),\n",
    "                \"option_a\": itemgetter(\"option_a\"), \n",
    "                \"option_b\": itemgetter(\"option_b\"),\n",
    "                \"option_c\": itemgetter(\"option_c\"),\n",
    "                \"context\": itemgetter(\"context\"), }\n",
    "\n",
    "thesis_chain = augmentation | prompt_template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "18ca079d-d85c-491f-87ab-2a0dfd626894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import outlines\n",
    "\n",
    "def thesisGeneration(query, merged, sources):\n",
    "    augmented_prompt = thesis_chain.invoke({'question': query, 'option_a': merged[0], 'option_b': merged[1], 'option_c': merged[2], 'context': sources})\n",
    "    normal_string = clean_text(augmented_prompt.text)\n",
    "    options = [merged[0], merged[1], merged[2]]\n",
    "    generator = outlines.generate.choice(new_model, options)\n",
    "    answer = generator(normal_string)\n",
    "    # if not answer:\n",
    "    #    answer = random.choice(options)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecf177e-3d1d-4e08-96d3-c9bf9144c986",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üîπ Test: how well the thesis alone is able to perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b4cf535d-2517-452e-8ef3-821dc8524cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling FSM index for all state transitions: 100%|‚ñà‚ñà| 67/67 [00:03<00:00, 21.41it/s]\n",
      "Compiling FSM index for all state transitions: 100%|‚ñà| 166/166 [00:07<00:00, 21.15it/s\n",
      "Compiling FSM index for all state transitions: 100%|‚ñà‚ñà| 44/44 [00:02<00:00, 21.19it/s]\n",
      "Compiling FSM index for all state transitions: 100%|‚ñà| 186/186 [00:08<00:00, 21.13it/s\n",
      "Compiling FSM index for all state transitions: 100%|‚ñà| 148/148 [00:06<00:00, 21.30it/s\n",
      "Compiling FSM index for all state transitions: 100%|‚ñà| 471/471 [00:22<00:00, 21.41it/s\n",
      "Compiling FSM index for all state transitions: 100%|‚ñà| 282/282 [00:13<00:00, 21.14it/s\n",
      "Compiling FSM index for all state transitions: 100%|‚ñà| 132/132 [00:06<00:00, 21.09it/s\n",
      "Compiling FSM index for all state transitions: 100%|‚ñà| 275/275 [00:12<00:00, 21.18it/s\n",
      "Compiling FSM index for all state transitions: 100%|‚ñà| 439/439 [00:20<00:00, 21.10it/s\n",
      "Compiling FSM index for all state transitions: 100%|‚ñà| 395/395 [00:18<00:00, 20.86it/s\n",
      "Compiling FSM index for all state transitions: 100%|‚ñà| 364/364 [00:17<00:00, 21.28it/s\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "for i in range(N_examples):\n",
    "    merged_options = shuffleAnswers(correct_answers[i], distractors_1[i], distractors_2[i])\n",
    "    answers.append(thesisGeneration(first_queries[i], merged_options, sources[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1f867057-7595-460f-b837-2e982e97a269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.'\",\n",
       " \"'Yes'\",\n",
       " \"'Yes'\",\n",
       " \"'11 to 22 per square foot'\",\n",
       " \"'Due to symptoms in the body'\",\n",
       " \"'45,278 and 60,659'\",\n",
       " \"'The most expensive patents are international patents, which can run up to 100,000 or higher.Domestically the costs can be 10,000 or above.'\",\n",
       " \"'140 to 202'\",\n",
       " \"'Is the brand name of a drug called procaine which is a local anaesthetic.'\",\n",
       " \"'Ecosystem'\",\n",
       " \"'Dr. Seuss'\",\n",
       " \"'45 minutes to an hour'\",\n",
       " \"'6-16 a square foot'\",\n",
       " \"'Granite.', 'Granite.'\",\n",
       " \"'4.64 - 6.36'\",\n",
       " '\"A dolphin\\'s life span varies according to its environment and species. Although some bottlenose dolphins can reach 40 years of age, their average age is between 15 and 16 years. Forty is an old age for a dolphin -- one making it to 40 is comparable to a human living to be 100.\"',\n",
       " '\"In a New York State hermit\\'s letter to the editor of an Adirondack Mountain newspaper.\"',\n",
       " \"'A macintosh made from cotton fabric treated with oil and pigment to make it waterproof.'\",\n",
       " \"'7 days before to 7 days after the rash appears'\",\n",
       " \"'Africa'\"]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e222ef24-3231-4ee1-ae92-4b1b67551500",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Antithesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fc7617-7074-49ec-a1db-2d86416aeca2",
   "metadata": {},
   "source": [
    "## üî∏ PromptTemplate definition and a LLMChain for the **antithesis** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9e21293c-f26c-4c36-b5b0-dc3cffe4c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    repetition_penalty=1.5,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=400,\n",
    "    top_p=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "33184819-91d2-42f0-97ab-44daab45f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def query_model(\n",
    "        system_message,\n",
    "        user_message,\n",
    "        temperature = 0.0,\n",
    "        max_length=1024\n",
    "        ):\n",
    "\n",
    "    user_message = \"Question: \" + user_message + \" Answer:\"\n",
    "    messages = [\n",
    "        {\"role\": \"System\", \"content\": system_message},\n",
    "        {\"role\": \"User\", \"content\": user_message},\n",
    "        ]\n",
    "    prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "        )\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    sequences = pipeline(\n",
    "        prompt,\n",
    "        do_sample=False,\n",
    "        top_p=0.0,\n",
    "        temperature=temperature,\n",
    "        #num_return_sequences=1,\n",
    "        eos_token_id=terminators,\n",
    "        max_new_tokens=max_length,\n",
    "        return_full_text=False,\n",
    "        pad_token_id=pipeline.model.config.eos_token_id\n",
    "    )\n",
    "\n",
    "    answer = sequences[0]['generated_text']\n",
    "\n",
    "    return user_message + \" \" + answer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "c29e2fd3-113f-4780-8ccf-a8b9e82e850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "    This is a chat between a user and an AI assistant. The assistant gives helpful, detailed, and polite answers to the user‚Äôs questions based on the context. \n",
    "    {context}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "user_message = \"\"\"\n",
    "    You are asked to check whether or not a question was answered correctly, given a certain number of candidate options and the context.\n",
    "    Question: {question} \n",
    "    The answer has to be one of these: {option_a}, {option_b}, {option_c}. \n",
    "    Candidate answer that you need to check: {candidate_answer}\n",
    "    Consider all the possible options and determine if the candidate is the most appropriate one, given the question and the context.\n",
    "    If you think so, answer by saying 'Correct'; otherwise, answer 'Incorrect'. Add which option is most proper to you and why.\\n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a643ed8f-0d35-44b4-84cd-293816c40ed4",
   "metadata": {},
   "source": [
    "## üî∏ Function to generate the antithesis given the question, the thesis, the context and the options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "b5ffee32-0ea6-439d-9ec5-1440dd58b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def antithesisGeneration(query, prompt_template, merged, candidate_answer, sources):\n",
    "    second_answer = query_model(system_message.format(context = sources),\n",
    "    user_message.format(question=query, option_a = merged[0], option_b = merged[1], \n",
    "                        option_c = merged[2], candidate_answer = candidate_answer, context = sources,), max_length=400)\n",
    "    return second_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "bdf63893-af51-4f42-8e8b-6de9403fd05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer_ant(text):\n",
    "    # Trova l'indice in cui inizia il testo \"Why or why not the answer is correct:\"\n",
    "    start_index = text.find(\"Answer:\")\n",
    "\n",
    "    \n",
    "    # Se l'indice √® stato trovato, estrai la risposta corretta\n",
    "    if start_index != -1:\n",
    "        start_index += len(\"Answer:\")\n",
    "        # Estrai il testo dopo \"Why or why not the answer is correct:\"\n",
    "        correct_answer_text = text[start_index:].strip()\n",
    "        return correct_answer_text\n",
    "    else:\n",
    "        return \"The correct answer could not be found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "ec71c35b-0598-429f-a99d-df9e82da06b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ant_answers = []\n",
    "for i in range(N_examples):\n",
    "    ant_answers.append(extract_answer_ant(antithesisGeneration(first_queries[i], prompt_template, merged_options[i], answers[i], sources[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "93695a06-214d-4bc0-9f41-219192027cac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"correct\", \"results-based accountability\".',\n",
       " '\"correct\", because he ran as republican against president carter',\n",
       " '\"sydneysurroundingsareasneedtime\",',\n",
       " \"'correct'\\n\\nThe correct response would depend upon what specific information about Honolulu they were looking up?\",\n",
       " \"'correct','due to symptom'\\n\\nconversion\",\n",
       " 'The correct response would have been \"inside the Rib Cage\".',\n",
       " '\"Most Correct\"\\n\\nThis statement directly addresses both domestic (lower) prices as well as internationally high priced ones without any other distractions within this passage',\n",
       " '\"correct\", \"the correct response would have been sophicles aeschlylusand euripedes.\"',\n",
       " '\\'incorrect\\'\\n\\'A Lapel pin refers specifically to those lapels with holes for attaching buttons; it does NOT include any type of clothing accessory such as brooches (which attach via prongs) nor do we consider \"pins\" here since this term encompasses many different kinds including safety ones etc...\\'',\n",
       " '\"incorrect\", \"somatic\".',\n",
       " \"'correct.'\",\n",
       " \"'correct'\\n\\nThe correct time frame mentioned within this passage text as it relates directly back into directions section where they specify exactly what temperature needs set before placing them inside oven!\",\n",
       " '\"correct\", \"yeast\".',\n",
       " 'The correct response would depend upon what exactly \"granit\" refers too but I believe your intended meaning here can only refer towards Mount Pinautbo being composed primarily with Granite rock formations rather than Basalts ones?',\n",
       " 'The correct range from \"the\" text above would have been \"$0.xx-$2.yy\", as it states this will give us instant estimates with no further information provided about what we\\'re estimating (i.e., size).',\n",
       " '\"fish\".',\n",
       " '\"New York state Hermits Letter To Editor Of An adirondeck mountain Newspaper\".',\n",
       " '\"Oil skin\", as its name suggests refers back into history where sailors would use animal oils such tarred canvas for protection against water damage while at sea; however today we see \"oil skins\\' used more often than ever before due too their versatility!',\n",
       " 'The correct response would have been \"Rubela\"',\n",
       " '\\'correct\\'\\n\\nthe correct response would have been \"a\"']"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ant_answers[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f600cfb8-6a35-4baa-8236-c02c2909868c",
   "metadata": {},
   "source": [
    "# Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1edb95-c057-4d4b-8a40-c24f578e6c6b",
   "metadata": {},
   "source": [
    "## üî∫ PromptTemplate definition and a LLMChain for the **synthesis** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "6091cee6-41d3-4164-804e-f6c492e88745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "    System: This is a chat between a user and an AI assistant. The assistant gives helpful, detailed, and polite answers to the user‚Äôs questions based on the context. \n",
    "    {context}\n",
    "    \n",
    "    User: You are asked to answer a certain question, given a certain number of options and the context. You are also provided with a candidate answer and its critique: \n",
    "    Question: {question}\n",
    "    Possible options: {option_a}, {option_b}, {option_c}\n",
    "    Candidate answer: {candidate_answer}\n",
    "    Correct the candidate answer according to what the critique suggests as the correct option:\n",
    "    {critique} \\n \n",
    "    \n",
    "    Assistant:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "88d4d68c-5e0e-4818-89cb-93e1731ec3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM chain definition\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from operator import itemgetter\n",
    "\n",
    "augmentation = {\"question\": itemgetter(\"question\"),\n",
    "                \"option_a\": itemgetter(\"option_a\"), \n",
    "                \"option_b\": itemgetter(\"option_b\"),\n",
    "                \"option_c\": itemgetter(\"option_c\"),\n",
    "                \"candidate_answer\": itemgetter(\"candidate_answer\"),\n",
    "                \"critique\": itemgetter(\"critique\"),\n",
    "                \"context\": itemgetter(\"context\"), }\n",
    "\n",
    "synthesis_chain = augmentation | prompt_template "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8170bc0d-1182-4c31-b33d-65fe64e1c385",
   "metadata": {},
   "source": [
    "## üî∫ Function to generate the synthesis given literally everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "38bef727-d240-4511-a556-0d2c19db4fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesisGeneration(query, prompt_template, merged, candidate_answer, critique, sources):\n",
    "    augmented_prompt = synthesis_chain.invoke({'question': query, \n",
    "                                            'option_a': merged[0], 'option_b': merged[1], 'option_c': merged[2], \n",
    "                                            'candidate_answer': candidate_answer,\n",
    "                                            'critique': critique,\n",
    "                                            'context': sources})\n",
    "\n",
    "    normal_string = clean_text(augmented_prompt.text)\n",
    "    options = [merged[0], merged[1], merged[2]]\n",
    "    generator = outlines.generate.choice(new_model, options)\n",
    "    final_answer = generator(normal_string)\n",
    "    \n",
    "    # if not final_answer:\n",
    "    #     final_answer = candidate_answer\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "ca4286d7-99cf-4f0c-a320-d9b3c6f84863",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling FSM index for all state transitions: 100%|‚ñà| 207/207 [00:09<00:00, 21.19it/s\n",
      "Compiling FSM index for all state transitions: 100%|‚ñà| 445/445 [00:21<00:00, 21.01it/s\n",
      "Compiling FSM index for all state transitions: 100%|‚ñà| 439/439 [00:21<00:00, 20.83it/s\n"
     ]
    }
   ],
   "source": [
    "syn_answers = []\n",
    "for i in range(N_examples):\n",
    "    merged_options = shuffleAnswers(correct_answers[i], distractors_1[i], distractors_2[i])\n",
    "    syn_answers.append(synthesisGeneration(first_queries[i], prompt_template, merged_options, answers[i], ant_answers[i], sources[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "65f3176f-7f6b-4c55-a4da-c984771bca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_new = {'query': first_queries, 'correct answer': correct_answers, 'thesis': answers, 'antithesis': ant_answers, 'synthesis': syn_answers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "ae959b96-7e8f-445e-8ddb-94e0a49116e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(dataset_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "612fcd8b-db5f-4d4a-b816-b043a28d2435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>correct answer</th>\n",
       "      <th>thesis</th>\n",
       "      <th>antithesis</th>\n",
       "      <th>synthesis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is rba</td>\n",
       "      <td>['Results-Based Accountability is a discipline...</td>\n",
       "      <td>'Results-Based Accountability is a disciplined...</td>\n",
       "      <td>\"correct\", \"results-based accountability\".</td>\n",
       "      <td>'Results-Based Accountability is a disciplined...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>was ronald reagan a democrat</td>\n",
       "      <td>['Yes']</td>\n",
       "      <td>'Yes'</td>\n",
       "      <td>\"correct\", because he ran as republican agains...</td>\n",
       "      <td>'Yes'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how long do you need for sydney and surroundin...</td>\n",
       "      <td>['20-25 minutes']</td>\n",
       "      <td>'Yes'</td>\n",
       "      <td>\"sydneysurroundingsareasneedtime\",</td>\n",
       "      <td>'Yes'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>price to install tile in shower</td>\n",
       "      <td>['$11 to $22 per square foot']</td>\n",
       "      <td>'11 to 22 per square foot'</td>\n",
       "      <td>'correct'\\n\\nThe correct response would depend...</td>\n",
       "      <td>'Honolulu'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>why conversion observed in body</td>\n",
       "      <td>['Due to symptoms in the body']</td>\n",
       "      <td>'Due to symptoms in the body'</td>\n",
       "      <td>'correct','due to symptom'\\n\\nconversion</td>\n",
       "      <td>'Due to symptoms in the body'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>where are the lungs located in the back</td>\n",
       "      <td>['Inside the rib cage.']</td>\n",
       "      <td>'45,278 and 60,659'</td>\n",
       "      <td>The correct response would have been \"inside t...</td>\n",
       "      <td>'Inside the rib cage.'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cost to get a patent</td>\n",
       "      <td>['The most expensive patents are international...</td>\n",
       "      <td>'The most expensive patents are international ...</td>\n",
       "      <td>\"Most Correct\"\\n\\nThis statement directly addr...</td>\n",
       "      <td>'The most expensive patents are international ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>best tragedies of ancient greece</td>\n",
       "      <td>['Sophocles, Aeschylus and Euripides']</td>\n",
       "      <td>'140 to 202'</td>\n",
       "      <td>\"correct\", \"the correct response would have be...</td>\n",
       "      <td>'Sophocles, Aeschylus and Euripides'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>what is a conifer</td>\n",
       "      <td>['A tree or shrub which produces distinctive c...</td>\n",
       "      <td>'Is the brand name of a drug called procaine w...</td>\n",
       "      <td>'incorrect'\\n'A Lapel pin refers specifically ...</td>\n",
       "      <td>'A tree or shrub which produces distinctive co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>in animals somatic cells are produced by and g...</td>\n",
       "      <td>['Somatic cells are produced by mitosis and ga...</td>\n",
       "      <td>'Ecosystem'</td>\n",
       "      <td>\"incorrect\", \"somatic\".</td>\n",
       "      <td>'Ecosystem'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>remembering the name of the author who wrote t...</td>\n",
       "      <td>['Dr. Seuss']</td>\n",
       "      <td>'Dr. Seuss'</td>\n",
       "      <td>'correct.'</td>\n",
       "      <td>'Dr. Seuss'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>how long cooking chicken legs in the big easy</td>\n",
       "      <td>['45 minutes to an hour']</td>\n",
       "      <td>'45 minutes to an hour'</td>\n",
       "      <td>'correct'\\n\\nThe correct time frame mentioned ...</td>\n",
       "      <td>'45 minutes to an hour'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>average cost of heating per square foot</td>\n",
       "      <td>['$6-$16 a square foot']</td>\n",
       "      <td>'6-16 a square foot'</td>\n",
       "      <td>\"correct\", \"yeast\".</td>\n",
       "      <td>'Candidiasis is a fungal infection caused by y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>is mount pinatubo made of granite or basalt</td>\n",
       "      <td>['Granite.', 'Granite.']</td>\n",
       "      <td>'Granite.', 'Granite.'</td>\n",
       "      <td>The correct response would depend upon what ex...</td>\n",
       "      <td>'To become a very good driver and To perform t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>concrete pads cost</td>\n",
       "      <td>['$4.64 - $6.36']</td>\n",
       "      <td>'4.64 - 6.36'</td>\n",
       "      <td>The correct range from \"the\" text above would ...</td>\n",
       "      <td>'4.64 - 6.36'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>what kind of organism is a black damsel</td>\n",
       "      <td>['Fish']</td>\n",
       "      <td>\"A dolphin's life span varies according to its...</td>\n",
       "      <td>\"fish\".</td>\n",
       "      <td>\"A dolphin's life span varies according to its...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>who coined the phrase it is what it is</td>\n",
       "      <td>[\"In a New York State hermit's letter to the e...</td>\n",
       "      <td>\"In a New York State hermit's letter to the ed...</td>\n",
       "      <td>\"New York state Hermits Letter To Editor Of An...</td>\n",
       "      <td>\"In a New York State hermit's letter to the ed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>what is oilskin fabric</td>\n",
       "      <td>['A macintosh made from cotton fabric treated ...</td>\n",
       "      <td>'A macintosh made from cotton fabric treated w...</td>\n",
       "      <td>\"Oil skin\", as its name suggests refers back i...</td>\n",
       "      <td>'Vice President of the United States in the sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>how long is german measles contagious</td>\n",
       "      <td>['7 days before to 7 days after the rash appea...</td>\n",
       "      <td>'7 days before to 7 days after the rash appears'</td>\n",
       "      <td>The correct response would have been \"Rubela\"</td>\n",
       "      <td>'7 days before to 7 days after the rash appears'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>what is a camerata</td>\n",
       "      <td>['The Camerata are a group of four powerful an...</td>\n",
       "      <td>'Africa'</td>\n",
       "      <td>'correct'\\n\\nthe correct response would have b...</td>\n",
       "      <td>'Africa'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  \\\n",
       "0                                         what is rba   \n",
       "1                        was ronald reagan a democrat   \n",
       "2   how long do you need for sydney and surroundin...   \n",
       "3                     price to install tile in shower   \n",
       "4                     why conversion observed in body   \n",
       "5             where are the lungs located in the back   \n",
       "6                                cost to get a patent   \n",
       "7                    best tragedies of ancient greece   \n",
       "8                                   what is a conifer   \n",
       "9   in animals somatic cells are produced by and g...   \n",
       "10  remembering the name of the author who wrote t...   \n",
       "11      how long cooking chicken legs in the big easy   \n",
       "12            average cost of heating per square foot   \n",
       "13        is mount pinatubo made of granite or basalt   \n",
       "14                                 concrete pads cost   \n",
       "15            what kind of organism is a black damsel   \n",
       "16             who coined the phrase it is what it is   \n",
       "17                             what is oilskin fabric   \n",
       "18              how long is german measles contagious   \n",
       "19                                 what is a camerata   \n",
       "\n",
       "                                       correct answer  \\\n",
       "0   ['Results-Based Accountability is a discipline...   \n",
       "1                                             ['Yes']   \n",
       "2                                   ['20-25 minutes']   \n",
       "3                      ['$11 to $22 per square foot']   \n",
       "4                     ['Due to symptoms in the body']   \n",
       "5                            ['Inside the rib cage.']   \n",
       "6   ['The most expensive patents are international...   \n",
       "7              ['Sophocles, Aeschylus and Euripides']   \n",
       "8   ['A tree or shrub which produces distinctive c...   \n",
       "9   ['Somatic cells are produced by mitosis and ga...   \n",
       "10                                      ['Dr. Seuss']   \n",
       "11                          ['45 minutes to an hour']   \n",
       "12                           ['$6-$16 a square foot']   \n",
       "13                           ['Granite.', 'Granite.']   \n",
       "14                                  ['$4.64 - $6.36']   \n",
       "15                                           ['Fish']   \n",
       "16  [\"In a New York State hermit's letter to the e...   \n",
       "17  ['A macintosh made from cotton fabric treated ...   \n",
       "18  ['7 days before to 7 days after the rash appea...   \n",
       "19  ['The Camerata are a group of four powerful an...   \n",
       "\n",
       "                                               thesis  \\\n",
       "0   'Results-Based Accountability is a disciplined...   \n",
       "1                                               'Yes'   \n",
       "2                                               'Yes'   \n",
       "3                          '11 to 22 per square foot'   \n",
       "4                       'Due to symptoms in the body'   \n",
       "5                                 '45,278 and 60,659'   \n",
       "6   'The most expensive patents are international ...   \n",
       "7                                        '140 to 202'   \n",
       "8   'Is the brand name of a drug called procaine w...   \n",
       "9                                         'Ecosystem'   \n",
       "10                                        'Dr. Seuss'   \n",
       "11                            '45 minutes to an hour'   \n",
       "12                               '6-16 a square foot'   \n",
       "13                             'Granite.', 'Granite.'   \n",
       "14                                      '4.64 - 6.36'   \n",
       "15  \"A dolphin's life span varies according to its...   \n",
       "16  \"In a New York State hermit's letter to the ed...   \n",
       "17  'A macintosh made from cotton fabric treated w...   \n",
       "18   '7 days before to 7 days after the rash appears'   \n",
       "19                                           'Africa'   \n",
       "\n",
       "                                           antithesis  \\\n",
       "0          \"correct\", \"results-based accountability\".   \n",
       "1   \"correct\", because he ran as republican agains...   \n",
       "2                  \"sydneysurroundingsareasneedtime\",   \n",
       "3   'correct'\\n\\nThe correct response would depend...   \n",
       "4            'correct','due to symptom'\\n\\nconversion   \n",
       "5   The correct response would have been \"inside t...   \n",
       "6   \"Most Correct\"\\n\\nThis statement directly addr...   \n",
       "7   \"correct\", \"the correct response would have be...   \n",
       "8   'incorrect'\\n'A Lapel pin refers specifically ...   \n",
       "9                             \"incorrect\", \"somatic\".   \n",
       "10                                         'correct.'   \n",
       "11  'correct'\\n\\nThe correct time frame mentioned ...   \n",
       "12                                \"correct\", \"yeast\".   \n",
       "13  The correct response would depend upon what ex...   \n",
       "14  The correct range from \"the\" text above would ...   \n",
       "15                                            \"fish\".   \n",
       "16  \"New York state Hermits Letter To Editor Of An...   \n",
       "17  \"Oil skin\", as its name suggests refers back i...   \n",
       "18      The correct response would have been \"Rubela\"   \n",
       "19  'correct'\\n\\nthe correct response would have b...   \n",
       "\n",
       "                                            synthesis  \n",
       "0   'Results-Based Accountability is a disciplined...  \n",
       "1                                               'Yes'  \n",
       "2                                               'Yes'  \n",
       "3                                          'Honolulu'  \n",
       "4                       'Due to symptoms in the body'  \n",
       "5                              'Inside the rib cage.'  \n",
       "6   'The most expensive patents are international ...  \n",
       "7                'Sophocles, Aeschylus and Euripides'  \n",
       "8   'A tree or shrub which produces distinctive co...  \n",
       "9                                         'Ecosystem'  \n",
       "10                                        'Dr. Seuss'  \n",
       "11                            '45 minutes to an hour'  \n",
       "12  'Candidiasis is a fungal infection caused by y...  \n",
       "13  'To become a very good driver and To perform t...  \n",
       "14                                      '4.64 - 6.36'  \n",
       "15  \"A dolphin's life span varies according to its...  \n",
       "16  \"In a New York State hermit's letter to the ed...  \n",
       "17  'Vice President of the United States in the sc...  \n",
       "18   '7 days before to 7 days after the rash appears'  \n",
       "19                                           'Africa'  "
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "8f34f7dc-6407-4f3b-b601-d86c5296d0dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"correct\", because he ran as republican against president carter'"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['antithesis'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "c045384f-b1da-44e4-a995-3aeff4ad682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('test-2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5856d161-623d-4258-bd8c-fbe224f1b286",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
